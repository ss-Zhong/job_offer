#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå¤šæ¨¡å‹æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®éªŒ - é›†æˆå±‚æ¬¡åŒ–æŸå¤±å‡½æ•°
æ”¯æŒå¤šçº§åˆ«å‡†ç¡®ç‡åˆ†æã€å¤šæ¨¡å‹å¯¹æ¯”å’Œå±‚æ¬¡åŒ–æŸå¤±
"""

import os
import sys
import time
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
import torch
torch.set_float32_matmul_precision('high')
import GPUtil
warnings.filterwarnings('ignore')

# å¯¼å…¥åŸºç¡€æ¨¡å—
from job_offers_classifier.job_offers_classfier_new import ChineseTransformerJobOffersClassifier, get_recommended_chinese_models
from job_offers_classifier.job_offers_utils_new import create_hierarchy_node
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# # å¯¼å…¥æ•°æ®å¢å¼ºæ¨¡å—
# from chinese_job_data_augmentation import EnhancedJobDataProcessor
# # å¯¼å…¥å±‚æ¬¡åŒ–æŸå¤±æ¨¡å—
# from hierarchical_loss import HierarchicalISCOLoss

def select_gpu_with_most_free_memory():
    """
    select gpu with most free memory
    """
    gpus = GPUtil.getGPUs()
    if not gpus:
        raise RuntimeError("No GPU found!")
    gpu_id = max(gpus, key=lambda gpu: gpu.memoryFree).id
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    print(f"Selected GPU: {gpu_id}, Free Memory: {gpus[gpu_id].memoryFree} MB")

# é€‰æ‹©æ˜¾å­˜æœ€å¤šçš„GPU
select_gpu_with_most_free_memory()


class HierarchicalMultiModelComparison:
    """é›†æˆå±‚æ¬¡åŒ–æŸå¤±çš„å¤šæ¨¡å‹å¯¹æ¯”å®éªŒ"""
    
    def __init__(self, csv_path: str, max_samples: int = 12000):
        self.csv_path = csv_path
        self.max_samples = max_samples
        
        # å®éªŒé…ç½®ï¼šåŒ…å«æ˜¯å¦ä½¿ç”¨å±‚æ¬¡åŒ–æŸå¤±çš„å¯¹æ¯”
        self.experiment_configs = [
            # {
            #     'name': 'Baseline',
            #     'use_hierarchical_loss': False,
            #     'use_multitask_learning': False,
            #     'description': 'åŸºçº¿æ¨¡å‹ï¼ˆæ ‡å‡†äº¤å‰ç†µæŸå¤±ï¼‰'
            # },
            # {
            #     'name': 'Hierarchical',
            #     'use_hierarchical_loss': True,
            #     'use_multitask_learning': False,
            #     # 'hierarchical_loss_weights': {1: 0.5, 2: 1.0, 3: 1.5, 4: 2.0},
            #     # 'hierarchical_loss_weights': {1: 2.0, 2: 1.5, 3: 1, 4: 0.5},
            #     'hierarchical_loss_weights': {1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0},
            #     'description': 'å±‚æ¬¡åŒ–æŸå¤±ï¼ˆå•ä»»åŠ¡ï¼‰'
            # },
            {
                'name': 'Multitask',
                'use_hierarchical_loss': True,
                'use_multitask_learning': True,
                'hierarchical_loss_weights': {1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0},
                'task_weights': {1: 0.1, 2: 0.2, 3: 0.3, 4: 0.4},
                'description': 'å¤šä»»åŠ¡å±‚æ¬¡åŒ–å­¦ä¹ '
            }
        ]
        
        # æµ‹è¯•æ¨¡å‹
        self.test_models = [
            'bert-base-chinese',
            # 'hfl/chinese-bert-wwm-ext',
            # 'hfl/chinese-roberta-wwm-ext'
        ]
        
        # æ¨¡å‹ä¿¡æ¯
        self.model_info = {
            'bert-base-chinese': {
                'name': 'Google Chinese BERT',
                'description': 'Googleå®˜æ–¹ä¸­æ–‡æ¨¡å‹'
            },
            'hfl/chinese-bert-wwm-ext': {
                'name': 'HFL Chinese BERT-wwm',
                'description': 'å…¨è¯æ©ç é¢„è®­ç»ƒ'
            },
            'hfl/chinese-roberta-wwm-ext': {
                'name': 'HFL Chinese RoBERTa',
                'description': 'RoBERTaæ¶æ„ä¼˜åŒ–'
            }
        }
        
        # è®­ç»ƒé…ç½®
        self.training_config = {
            'max_epochs': 8, # é»˜è®¤è®¾çš„æ˜¯8
            'patience': 4,
            'max_seq_length': 256,
            'batch_size': 64, # åŸæ¥æ˜¯16
            'learning_rate': 2e-5
        }
        
        # ISCOå±‚çº§å®šä¹‰
        self.isco_levels = {
            1: "ä¸»è¦èŒä¸šç»„",
            2: "æ¬¡è¦èŒä¸šç»„",
            3: "æ¬¡çº§èŒä¸šç»„",
            4: "åŸºæœ¬èŒä¸šç»„"
        }
        
        print("ğŸ”¬ å±‚æ¬¡åŒ–æŸå¤±å¤šæ¨¡å‹å¯¹æ¯”å®éªŒåˆå§‹åŒ–")
        print(f"   æµ‹è¯•æ¨¡å‹æ•°é‡: {len(self.test_models)}")
        print(f"   å®éªŒé…ç½®æ•°é‡: {len(self.experiment_configs)}")
        print(f"   æœ€å¤§æ ·æœ¬æ•°: {self.max_samples}")

    def calculate_hierarchical_errors(self, y_true, y_pred):
        """è®¡ç®—å±‚æ¬¡åŒ–é”™è¯¯åˆ†æ"""
        error_analysis = {
            'total_errors': 0,
            'level_1_errors': 0,
            'level_2_errors': 0,
            'level_3_errors': 0,
            'level_4_errors': 0,
            'error_examples': []
        }
        
        for true_label, pred_label in zip(y_true, y_pred):
            if true_label != pred_label:
                error_analysis['total_errors'] += 1
                
                # ç¡®å®šé”™è¯¯çº§åˆ«
                error_level = 4
                for level in [1, 2, 3]:
                    if true_label[:level] != pred_label[:level]:
                        error_level = level
                        break
                
                error_analysis[f'level_{error_level}_errors'] += 1
                
                # ä¿å­˜é”™è¯¯ç¤ºä¾‹
                if len(error_analysis['error_examples']) < 10:
                    error_analysis['error_examples'].append({
                        'true': true_label,
                        'pred': pred_label,
                        'error_level': error_level
                    })
        
        return error_analysis

    def train_model_with_config(self, model_name, texts, labels, config, experiment_name, results_dir):
        """ä½¿ç”¨ç‰¹å®šé…ç½®è®­ç»ƒæ¨¡å‹"""
        model_display_name = self.model_info[model_name]['name']
        config_name = config['name']
        
        print(f"\nğŸ¤– è®­ç»ƒ: {model_display_name} - {config_name} - {experiment_name}")
        print(f"   é…ç½®: {config['description']}")
        
        # æ•°æ®åˆ’åˆ†
        train_texts, test_texts, train_labels, test_labels, level_analysis = self.safe_train_test_split_with_levels(texts, labels)
        
        if len(test_texts) < 5:
            print(f"   âŒ æµ‹è¯•é›†æ ·æœ¬ä¸è¶³")
            return None
        
        # åˆ›å»ºå±‚æ¬¡ç»“æ„
        hierarchy = self.create_isco_hierarchy_from_codes(set(labels))
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        safe_model_name = model_name.replace('/', '_').replace('-', '_')
        model_dir = results_dir / f"model_{safe_model_name}_{config_name}_{experiment_name}"
        
        start_time = time.time()
        
        try:
            # åˆ›å»ºåˆ†ç±»å™¨ï¼Œåº”ç”¨å±‚æ¬¡åŒ–é…ç½®
            classifier = ChineseTransformerJobOffersClassifier(
                model_dir=str(model_dir),
                hierarchy=hierarchy,
                transformer_model=model_name,
                learning_rate=self.training_config['learning_rate'],
                batch_size=self.training_config['batch_size'],
                max_epochs=self.training_config['max_epochs'],
                early_stopping=True,
                early_stopping_patience=self.training_config['patience'],
                max_sequence_length=self.training_config['max_seq_length'],
                devices=1,
                accelerator="gpu" if torch.cuda.is_available() else "cpu",
                precision=16 if torch.cuda.is_available() else 32,
                # å±‚æ¬¡åŒ–æŸå¤±é…ç½®
                use_hierarchical_loss=config.get('use_hierarchical_loss', False),
                use_multitask_learning=config.get('use_multitask_learning', False),
                hierarchical_loss_weights=config.get('hierarchical_loss_weights', None),
                task_weights=config.get('task_weights', None),
                verbose=True
            )
            
            # å‡†å¤‡éªŒè¯é›†
            val_size = min(200, len(test_texts) // 3)
            
            if val_size > 0:
                val_texts = test_texts[:val_size]
                val_labels = test_labels[:val_size]
                final_test_texts = test_texts[val_size:]
                final_test_labels = test_labels[val_size:]
            else:
                val_texts, val_labels = [], []
                final_test_texts = test_texts
                final_test_labels = test_labels
            
            print(f"   è®­ç»ƒæ ·æœ¬: {len(train_texts)}")
            print(f"   éªŒè¯æ ·æœ¬: {val_size if val_size > 0 else 0}")
            print(f"   æµ‹è¯•æ ·æœ¬: {len(final_test_texts)}")
            
            # è®­ç»ƒ
            print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ...")
            
            classifier.fit(train_labels, train_texts, y_val=val_labels, X_val=val_texts)
            
            # é¢„æµ‹
            print(f"ğŸ”® é¢„æµ‹ä¸­...")
            predictions_df = classifier.predict(final_test_texts, format='dataframe', top_k=5)
            
            # æå–é¢„æµ‹ç»“æœ
            y_true = final_test_labels
            y_pred = predictions_df['class_1'].tolist()
            
            # è®¡ç®—å„ç§æŒ‡æ ‡
            accuracy = accuracy_score(y_true, y_pred)
            
            # å±‚æ¬¡åŒ–å‡†ç¡®ç‡
            hierarchical_acc = self.calculate_hierarchical_accuracy(y_true, y_pred, None)
            
            # é”™è¯¯åˆ†æ
            error_analysis = self.calculate_hierarchical_errors(y_true, y_pred)
            
            # Top-kå‡†ç¡®ç‡
            top_3_acc = sum(
                true_label in [predictions_df.iloc[i][f'class_{j}'] for j in range(1, 4)]
                for i, true_label in enumerate(y_true)
            ) / len(y_true)
            
            top_5_acc = sum(
                true_label in [predictions_df.iloc[i][f'class_{j}'] for j in range(1, 6)]
                for i, true_label in enumerate(y_true)
            ) / len(y_true)
            
            training_time = time.time() - start_time
            
            result = {
                'model_name': model_name,
                'model_display_name': model_display_name,
                'config_name': config_name,
                'config_description': config['description'],
                'experiment_name': experiment_name,
                'train_samples': len(train_texts),
                'test_samples': len(final_test_texts),
                'accuracy': accuracy,
                'top_3_accuracy': top_3_acc,
                'top_5_accuracy': top_5_acc,
                'training_time_minutes': training_time / 60,
                'hierarchical_accuracy': hierarchical_acc,
                'error_analysis': error_analysis,
                'status': 'success'
            }
            
            print(f"âœ… è®­ç»ƒå®Œæˆ!")
            print(f"   4çº§å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"   å±‚æ¬¡åŒ–é”™è¯¯åˆ†å¸ƒ:")
            for level in [1, 2, 3, 4]:
                level_errors = error_analysis[f'level_{level}_errors']
                print(f"     {level}çº§é”™è¯¯: {level_errors} ({level_errors/len(y_true)*100:.2f}%)")
            
            return result
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'model_name': model_name,
                'config_name': config_name,
                'experiment_name': experiment_name,
                'status': 'failed',
                'error': str(e)
            }

    def run_hierarchical_comparison(self):
        """è¿è¡Œå±‚æ¬¡åŒ–æŸå¤±å¯¹æ¯”å®éªŒ"""
        print("ğŸ”¬ å¼€å§‹å±‚æ¬¡åŒ–æŸå¤±å¤šæ¨¡å‹å¯¹æ¯”å®éªŒ")
        print("=" * 80)
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = Path("hierarchical_loss_comparison_results")
        results_dir.mkdir(exist_ok=True)
        
        experiment_start_time = time.time()
        all_results = []
        
        # åŠ è½½æ•°æ®
        print(f"\nğŸ“Š æ•°æ®å‡†å¤‡é˜¶æ®µ")
        print("=" * 80)
        
        # åªåŠ è½½åŸå§‹æ•°æ®è¿›è¡Œå¯¹æ¯”
        texts, labels, stats = self.load_data()
        
        # å¯¹æ¯ä¸ªæ¨¡å‹å’Œé…ç½®è¿›è¡Œå®éªŒ
        for model_idx, model_name in enumerate(self.test_models):
            model_display_name = self.model_info[model_name]['name']
            
            print(f"\n{'='*80}")
            print(f"ğŸ¤– æ¨¡å‹ {model_idx+1}/{len(self.test_models)}: {model_display_name}")
            print(f"{'='*80}")
            
            for config in self.experiment_configs:
                result = self.train_model_with_config(
                    model_name, texts, labels, config, "Original", results_dir
                )
                
                if result:
                    all_results.append(result)
        
        total_time = time.time() - experiment_start_time
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_hierarchical_report(all_results, total_time, results_dir)
        
        return all_results, results_dir

    def generate_hierarchical_report(self, results, total_time, results_dir):
        """ç”Ÿæˆå±‚æ¬¡åŒ–æŸå¤±å¯¹æ¯”æŠ¥å‘Š"""
        print(f"\nğŸ“Š ç”Ÿæˆå±‚æ¬¡åŒ–æŸå¤±å¯¹æ¯”æŠ¥å‘Š...")
        
        # ä¿å­˜åŸå§‹ç»“æœ
        results_df = pd.DataFrame(results)
        results_df.to_csv(results_dir / "hierarchical_comparison_results.csv", index=False, encoding='utf-8')
        
        # åˆ†æç»“æœ
        successful_results = [r for r in results if r['status'] == 'success']
        
        if not successful_results:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„å®éªŒç»“æœ")
            return
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        
        for result in successful_results:
            if result['status'] == 'success':
                error_analysis = result['error_analysis']
                
                row = {
                    'Model': result['model_display_name'],
                    'Config': result['config_name'],
                    'Accuracy': result['accuracy'],
                    'Top3_Acc': result['top_3_accuracy'],
                    'Top5_Acc': result['top_5_accuracy'],
                    'Level1_Errors': error_analysis['level_1_errors'],
                    'Level2_Errors': error_analysis['level_2_errors'],
                    'Level3_Errors': error_analysis['level_3_errors'],
                    'Level4_Errors': error_analysis['level_4_errors'],
                    'Total_Errors': error_analysis['total_errors'],
                    'Training_Time': result['training_time_minutes']
                }
                
                comparison_data.append(row)
        
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df.to_csv(results_dir / "hierarchical_comparison_table.csv", index=False)
        
        # åˆ›å»ºå¯è§†åŒ–
        self.create_hierarchical_visualizations(comparison_df, results_dir)
        
        # æ‰“å°æ€»ç»“
        print(f"\nğŸ‰ å±‚æ¬¡åŒ–æŸå¤±å¯¹æ¯”å®éªŒå®Œæˆ!")
        print(f"ğŸ“ˆ å®éªŒæ€»ç»“:")
        print(f"   æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
        print(f"   æµ‹è¯•æ¨¡å‹: {len(self.test_models)} ä¸ª")
        print(f"   å®éªŒé…ç½®: {len(self.experiment_configs)} ç§")
        print(f"   æˆåŠŸå®éªŒ: {len(successful_results)}/{len(results)}")
        
        # æœ€ä½³é…ç½®åˆ†æ
        best_accuracy = comparison_df.loc[comparison_df['Accuracy'].idxmax()]
        least_l1_errors = comparison_df.loc[comparison_df['Level1_Errors'].idxmin()]
        
        print(f"\nğŸ† æœ€ä½³ç»“æœ:")
        print(f"   æœ€é«˜å‡†ç¡®ç‡: {best_accuracy['Model']} - {best_accuracy['Config']} ({best_accuracy['Accuracy']:.4f})")
        print(f"   æœ€å°‘1çº§é”™è¯¯: {least_l1_errors['Model']} - {least_l1_errors['Config']} ({least_l1_errors['Level1_Errors']}ä¸ª)")
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {results_dir}")

    def create_hierarchical_visualizations(self, df, results_dir):
        """åˆ›å»ºå±‚æ¬¡åŒ–æŸå¤±å¯¹æ¯”å¯è§†åŒ–"""
        # è®¾ç½®matplotlibä¸­æ–‡æ”¯æŒ
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. é”™è¯¯çº§åˆ«åˆ†å¸ƒå¯¹æ¯”
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # æŒ‰é…ç½®åˆ†ç»„
        configs = df['Config'].unique()
        models = df['Model'].unique()
        
        # ç»˜åˆ¶æ¯ä¸ªçº§åˆ«çš„é”™è¯¯å¯¹æ¯”
        for i, level in enumerate([1, 2, 3, 4]):
            ax = axes[i//2, i%2]
            
            # å‡†å¤‡æ•°æ®
            error_data = []
            for config in configs:
                config_data = df[df['Config'] == config]
                errors = config_data[f'Level{level}_Errors'].values
                error_data.append(errors)
            
            # åˆ›å»ºæŸ±çŠ¶å›¾
            x = np.arange(len(models))
            width = 0.25
            
            for j, (config, data) in enumerate(zip(configs, error_data)):
                offset = (j - len(configs)/2) * width + width/2
                ax.bar(x + offset, data, width, label=config, alpha=0.8)
            
            ax.set_xlabel('æ¨¡å‹', fontsize=12)
            ax.set_ylabel(f'{level}çº§é”™è¯¯æ•°', fontsize=12)
            ax.set_title(f'{level}çº§åˆ†ç±»é”™è¯¯å¯¹æ¯”\n({self.isco_levels[level]})', fontsize=14)
            ax.set_xticks(x)
            ax.set_xticklabels([m.replace(' ', '\n') for m in models], fontsize=10)
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(results_dir / "hierarchical_errors_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. å‡†ç¡®ç‡å¯¹æ¯”çƒ­åŠ›å›¾
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # åˆ›å»ºæ•°æ®é€è§†è¡¨
        pivot_data = df.pivot(index='Model', columns='Config', values='Accuracy')
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(pivot_data, annot=True, fmt='.4f', cmap='RdYlGn', 
                    center=pivot_data.mean().mean(), ax=ax, cbar_kws={'label': 'å‡†ç¡®ç‡'})
        
        ax.set_title('ä¸åŒé…ç½®ä¸‹çš„æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel('å®éªŒé…ç½®', fontsize=12)
        ax.set_ylabel('æ¨¡å‹', fontsize=12)
        
        plt.tight_layout()
        plt.savefig(results_dir / "accuracy_heatmap_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. é”™è¯¯åˆ†å¸ƒé›·è¾¾å›¾
        fig, axes = plt.subplots(1, len(configs), figsize=(6*len(configs), 6))
        if len(configs) == 1:
            axes = [axes]
        
        for idx, config in enumerate(configs):
            ax = axes[idx]
            
            # å‡†å¤‡æ•°æ®
            config_data = df[df['Config'] == config]
            
            # è®¾ç½®é›·è¾¾å›¾
            categories = ['1çº§é”™è¯¯', '2çº§é”™è¯¯', '3çº§é”™è¯¯', '4çº§é”™è¯¯']
            angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
            angles += angles[:1]
            
            ax = plt.subplot(1, len(configs), idx+1, projection='polar')
            
            for _, row in config_data.iterrows():
                values = [
                    row['Level1_Errors'],
                    row['Level2_Errors'],
                    row['Level3_Errors'],
                    row['Level4_Errors']
                ]
                
                # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
                max_errors = max(values) if max(values) > 0 else 1
                values = [v/max_errors for v in values]
                values += values[:1]
                
                ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'])
                ax.fill(angles, values, alpha=0.25)
            
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(categories)
            ax.set_ylim(0, 1)
            ax.set_title(f'{config}é…ç½®\né”™è¯¯åˆ†å¸ƒ', fontsize=14, fontweight='bold', pad=20)
            ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))
            ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(results_dir / "error_distribution_radar.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. è®­ç»ƒæ—¶é—´vså‡†ç¡®ç‡æ•£ç‚¹å›¾
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # ä¸ºæ¯ä¸ªé…ç½®ä½¿ç”¨ä¸åŒçš„æ ‡è®°
        markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p']
        colors = plt.cm.tab10(np.linspace(0, 1, len(configs)))
        
        for i, config in enumerate(configs):
            config_data = df[df['Config'] == config]
            ax.scatter(config_data['Training_Time'], config_data['Accuracy'], 
                      label=config, marker=markers[i % len(markers)], 
                      s=100, alpha=0.7, color=colors[i])
            
            # æ·»åŠ æ¨¡å‹æ ‡ç­¾
            for _, row in config_data.iterrows():
                ax.annotate(row['Model'].split()[-1], 
                           (row['Training_Time'], row['Accuracy']),
                           xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        ax.set_xlabel('è®­ç»ƒæ—¶é—´ (åˆ†é’Ÿ)', fontsize=12)
        ax.set_ylabel('å‡†ç¡®ç‡', fontsize=12)
        ax.set_title('è®­ç»ƒæ•ˆç‡å¯¹æ¯”', fontsize=16, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(results_dir / "training_efficiency_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å±‚æ¬¡åŒ–å¯¹æ¯”å¯è§†åŒ–å·²ç”Ÿæˆ:")
        print(f"   - hierarchical_errors_comparison.png: å„çº§åˆ«é”™è¯¯å¯¹æ¯”")
        print(f"   - accuracy_heatmap_comparison.png: å‡†ç¡®ç‡çƒ­åŠ›å›¾")
        print(f"   - error_distribution_radar.png: é”™è¯¯åˆ†å¸ƒé›·è¾¾å›¾")
        print(f"   - training_efficiency_comparison.png: è®­ç»ƒæ•ˆç‡å¯¹æ¯”")

    def create_isco_hierarchy_from_codes(self, isco_codes):
        """ä»ISCOç¼–ç åˆ›å»ºå±‚æ¬¡ç»“æ„"""
        hierarchy = {}
        
        for code in isco_codes:
            code_str = str(code).zfill(4)
            
            for level in [1, 2, 3, 4]:
                level_code = code_str[:level]
                if level_code not in hierarchy:
                    hierarchy[level_code] = create_hierarchy_node(
                        level_code, 
                        f"ISCO-{level}ä½-{level_code}"
                    )
        
        return hierarchy

    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print(f"\nğŸ“Š åŠ è½½æ•°æ®...")
        
        try:
            df = pd.read_csv(self.csv_path, encoding='utf-8')
        except UnicodeDecodeError:
            df = pd.read_csv(self.csv_path, encoding='gbk')
        
        # åŸºç¡€æ–‡æœ¬ç»„åˆ
        def combine_features(row):
            parts = []
            for col in ['å²—ä½', 'å²—ä½æè¿°', 'å²—ä½èŒèƒ½']:
                if col in row and pd.notna(row[col]):
                    content = str(row[col])
                    if col == 'å²—ä½èŒèƒ½' and content.startswith('['):
                        try:
                            import ast
                            job_funcs = ast.literal_eval(content)
                            content = ' '.join(job_funcs) if isinstance(job_funcs, list) else content
                        except:
                            pass
                    parts.append(content)
            return ' '.join(parts)
        
        df['combined_text'] = df.apply(combine_features, axis=1)
        df['isco_code'] = df['ISCO_4_Digit_Code_Gemini'].astype(str).str.zfill(4)
        
        # ç§»é™¤ç©ºæ–‡æœ¬
        df = df[df['combined_text'].str.strip() != ''].copy()
        
        # é‡‡æ ·
        if self.max_samples and len(df) > self.max_samples:
            df = df.sample(n=self.max_samples, random_state=42)
        
        texts = df['combined_text'].tolist()
        labels = df['isco_code'].tolist()
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_samples': len(texts),
            'unique_labels': len(set(labels)),
            'avg_text_length': np.mean([len(text) for text in texts])
        }
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   æ ·æœ¬æ•°: {len(texts)}")
        print(f"   ç±»åˆ«æ•°: {len(set(labels))}")
        
        return texts, labels, stats

    def calculate_hierarchical_accuracy(self, y_true, y_pred, top_k_preds=None):
        """è®¡ç®—å±‚æ¬¡åŒ–å‡†ç¡®ç‡"""
        results = {}
        
        for level in [1, 2, 3, 4]:
            true_level = [label[:level] for label in y_true]
            pred_level = [label[:level] for label in y_pred]
            
            accuracy = accuracy_score(true_level, pred_level)
            results[f'level_{level}_accuracy'] = accuracy
        
        return results

    def safe_train_test_split_with_levels(self, texts, labels):
        """å®‰å…¨çš„æ•°æ®åˆ’åˆ†"""
        label_counts = Counter(labels)
        
        single_sample_classes = [label for label, count in label_counts.items() if count == 1]
        multi_sample_classes = [label for label, count in label_counts.items() if count > 1]
        
        train_indices = []
        test_indices = []
        
        # å•æ ·æœ¬ç±»åˆ«å…¨éƒ¨æ”¾å…¥è®­ç»ƒé›†
        for i, (text, label) in enumerate(zip(texts, labels)):
            if label in single_sample_classes:
                train_indices.append(i)
        
        # å¤šæ ·æœ¬ç±»åˆ«åˆ†å±‚åˆ’åˆ†
        if multi_sample_classes:
            multi_texts = []
            multi_labels = []
            multi_indices = []
            
            for i, (text, label) in enumerate(zip(texts, labels)):
                if label in multi_sample_classes:
                    multi_texts.append(text)
                    multi_labels.append(label)
                    multi_indices.append(i)
            
            if len(multi_texts) > 0:
                try:
                    multi_train_idx, multi_test_idx = train_test_split(
                        range(len(multi_texts)), 
                        test_size=0.2, 
                        random_state=42, 
                        stratify=multi_labels
                    )
                except:
                    multi_train_idx, multi_test_idx = train_test_split(
                        range(len(multi_texts)), 
                        test_size=0.2, 
                        random_state=42
                    )
                
                train_indices.extend([multi_indices[i] for i in multi_train_idx])
                test_indices.extend([multi_indices[i] for i in multi_test_idx])
        
        train_texts = [texts[i] for i in train_indices]
        train_labels = [labels[i] for i in train_indices]
        test_texts = [texts[i] for i in test_indices]
        test_labels = [labels[i] for i in test_indices]
        
        # çº§åˆ«åˆ†æ
        level_analysis = {}
        for level in [1, 2, 3, 4]:
            level_codes = [label[:level] for label in labels]
            level_counts = Counter(level_codes)
            level_analysis[level] = {
                'total_classes': len(level_counts),
                'single_sample_classes': sum(1 for c in level_counts.values() if c == 1)
            }
        
        return train_texts, test_texts, train_labels, test_labels, level_analysis


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å±‚æ¬¡åŒ–æŸå¤±å¤šæ¨¡å‹å¯¹æ¯”å®éªŒ")
    print("=" * 80)
    
    # é…ç½®å®éªŒ
    csv_path = "./newjob1_sortall.csv"
    
    if not os.path.exists(csv_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {csv_path}")
        return
    
    print("å®éªŒè¯´æ˜:")
    print("æœ¬å®éªŒå°†å¯¹æ¯”ä¸‰ç§æŸå¤±å‡½æ•°é…ç½®:")
    print("1. Baseline: æ ‡å‡†äº¤å‰ç†µæŸå¤±")
    print("2. Hierarchical: å±‚æ¬¡åŒ–æŸå¤±ï¼ˆæ ¹æ®é”™è¯¯çº§åˆ«ç»™äºˆä¸åŒæƒ©ç½šï¼‰")
    print("3. Multitask: å¤šä»»åŠ¡å±‚æ¬¡åŒ–å­¦ä¹ ï¼ˆåŒæ—¶é¢„æµ‹1-4çº§ï¼‰")
    
    # confirm = input("\nç¡®è®¤å¼€å§‹å®éªŒ? (y/N): ")
    # if confirm.lower() != 'y':
    #     print("å®éªŒå·²å–æ¶ˆ")
    #     return
    
    try:
        # åˆ›å»ºå®éªŒå¯¹è±¡
        experiment = HierarchicalMultiModelComparison(csv_path, max_samples=4000)
        
        # è¿è¡Œå¯¹æ¯”å®éªŒ
        results, results_dir = experiment.run_hierarchical_comparison()
        
        print(f"\nğŸ¯ å®éªŒå®Œæˆ!")
        print(f"ğŸ“ æŸ¥çœ‹è¯¦ç»†ç»“æœ: {results_dir}")
        
    except Exception as e:
        print(f"âŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()