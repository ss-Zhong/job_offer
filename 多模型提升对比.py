#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå¤šæ¨¡å‹æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®éªŒ
æ”¯æŒå¤šçº§åˆ«å‡†ç¡®ç‡åˆ†æå’Œå¤šæ¨¡å‹å¯¹æ¯”
"""

import os
import time
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
import torch

warnings.filterwarnings('ignore')

# å¯¼å…¥åŸæœ‰æ¨¡å—
if __name__ == '__main__':
    # æ·»åŠ è·¯å¾„ä»¥ä¾¿å¯¼å…¥lunwenimproä¸‹çš„æ¨¡å—
    import sys
    sys.path.append('.')  # å½“å‰ç›®å½•
    sys.path.append('lunwenimpro')  # lunwenimproç›®å½•
    
    from lunwenimpro.job_offers_classifier.job_offers_classfier import (
        ChineseTransformerJobOffersClassifier,
        get_recommended_chinese_models
    )
    from lunwenimpro.job_offers_classifier.job_offers_utils import create_hierarchy_node
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, accuracy_score
    
    # å¯¼å…¥æ•°æ®å¢å¼ºæ¨¡å— - ç¡®ä¿è¿™ä¸ªæ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹
    from chinese_job_data_augmentation import EnhancedJobDataProcessor
    from job_offers_classifier.job_offers_utils_old import create_hierarchy_node
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, accuracy_score
    
    # å¯¼å…¥æ•°æ®å¢å¼ºæ¨¡å—
    from chinese_job_data_augmentation import EnhancedJobDataProcessor


class EnhancedMultiModelComparison:
    """å¢å¼ºç‰ˆå¤šæ¨¡å‹æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®éªŒ"""
    
    def __init__(self, csv_path: str, max_samples: int = 12000):
        self.csv_path = csv_path
        self.max_samples = max_samples
        
        # å¤šæ¨¡å‹é…ç½® - ä½¿ç”¨æ”¯æŒçš„ä¸­æ–‡æ¨¡å‹
        self.test_models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
        
        # æ¨¡å‹ä¿¡æ¯æ˜ å°„
        self.model_info = {
            'bert-base-chinese': {
                'name': 'Google Chinese BERT',
                'description': 'Googleå®˜æ–¹ä¸­æ–‡æ¨¡å‹',
                'params': '110M'
            },
            'hfl/chinese-bert-wwm-ext': {
                'name': 'HFL Chinese BERT-wwm',
                'description': 'å…¨è¯æ©ç é¢„è®­ç»ƒ',
                'params': '110M'
            },
            'hfl/chinese-roberta-wwm-ext': {
                'name': 'HFL Chinese RoBERTa',
                'description': 'RoBERTaæ¶æ„ä¼˜åŒ–',
                'params': '110M'
            },
            'ckiplab/bert-base-chinese': {
                'name': 'CKIP Chinese BERT',
                'description': 'å°æ¹¾ä¸­ç ”é™¢ç‰ˆæœ¬',
                'params': '110M'
            },
            'bert-base-multilingual-cased': {
                'name': 'BERT Multilingual',
                'description': 'å¤šè¯­è¨€åŸºå‡†æ¨¡å‹',
                'params': '110M'
            }
        }
        
        # è®­ç»ƒé…ç½®
        self.training_config = {
            'max_epochs': 8,
            'patience': 4,
            'max_seq_length': 256,
            'batch_size': 16,
            'learning_rate': 2e-5
        }
        
        # ISCOå±‚çº§å®šä¹‰
        self.isco_levels = {
            1: "ä¸»è¦èŒä¸šç»„",
            2: "æ¬¡è¦èŒä¸šç»„", 
            3: "æ¬¡çº§èŒä¸šç»„",
            4: "åŸºæœ¬èŒä¸šç»„"
        }
        
        print("ğŸ”¬ å¢å¼ºç‰ˆå¤šæ¨¡å‹æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®éªŒåˆå§‹åŒ–")
        print(f"   æµ‹è¯•æ¨¡å‹æ•°é‡: {len(self.test_models)}")
        print(f"   æœ€å¤§æ ·æœ¬æ•°: {self.max_samples}")
        print(f"   ISCOå±‚çº§åˆ†æ: {list(self.isco_levels.keys())}")

    def create_isco_hierarchy_from_codes(self, isco_codes):
        """ä»ISCOç¼–ç åˆ›å»ºå±‚æ¬¡ç»“æ„"""
        hierarchy = {}
        
        for code in isco_codes:
            code_str = str(code).zfill(4)
            
            for level in [1, 2, 3, 4]:
                level_code = code_str[:level]
                if level_code not in hierarchy:
                    hierarchy[level_code] = create_hierarchy_node(
                        level_code, 
                        f"ISCO-{level}ä½-{level_code}"
                    )
        
        return hierarchy

    def load_data(self, enable_augmentation=False):
        """åŠ è½½æ•°æ®"""
        data_type = "å¢å¼º" if enable_augmentation else "åŸå§‹"
        print(f"\nğŸ“Š åŠ è½½{data_type}æ•°æ®...")
        
        if enable_augmentation:
            # ä½¿ç”¨å¢å¼ºæ•°æ®
            processor = EnhancedJobDataProcessor()
            texts, labels, processing_stats = processor.process_csv_data(
                csv_path=self.csv_path,
                enable_augmentation=True,
                balance_data=True,
                target_samples_per_class=8
            )
            
            # é™åˆ¶æ ·æœ¬æ•°
            if self.max_samples and len(texts) > self.max_samples:
                indices = np.random.choice(len(texts), size=self.max_samples, replace=False)
                texts = [texts[i] for i in indices]
                labels = [labels[i] for i in indices]
            
            stats = processing_stats['final_stats']
            
        else:
            # ä½¿ç”¨åŸå§‹æ•°æ®
            try:
                df = pd.read_csv(self.csv_path, encoding='utf-8')
            except UnicodeDecodeError:
                df = pd.read_csv(self.csv_path, encoding='gbk')
            
            # åŸºç¡€æ–‡æœ¬ç»„åˆ
            def combine_features(row):
                parts = []
                for col in ['å²—ä½', 'å²—ä½æè¿°', 'å²—ä½èŒèƒ½']:
                    if col in row and pd.notna(row[col]):
                        content = str(row[col])
                        if col == 'å²—ä½èŒèƒ½' and content.startswith('['):
                            try:
                                import ast
                                job_funcs = ast.literal_eval(content)
                                content = ' '.join(job_funcs) if isinstance(job_funcs, list) else content
                            except:
                                pass
                        parts.append(content)
                return ' '.join(parts)
            
            df['combined_text'] = df.apply(combine_features, axis=1)
            df['isco_code'] = df['ISCO_4_Digit_Code_Gemini'].astype(str).str.zfill(4)
            
            # ç§»é™¤ç©ºæ–‡æœ¬
            df = df[df['combined_text'].str.strip() != ''].copy()
            
            # æ™ºèƒ½é‡‡æ ·
            if self.max_samples and len(df) > self.max_samples:
                class_counts = df['isco_code'].value_counts()
                single_classes = class_counts[class_counts == 1].index.tolist()
                multi_classes = class_counts[class_counts > 1].index.tolist()
                
                sampled_data = []
                single_sample_data = df[df['isco_code'].isin(single_classes)]
                remaining_budget = self.max_samples - len(single_sample_data)
                
                if remaining_budget > 0:
                    sampled_data.append(single_sample_data)
                    multi_sample_data = df[df['isco_code'].isin(multi_classes)]
                    
                    if len(multi_sample_data) > remaining_budget:
                        try:
                            multi_sample_data = multi_sample_data.groupby('isco_code', group_keys=False).apply(
                                lambda x: x.sample(min(len(x), max(2, remaining_budget // len(multi_classes))), 
                                                 random_state=42)
                            ).reset_index(drop=True)
                            
                            if len(multi_sample_data) > remaining_budget:
                                multi_sample_data = multi_sample_data.sample(n=remaining_budget, random_state=42)
                        except ValueError:
                            multi_sample_data = multi_sample_data.sample(n=remaining_budget, random_state=42)
                    
                    sampled_data.append(multi_sample_data)
                else:
                    sampled_data.append(single_sample_data.sample(n=self.max_samples, random_state=42))
                
                df = pd.concat(sampled_data, ignore_index=True)
            
            texts = df['combined_text'].tolist()
            labels = df['isco_code'].tolist()
            
            # è¿‡æ»¤å•æ ·æœ¬ç±»åˆ«
            final_class_counts = pd.Series(labels).value_counts()
            single_classes = final_class_counts[final_class_counts == 1].index.tolist()
            
            if len(single_classes) > len(final_class_counts) * 0.3:
                multi_classes = final_class_counts[final_class_counts > 1].index.tolist()
                filtered_data = [(text, label) for text, label in zip(texts, labels) if label in multi_classes]
                
                if len(filtered_data) > 100:
                    texts, labels = zip(*filtered_data)
                    texts, labels = list(texts), list(labels)
            
            # ç»Ÿè®¡ä¿¡æ¯
            text_lengths = [len(text) for text in texts]
            stats = {
                'total_samples': len(texts),
                'unique_labels': len(set(labels)),
                'avg_text_length': np.mean(text_lengths),
                'avg_word_count': np.mean([len(text.split()) for text in texts]),
                'label_distribution': Counter(labels)
            }
        
        print(f"âœ… {data_type}æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   æ ·æœ¬æ•°: {len(texts)}")
        print(f"   ç±»åˆ«æ•°: {len(set(labels))}")
        print(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {stats['avg_text_length']:.1f}")
        
        return texts, labels, stats

    def analyze_isco_levels(self, labels):
        """åˆ†æISCOå„çº§åˆ«åˆ†å¸ƒ"""
        level_stats = {}
        
        for level in [1, 2, 3, 4]:
            level_codes = [label[:level] for label in labels]
            level_unique = len(set(level_codes))
            level_stats[level] = {
                'unique_codes': level_unique,
                'description': self.isco_levels[level],
                'codes': list(set(level_codes))
            }
        
        return level_stats

    def calculate_hierarchical_accuracy(self, y_true, y_pred, top_k_preds=None):
        """è®¡ç®—å±‚æ¬¡åŒ–å‡†ç¡®ç‡"""
        results = {}
        
        # å„çº§åˆ«å‡†ç¡®ç‡
        for level in [1, 2, 3, 4]:
            true_level = [label[:level] for label in y_true]
            pred_level = [label[:level] for label in y_pred]
            
            accuracy = accuracy_score(true_level, pred_level)
            results[f'level_{level}_accuracy'] = accuracy
            
            # Top-kå±‚æ¬¡åŒ–å‡†ç¡®ç‡ï¼ˆå¦‚æœæœ‰top_ké¢„æµ‹ï¼‰
            if top_k_preds is not None:
                top_k_level_acc = []
                for i, true_code in enumerate(true_level):
                    # æ£€æŸ¥true_codeæ˜¯å¦åœ¨top_ké¢„æµ‹çš„ä»»ä½•ä¸€ä¸ªçš„ç›¸åº”çº§åˆ«ä¸­
                    match_found = False
                    for pred_code in top_k_preds[i]:
                        if str(pred_code)[:level] == true_code:
                            match_found = True
                            break
                    top_k_level_acc.append(match_found)
                
                results[f'level_{level}_top5_accuracy'] = np.mean(top_k_level_acc)
        
        return results

    def safe_train_test_split_with_levels(self, texts, labels):
        """è€ƒè™‘å±‚æ¬¡ç»“æ„çš„å®‰å…¨æ•°æ®åˆ’åˆ†"""
        print("   ğŸ“Š åˆ†ææ•°æ®åˆ†å¸ƒ...")
        label_counts = Counter(labels)
        
        # åˆ†æå„çº§åˆ«çš„åˆ†å¸ƒ
        level_analysis = {}
        for level in [1, 2, 3, 4]:
            level_codes = [label[:level] for label in labels]
            level_counts = Counter(level_codes)
            single_sample_classes = [code for code, count in level_counts.items() if count == 1]
            level_analysis[level] = {
                'total_classes': len(level_counts),
                'single_sample_classes': len(single_sample_classes),
                'multi_sample_classes': len(level_counts) - len(single_sample_classes)
            }
        
        # ä½¿ç”¨4çº§ç¼–ç è¿›è¡Œåˆ’åˆ†ï¼ˆæœ€ç»†ç²’åº¦ï¼‰
        single_sample_classes = [label for label, count in label_counts.items() if count == 1]
        multi_sample_classes = [label for label, count in label_counts.items() if count > 1]
        
        print(f"   4çº§ç¼–ç  - å•æ ·æœ¬ç±»åˆ«: {len(single_sample_classes)}, å¤šæ ·æœ¬ç±»åˆ«: {len(multi_sample_classes)}")
        
        train_indices = []
        test_indices = []
        
        # å•æ ·æœ¬ç±»åˆ«å…¨éƒ¨æ”¾å…¥è®­ç»ƒé›†
        for i, (text, label) in enumerate(zip(texts, labels)):
            if label in single_sample_classes:
                train_indices.append(i)
        
        # å¤šæ ·æœ¬ç±»åˆ«æ­£å¸¸åˆ†å±‚åˆ’åˆ†
        if multi_sample_classes:
            multi_texts = []
            multi_labels = []
            multi_indices = []
            
            for i, (text, label) in enumerate(zip(texts, labels)):
                if label in multi_sample_classes:
                    multi_texts.append(text)
                    multi_labels.append(label)
                    multi_indices.append(i)
            
            if len(multi_texts) > 0:
                try:
                    multi_train_idx, multi_test_idx = train_test_split(
                        range(len(multi_texts)), 
                        test_size=0.2, 
                        random_state=42, 
                        stratify=multi_labels
                    )
                except ValueError:
                    # åˆ†å±‚å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆ’åˆ†
                    multi_train_idx, multi_test_idx = train_test_split(
                        range(len(multi_texts)), 
                        test_size=0.2, 
                        random_state=42
                    )
                
                train_indices.extend([multi_indices[i] for i in multi_train_idx])
                test_indices.extend([multi_indices[i] for i in multi_test_idx])
        
        # æ„å»ºæœ€ç»ˆçš„è®­ç»ƒæµ‹è¯•é›†
        train_texts = [texts[i] for i in train_indices]
        train_labels = [labels[i] for i in train_indices]
        test_texts = [texts[i] for i in test_indices]
        test_labels = [labels[i] for i in test_indices]
        
        # éªŒè¯æµ‹è¯•é›†
        if len(test_texts) == 0 and len(train_texts) >= 10:
            split_point = max(1, len(train_texts) // 5)
            test_texts = train_texts[-split_point:]
            test_labels = train_labels[-split_point:]
            train_texts = train_texts[:-split_point]
            train_labels = train_labels[:-split_point]
        
        print(f"   æœ€ç»ˆåˆ’åˆ† - è®­ç»ƒ: {len(train_texts)}, æµ‹è¯•: {len(test_texts)}")
        
        return train_texts, test_texts, train_labels, test_labels, level_analysis

    def train_and_evaluate_model(self, model_name, texts, labels, experiment_name, results_dir):
        """è®­ç»ƒå’Œè¯„ä¼°å•ä¸ªæ¨¡å‹"""
        model_display_name = self.model_info[model_name]['name']
        print(f"\nğŸ¤– å¼€å§‹è®­ç»ƒ: {model_display_name} ({model_name}) - {experiment_name}")
        
        # æ•°æ®åˆ’åˆ†
        train_texts, test_texts, train_labels, test_labels, level_analysis = self.safe_train_test_split_with_levels(texts, labels)
        
        if len(test_texts) < 5:
            print(f"   âŒ æµ‹è¯•é›†æ ·æœ¬ä¸è¶³: {len(test_texts)}")
            return {
                'model_name': model_name,
                'model_display_name': model_display_name,
                'experiment_name': experiment_name,
                'status': 'failed',
                'error': 'Insufficient test data'
            }
        
        # åˆ›å»ºå±‚æ¬¡ç»“æ„
        hierarchy = self.create_isco_hierarchy_from_codes(set(labels))
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        safe_model_name = model_name.replace('/', '_').replace('-', '_')
        model_dir = results_dir / f"model_{safe_model_name}_{experiment_name.lower().replace(' ', '_')}"
        
        start_time = time.time()
        
        try:
            # ä¸ºä¸åŒæ¨¡å‹ä¼˜åŒ–å­¦ä¹ ç‡
            learning_rate = self.training_config['learning_rate']
            if 'roberta' in model_name.lower():
                learning_rate = 2.5e-5  # RoBERTaé€šå¸¸éœ€è¦ç¨é«˜çš„å­¦ä¹ ç‡
            elif 'multilingual' in model_name.lower():
                learning_rate = 1.5e-5  # å¤šè¯­è¨€æ¨¡å‹ä½¿ç”¨è¾ƒä½å­¦ä¹ ç‡
            
            # åˆ›å»ºåˆ†ç±»å™¨
            classifier = ChineseTransformerJobOffersClassifier(
                model_dir=str(model_dir),
                hierarchy=hierarchy,
                transformer_model=model_name,
                learning_rate=learning_rate,
                batch_size=self.training_config['batch_size'],
                max_epochs=self.training_config['max_epochs'],
                early_stopping=True,
                early_stopping_patience=self.training_config['patience'],
                max_sequence_length=self.training_config['max_seq_length'],
                devices=1,
                accelerator="gpu" if torch.cuda.is_available() else "cpu",
                precision="16-mixed" if torch.cuda.is_available() else 32,
                threads=0,
                verbose=True
            )
            
            # å‡†å¤‡éªŒè¯é›†
            val_size = min(200, len(test_texts) // 3)
            if val_size > 0:
                val_texts = test_texts[:val_size]
                val_labels = test_labels[:val_size]
                final_test_texts = test_texts[val_size:]
                final_test_labels = test_labels[val_size:]
            else:
                val_texts, val_labels = None, None
                final_test_texts = test_texts
                final_test_labels = test_labels
            
            print(f"   è®­ç»ƒæ ·æœ¬: {len(train_texts)}")
            print(f"   éªŒè¯æ ·æœ¬: {val_size if val_size > 0 else 0}")
            print(f"   æµ‹è¯•æ ·æœ¬: {len(final_test_texts)}")
            print(f"   å­¦ä¹ ç‡: {learning_rate}")
            
            # è®­ç»ƒ
            print(f"   ğŸ¯ å¼€å§‹è®­ç»ƒ...")
            classifier.fit(train_labels, train_texts, y_val=val_labels, X_val=val_texts)
            
            # é¢„æµ‹
            print(f"   ğŸ”® é¢„æµ‹ä¸­...")
            predictions_df = classifier.predict(final_test_texts, format='dataframe', top_k=5)
            
            # æå–top-ké¢„æµ‹ç”¨äºå±‚æ¬¡åŒ–åˆ†æ
            top_k_predictions = []
            for i in range(len(final_test_texts)):
                row_preds = []
                for k in range(1, 6):  # top-5
                    pred = predictions_df.iloc[i][f'class_{k}']
                    if pd.notna(pred):
                        row_preds.append(pred)
                top_k_predictions.append(row_preds)
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            y_true = final_test_labels
            y_pred = predictions_df['class_1'].tolist()
            
            accuracy = accuracy_score(y_true, y_pred)
            top_3_acc = sum(
                true_label in [predictions_df.iloc[i][f'class_{j}'] for j in range(1, 4)]
                for i, true_label in enumerate(y_true)
            ) / len(y_true)
            top_5_acc = sum(
                true_label in [predictions_df.iloc[i][f'class_{j}'] for j in range(1, 6)]
                for i, true_label in enumerate(y_true)
            ) / len(y_true)
            
            # è®¡ç®—å±‚æ¬¡åŒ–å‡†ç¡®ç‡
            hierarchical_results = self.calculate_hierarchical_accuracy(y_true, y_pred, top_k_predictions)
            
            # åˆ†æISCOçº§åˆ«åˆ†å¸ƒ
            test_level_stats = self.analyze_isco_levels(final_test_labels)
            
            training_time = time.time() - start_time
            
            result = {
                'model_name': model_name,
                'model_display_name': model_display_name,
                'model_description': self.model_info[model_name]['description'],
                'experiment_name': experiment_name,
                'train_samples': len(train_texts),
                'test_samples': len(final_test_texts),
                'learning_rate': learning_rate,
                'accuracy': accuracy,
                'top_3_accuracy': top_3_acc,
                'top_5_accuracy': top_5_acc,
                'training_time_minutes': training_time / 60,
                'status': 'success',
                'hierarchical_accuracy': hierarchical_results,
                'level_analysis': {
                    'training_distribution': level_analysis,
                    'test_distribution': test_level_stats
                }
            }
            
            print(f"âœ… {model_display_name} - {experiment_name} è®­ç»ƒå®Œæˆ!")
            print(f"   4çº§å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
            print(f"   3çº§å‡†ç¡®ç‡: {hierarchical_results['level_3_accuracy']:.4f}")
            print(f"   2çº§å‡†ç¡®ç‡: {hierarchical_results['level_2_accuracy']:.4f}")
            print(f"   1çº§å‡†ç¡®ç‡: {hierarchical_results['level_1_accuracy']:.4f}")
            print(f"   è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            detailed_results = pd.DataFrame({
                'true_label': y_true,
                'predicted_label': y_pred,
                'confidence': predictions_df['prob_1'].tolist(),
                'correct': [t == p for t, p in zip(y_true, y_pred)],
                'true_level_1': [label[:1] for label in y_true],
                'pred_level_1': [label[:1] for label in y_pred],
                'true_level_2': [label[:2] for label in y_true],
                'pred_level_2': [label[:2] for label in y_pred],
                'true_level_3': [label[:3] for label in y_true],
                'pred_level_3': [label[:3] for label in y_pred]
            })
            
            detailed_results.to_csv(
                results_dir / f"{safe_model_name}_{experiment_name.lower().replace(' ', '_')}_detailed.csv", 
                index=False, encoding='utf-8'
            )
            
            return result
            
        except Exception as e:
            print(f"âŒ {model_display_name} - {experiment_name} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'model_name': model_name,
                'model_display_name': model_display_name,
                'experiment_name': experiment_name,
                'accuracy': 0.0,
                'training_time_minutes': 0.0,
                'status': 'failed',
                'error': str(e)
            }

    def run_comprehensive_comparison(self):
        """è¿è¡Œå…¨é¢çš„å¤šæ¨¡å‹å¯¹æ¯”å®éªŒ"""
        print("ğŸ”¬ å¼€å§‹å¢å¼ºç‰ˆå¤šæ¨¡å‹æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®éªŒ")
        print("=" * 80)
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = Path("enhanced_multi_model_comparison_results")
        results_dir.mkdir(exist_ok=True)
        
        experiment_start_time = time.time()
        all_results = []
        data_stats = {}
        
        # åŠ è½½åŸå§‹å’Œå¢å¼ºæ•°æ®
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ•°æ®å‡†å¤‡é˜¶æ®µ")
        print(f"{'='*80}")
        
        original_texts, original_labels, original_stats = self.load_data(enable_augmentation=False)
        augmented_texts, augmented_labels, augmented_stats = self.load_data(enable_augmentation=True)
        
        data_stats['original'] = original_stats
        data_stats['augmented'] = augmented_stats
        
        # åˆ†æISCOçº§åˆ«åˆ†å¸ƒ
        original_level_analysis = self.analyze_isco_levels(original_labels)
        augmented_level_analysis = self.analyze_isco_levels(augmented_labels)
        
        print(f"\nğŸ“ˆ ISCOçº§åˆ«åˆ†å¸ƒå¯¹æ¯”:")
        for level in [1, 2, 3, 4]:
            orig_count = original_level_analysis[level]['unique_codes']
            aug_count = augmented_level_analysis[level]['unique_codes']
            print(f"   {level}çº§({self.isco_levels[level]}): åŸå§‹{orig_count} â†’ å¢å¼º{aug_count}")
        
        # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡ŒåŸå§‹vså¢å¼ºå¯¹æ¯”
        for model_idx, model_name in enumerate(self.test_models):
            model_display_name = self.model_info[model_name]['name']
            print(f"\n{'='*80}")
            print(f"ğŸ¤– æ¨¡å‹ {model_idx+1}/{len(self.test_models)}: {model_display_name}")
            print(f"   æ¨¡å‹ä»£ç : {model_name}")
            print(f"   æ¨¡å‹ç‰¹ç‚¹: {self.model_info[model_name]['description']}")
            print(f"   å‚æ•°é‡: {self.model_info[model_name]['params']}")
            print(f"{'='*80}")
            
            # åŸå§‹æ•°æ®å®éªŒ
            original_result = self.train_and_evaluate_model(
                model_name, original_texts, original_labels, "Original", results_dir
            )
            all_results.append(original_result)
            
            # å¢å¼ºæ•°æ®å®éªŒ
            augmented_result = self.train_and_evaluate_model(
                model_name, augmented_texts, augmented_labels, "Augmented", results_dir
            )
            all_results.append(augmented_result)
            
            # æ‰“å°å½“å‰æ¨¡å‹çš„å¯¹æ¯”ç»“æœ
            if original_result['status'] == 'success' and augmented_result['status'] == 'success':
                print(f"\nğŸ“Š {model_display_name} å¯¹æ¯”ç»“æœ:")
                print(f"   4çº§å‡†ç¡®ç‡: {original_result['accuracy']:.4f} â†’ {augmented_result['accuracy']:.4f} ({(augmented_result['accuracy']-original_result['accuracy'])*100:+.2f}%)")
                for level in [1, 2, 3]:
                    orig_acc = original_result['hierarchical_accuracy'][f'level_{level}_accuracy']
                    aug_acc = augmented_result['hierarchical_accuracy'][f'level_{level}_accuracy']
                    print(f"   {level}çº§å‡†ç¡®ç‡: {orig_acc:.4f} â†’ {aug_acc:.4f} ({(aug_acc-orig_acc)*100:+.2f}%)")
                
                training_time_diff = augmented_result['training_time_minutes'] - original_result['training_time_minutes']
                print(f"   è®­ç»ƒæ—¶é—´: {original_result['training_time_minutes']:.1f}åˆ† â†’ {augmented_result['training_time_minutes']:.1f}åˆ† ({training_time_diff:+.1f}åˆ†)")
            else:
                print(f"\nâš ï¸ {model_display_name} éƒ¨åˆ†å®éªŒå¤±è´¥")
                if original_result['status'] != 'success':
                    print(f"   åŸå§‹æ•°æ®å®éªŒå¤±è´¥: {original_result.get('error', 'Unknown error')}")
                if augmented_result['status'] != 'success':
                    print(f"   å¢å¼ºæ•°æ®å®éªŒå¤±è´¥: {augmented_result.get('error', 'Unknown error')}")
        total_time = time.time() - experiment_start_time
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_comprehensive_report(all_results, data_stats, total_time, results_dir)
        
        return all_results, results_dir

    def generate_comprehensive_report(self, results, data_stats, total_time, results_dir):
        """ç”Ÿæˆç»¼åˆå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\nğŸ“Š ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        
        # åˆ›å»ºç»“æœDataFrame
        results_df = pd.DataFrame(results)
        results_df.to_csv(results_dir / "comprehensive_results.csv", index=False, encoding='utf-8')
        
        # åˆ†ç»„åˆ†æç»“æœ
        successful_results = [r for r in results if r['status'] == 'success']
        
        if len(successful_results) == 0:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„å®éªŒç»“æœ")
            return
        
        # æŒ‰æ¨¡å‹å’Œå®éªŒç±»å‹åˆ†ç»„
        model_comparison = defaultdict(dict)
        for result in successful_results:
            model_name = result['model_name']
            exp_type = result['experiment_name']
            model_comparison[model_name][exp_type] = result
        
        # è®¡ç®—æ”¹è¿›æƒ…å†µ
        improvements = []
        level_improvements = {1: [], 2: [], 3: [], 4: []}
        
        for model_name, experiments in model_comparison.items():
            if 'Original' in experiments and 'Augmented' in experiments:
                orig = experiments['Original']
                aug = experiments['Augmented']
                
                # 4çº§å‡†ç¡®ç‡æ”¹è¿›
                improvement = aug['accuracy'] - orig['accuracy']
                improvements.append({
                    'model_name': model_name,
                    'original_accuracy': orig['accuracy'],
                    'augmented_accuracy': aug['accuracy'],
                    'improvement': improvement,
                    'improvement_percentage': (improvement / orig['accuracy'] * 100) if orig['accuracy'] > 0 else 0
                })
                
                # å„çº§åˆ«å‡†ç¡®ç‡æ”¹è¿›
                for level in [1, 2, 3, 4]:
                    orig_acc = orig['hierarchical_accuracy'][f'level_{level}_accuracy']
                    aug_acc = aug['hierarchical_accuracy'][f'level_{level}_accuracy']
                    level_improvements[level].append({
                        'model_name': model_name,
                        'original': orig_acc,
                        'augmented': aug_acc,
                        'improvement': aug_acc - orig_acc
                    })
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = {
            'experiment_info': {
                'experiment_type': 'Enhanced Multi-Model Data Augmentation Comparison',
                'timestamp': datetime.now().isoformat(),
                'total_time_hours': total_time / 3600,
                'models_tested': self.test_models,
                'max_samples': self.max_samples
            },
            'data_statistics': data_stats,
            'results': results,
            'model_improvements': improvements,
            'level_improvements': level_improvements,
            'summary_statistics': {
                'avg_improvement': np.mean([imp['improvement'] for imp in improvements]) if improvements else 0,
                'best_model': max(improvements, key=lambda x: x['improvement'])['model_name'] if improvements else None,
                'worst_model': min(improvements, key=lambda x: x['improvement'])['model_name'] if improvements else None
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(results_dir / "comprehensive_report.json", 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # ç”Ÿæˆå¯è§†åŒ–ç»“æœè¡¨æ ¼
        self.create_visualization_tables(model_comparison, results_dir)
        
        # æ‰“å°ç»¼åˆç»“æœ
        print(f"\nğŸ‰ å¢å¼ºç‰ˆå¤šæ¨¡å‹å¯¹æ¯”å®éªŒå®Œæˆ!")
        print(f"ğŸ“ˆ å®éªŒæ€»ç»“:")
        print(f"   æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
        print(f"   æµ‹è¯•æ¨¡å‹: {len(self.test_models)} ä¸ª")
        print(f"   æˆåŠŸå®éªŒ: {len(successful_results)}/{len(results)}")
        
        if improvements:
            print(f"\nğŸ† å„çº§åˆ«å‡†ç¡®ç‡æ”¹è¿›æ’å:")
            
            # 4çº§å‡†ç¡®ç‡æ’å
            improvements.sort(key=lambda x: x['improvement'], reverse=True)
            print(f"\n   4çº§å‡†ç¡®ç‡æ”¹è¿›:")
            for i, imp in enumerate(improvements, 1):
                model_short = imp['model_name'].split('/')[-1]
                print(f"     {i}. {model_short}: {imp['improvement']:.4f} ({imp['improvement_percentage']:+.2f}%)")
            
            # å„çº§åˆ«æ”¹è¿›æ’å
            for level in [1, 2, 3]:
                level_imps = level_improvements[level]
                level_imps.sort(key=lambda x: x['improvement'], reverse=True)
                print(f"\n   {level}çº§å‡†ç¡®ç‡æ”¹è¿›:")
                for i, imp in enumerate(level_imps, 1):
                    model_short = imp['model_name'].split('/')[-1]
                    improvement_pct = (imp['improvement'] / imp['original'] * 100) if imp['original'] > 0 else 0
                    print(f"     {i}. {model_short}: {imp['improvement']:.4f} ({improvement_pct:+.2f}%)")
            
            # æœ€ä½³æ¨¡å‹æ¨è
            best_model = report['summary_statistics']['best_model']
            avg_improvement = report['summary_statistics']['avg_improvement']
            print(f"\nğŸ¯ å®éªŒç»“è®º:")
            print(f"   æœ€ä½³æ”¹è¿›æ¨¡å‹: {best_model.split('/')[-1]}")
            print(f"   å¹³å‡å‡†ç¡®ç‡æå‡: {avg_improvement:.4f} ({avg_improvement*100:+.2f}%)")
            
            if avg_improvement > 0.01:
                print(f"   ğŸ‰ æ•°æ®å¢å¼ºæ•ˆæœæ˜¾è‘—ï¼")
            elif avg_improvement > 0:
                print(f"   âš–ï¸ æ•°æ®å¢å¼ºæ•ˆæœä¸­æ€§")
            else:
                print(f"   âš ï¸ æ•°æ®å¢å¼ºéœ€è¦ä¼˜åŒ–")
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {results_dir}")
        
        # åç»­åˆ†æå»ºè®®
        print(f"\nğŸ’¡ æ·±åº¦åˆ†æå»ºè®®:")
        print(f"   1. æŸ¥çœ‹ comprehensive_results.csv äº†è§£æ‰€æœ‰æ¨¡å‹è¯¦ç»†æŒ‡æ ‡")
        print(f"   2. æŸ¥çœ‹ model_comparison_table.csv äº†è§£æ¨¡å‹å¯¹æ¯”")
        print(f"   3. æŸ¥çœ‹ level_accuracy_comparison.csv äº†è§£å„çº§åˆ«å‡†ç¡®ç‡")
        print(f"   4. ä½¿ç”¨ create_visualizations() ç”Ÿæˆå›¾è¡¨")

    def create_visualization_tables(self, model_comparison, results_dir):
        """åˆ›å»ºå¯è§†åŒ–å¯¹æ¯”è¡¨æ ¼"""
        
        # æ¨¡å‹å¯¹æ¯”è¡¨
        comparison_data = []
        for model_name, experiments in model_comparison.items():
            if 'Original' in experiments and 'Augmented' in experiments:
                orig = experiments['Original']
                aug = experiments['Augmented']
                
                row = {
                    'Model': orig.get('model_display_name', model_name.split('/')[-1]),
                    'Model_Code': model_name,
                    'Description': orig.get('model_description', ''),
                    'Original_4Level_Acc': orig['accuracy'],
                    'Augmented_4Level_Acc': aug['accuracy'],
                    '4Level_Improvement': aug['accuracy'] - orig['accuracy'],
                    '4Level_Improvement_Pct': ((aug['accuracy'] - orig['accuracy']) / orig['accuracy'] * 100) if orig['accuracy'] > 0 else 0,
                    'Original_3Level_Acc': orig['hierarchical_accuracy']['level_3_accuracy'],
                    'Augmented_3Level_Acc': aug['hierarchical_accuracy']['level_3_accuracy'],
                    '3Level_Improvement': aug['hierarchical_accuracy']['level_3_accuracy'] - orig['hierarchical_accuracy']['level_3_accuracy'],
                    'Original_2Level_Acc': orig['hierarchical_accuracy']['level_2_accuracy'],
                    'Augmented_2Level_Acc': aug['hierarchical_accuracy']['level_2_accuracy'],
                    '2Level_Improvement': aug['hierarchical_accuracy']['level_2_accuracy'] - orig['hierarchical_accuracy']['level_2_accuracy'],
                    'Original_1Level_Acc': orig['hierarchical_accuracy']['level_1_accuracy'],
                    'Augmented_1Level_Acc': aug['hierarchical_accuracy']['level_1_accuracy'],
                    '1Level_Improvement': aug['hierarchical_accuracy']['level_1_accuracy'] - orig['hierarchical_accuracy']['level_1_accuracy'],
                    'Training_Time_Increase': aug['training_time_minutes'] - orig['training_time_minutes'],
                    'Original_LR': orig.get('learning_rate', 'N/A'),
                    'Augmented_LR': aug.get('learning_rate', 'N/A')
                }
                comparison_data.append(row)
        
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df.to_csv(results_dir / "model_comparison_table.csv", index=False)
        
        # çº§åˆ«å‡†ç¡®ç‡å¯¹æ¯”è¡¨
        level_data = []
        for model_name, experiments in model_comparison.items():
            if 'Original' in experiments and 'Augmented' in experiments:
                orig = experiments['Original']
                aug = experiments['Augmented']
                
                for level in [1, 2, 3, 4]:
                    level_data.append({
                        'Model': orig.get('model_display_name', model_name.split('/')[-1]),
                        'Model_Code': model_name,
                        'Level': level,
                        'Level_Name': self.isco_levels[level],
                        'Original_Accuracy': orig['hierarchical_accuracy'][f'level_{level}_accuracy'],
                        'Augmented_Accuracy': aug['hierarchical_accuracy'][f'level_{level}_accuracy'],
                        'Improvement': aug['hierarchical_accuracy'][f'level_{level}_accuracy'] - orig['hierarchical_accuracy'][f'level_{level}_accuracy'],
                        'Improvement_Percentage': ((aug['hierarchical_accuracy'][f'level_{level}_accuracy'] - orig['hierarchical_accuracy'][f'level_{level}_accuracy']) / orig['hierarchical_accuracy'][f'level_{level}_accuracy'] * 100) if orig['hierarchical_accuracy'][f'level_{level}_accuracy'] > 0 else 0
                    })
        
        level_df = pd.DataFrame(level_data)
        level_df.to_csv(results_dir / "level_accuracy_comparison.csv", index=False)
        
        print(f"âœ… å¯è§†åŒ–è¡¨æ ¼å·²ç”Ÿæˆ:")
        print(f"   - model_comparison_table.csv: æ¨¡å‹æ¨ªå‘å¯¹æ¯”")
        print(f"   - level_accuracy_comparison.csv: çº§åˆ«å‡†ç¡®ç‡è¯¦ç»†å¯¹æ¯”")

    def create_visualizations(self, results_dir):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        try:
            # è¯»å–å¯¹æ¯”æ•°æ®
            comparison_df = pd.read_csv(results_dir / "model_comparison_table.csv")
            level_df = pd.read_csv(results_dir / "level_accuracy_comparison.csv")
            
            # è®¾ç½®matplotlibä¸­æ–‡æ”¯æŒå’Œæ›´å¥½çš„å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'sans-serif']
            plt.rcParams['axes.unicode_minus'] = False
            plt.rcParams['figure.facecolor'] = 'white'
            plt.rcParams['axes.facecolor'] = 'white'
            
            # 1. æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”å›¾ - æ›´ç´§å‡‘çš„å¸ƒå±€
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            
            colors_original = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']
            colors_augmented = ['#2980b9', '#c0392b', '#27ae60', '#d68910', '#8e44ad']
            
            for i, level in enumerate([4, 3, 2, 1]):
                ax = axes[i//2, i%2]
                
                models = comparison_df['Model']
                original_acc = comparison_df[f'Original_{level}Level_Acc']
                augmented_acc = comparison_df[f'Augmented_{level}Level_Acc']
                
                x = np.arange(len(models))
                width = 0.35
                
                bars1 = ax.bar(x - width/2, original_acc, width, label='åŸå§‹æ•°æ®', 
                              color=colors_original[:len(models)], alpha=0.8, edgecolor='white', linewidth=1)
                bars2 = ax.bar(x + width/2, augmented_acc, width, label='æ•°æ®å¢å¼º', 
                              color=colors_augmented[:len(models)], alpha=0.8, edgecolor='white', linewidth=1)
                
                ax.set_ylabel('å‡†ç¡®ç‡', fontsize=12, fontweight='bold')
                ax.set_title(f'{level}çº§å‡†ç¡®ç‡å¯¹æ¯”\n{self.isco_levels[level]}', fontsize=14, fontweight='bold', pad=20)
                ax.set_xticks(x)
                
                # ç¼©çŸ­æ¨¡å‹åç§°æ˜¾ç¤º
                short_names = []
                for model in models:
                    if 'Google' in model:
                        short_names.append('Google BERT')
                    elif 'HFL' in model and 'RoBERTa' in model:
                        short_names.append('HFL RoBERTa')
                    elif 'HFL' in model and 'BERT' in model:
                        short_names.append('HFL BERT')
                    elif 'CKIP' in model:
                        short_names.append('CKIP BERT')
                    elif 'Multilingual' in model:
                        short_names.append('Multilingual')
                    else:
                        short_names.append(model)
                
                ax.set_xticklabels(short_names, rotation=45, ha='right', fontsize=10)
                ax.legend(fontsize=10, loc='upper left')
                ax.grid(True, alpha=0.3, axis='y')
                ax.set_ylim(0, max(max(original_acc), max(augmented_acc)) * 1.1)
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar in bars1:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
                
                for bar in bars2:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
                
                # æ·»åŠ æ”¹è¿›å¹…åº¦æ ‡æ³¨
                for j, (orig, aug) in enumerate(zip(original_acc, augmented_acc)):
                    improvement = aug - orig
                    if improvement > 0:
                        ax.annotate(f'+{improvement:.3f}', 
                                  xy=(j, max(orig, aug) + 0.02), 
                                  ha='center', va='bottom', 
                                  fontsize=8, color='green', fontweight='bold',
                                  bbox=dict(boxstyle='round,pad=0.2', facecolor='lightgreen', alpha=0.7))
                    elif improvement < 0:
                        ax.annotate(f'{improvement:.3f}', 
                                  xy=(j, max(orig, aug) + 0.02), 
                                  ha='center', va='bottom', 
                                  fontsize=8, color='red', fontweight='bold',
                                  bbox=dict(boxstyle='round,pad=0.2', facecolor='lightcoral', alpha=0.7))
            
            plt.suptitle('å„çº§åˆ«å‡†ç¡®ç‡å¯¹æ¯” - åŸå§‹æ•°æ® vs æ•°æ®å¢å¼º', fontsize=16, fontweight='bold', y=0.98)
            plt.tight_layout()
            plt.subplots_adjust(top=0.93)
            plt.savefig(results_dir / "model_accuracy_comparison.png", dpi=300, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
            plt.close()
            
            # 2. æ”¹è¿›å¹…åº¦çƒ­åŠ›å›¾ - å¢å¼ºç‰ˆ
            fig, ax = plt.subplots(figsize=(12, 8))
            
            improvement_matrix = []
            model_display_names = []
            level_names = ['1çº§\nä¸»è¦èŒä¸šç»„', '2çº§\næ¬¡è¦èŒä¸šç»„', '3çº§\næ¬¡çº§èŒä¸šç»„', '4çº§\nåŸºæœ¬èŒä¸šç»„']
            
            for _, row in comparison_df.iterrows():
                model_display_names.append(row['Model'])
                improvements = [
                    row['1Level_Improvement'],
                    row['2Level_Improvement'], 
                    row['3Level_Improvement'],
                    row['4Level_Improvement']
                ]
                improvement_matrix.append(improvements)
            
            improvement_matrix = np.array(improvement_matrix)
            
            # ä½¿ç”¨æ›´å¥½çš„é…è‰²æ–¹æ¡ˆ
            im = ax.imshow(improvement_matrix, cmap='RdYlGn', aspect='auto', vmin=-0.02, vmax=0.05)
            
            # è®¾ç½®æ ‡ç­¾
            ax.set_xticks(np.arange(len(level_names)))
            ax.set_yticks(np.arange(len(model_display_names)))
            ax.set_xticklabels(level_names, fontsize=12, fontweight='bold')
            ax.set_yticklabels(model_display_names, fontsize=12, fontweight='bold')
            
            # æ·»åŠ æ•°å€¼å’Œç™¾åˆ†æ¯”
            for i in range(len(model_display_names)):
                for j in range(len(level_names)):
                    improvement = improvement_matrix[i, j]
                    percentage = improvement * 100
                    text_color = 'white' if abs(improvement) > 0.02 else 'black'
                    ax.text(j, i, f'{improvement:.3f}\n({percentage:+.1f}%)',
                           ha="center", va="center", color=text_color, 
                           fontweight='bold', fontsize=10)
            
            ax.set_title("æ•°æ®å¢å¼ºæ”¹è¿›æ•ˆæœçƒ­åŠ›å›¾", fontsize=16, fontweight='bold', pad=20)
            
            # æ·»åŠ é¢œè‰²æ¡
            cbar = plt.colorbar(im, ax=ax, shrink=0.8)
            cbar.set_label('å‡†ç¡®ç‡æ”¹è¿›', fontsize=12, fontweight='bold')
            cbar.ax.tick_params(labelsize=10)
            
            plt.tight_layout()
            plt.savefig(results_dir / "improvement_heatmap.png", dpi=300, bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            plt.close()
            
            # 3. çº§åˆ«å‡†ç¡®ç‡è¶‹åŠ¿å›¾ - æ”¹è¿›ç‰ˆ
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
            
            # å·¦å›¾ï¼šåŸå§‹æ•°æ®è¶‹åŠ¿
            for i, model in enumerate(comparison_df['Model'].unique()):
                model_data = level_df[level_df['Model'] == model]
                levels = model_data['Level']
                original_acc = model_data['Original_Accuracy']
                
                ax1.plot(levels, original_acc, 'o-', label=model, linewidth=2.5, 
                        markersize=8, color=colors_original[i], alpha=0.8)
            
            ax1.set_xlabel('ISCO çº§åˆ«', fontsize=12, fontweight='bold')
            ax1.set_ylabel('å‡†ç¡®ç‡', fontsize=12, fontweight='bold')
            ax1.set_title('åŸå§‹æ•°æ® - å„çº§åˆ«å‡†ç¡®ç‡è¶‹åŠ¿', fontsize=14, fontweight='bold')
            ax1.set_xticks([1, 2, 3, 4])
            ax1.set_xticklabels(['1çº§\nä¸»è¦èŒä¸šç»„', '2çº§\næ¬¡è¦èŒä¸šç»„', '3çº§\næ¬¡çº§èŒä¸šç»„', '4çº§\nåŸºæœ¬èŒä¸šç»„'])
            ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
            ax1.grid(True, alpha=0.3)
            
            # å³å›¾ï¼šæ•°æ®å¢å¼ºè¶‹åŠ¿
            for i, model in enumerate(comparison_df['Model'].unique()):
                model_data = level_df[level_df['Model'] == model]
                levels = model_data['Level']
                augmented_acc = model_data['Augmented_Accuracy']
                
                ax2.plot(levels, augmented_acc, 's-', label=model, linewidth=2.5, 
                        markersize=8, color=colors_augmented[i], alpha=0.8)
            
            ax2.set_xlabel('ISCO çº§åˆ«', fontsize=12, fontweight='bold')
            ax2.set_ylabel('å‡†ç¡®ç‡', fontsize=12, fontweight='bold')
            ax2.set_title('æ•°æ®å¢å¼º - å„çº§åˆ«å‡†ç¡®ç‡è¶‹åŠ¿', fontsize=14, fontweight='bold')
            ax2.set_xticks([1, 2, 3, 4])
            ax2.set_xticklabels(['1çº§\nä¸»è¦èŒä¸šç»„', '2çº§\næ¬¡è¦èŒä¸šç»„', '3çº§\næ¬¡çº§èŒä¸šç»„', '4çº§\nåŸºæœ¬èŒä¸šç»„'])
            ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
            ax2.grid(True, alpha=0.3)
            
            plt.suptitle('å„çº§åˆ«å‡†ç¡®ç‡è¶‹åŠ¿å¯¹æ¯”', fontsize=16, fontweight='bold')
            plt.tight_layout()
            plt.subplots_adjust(top=0.93)
            plt.savefig(results_dir / "level_accuracy_trends.png", dpi=300, bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            plt.close()
            
            # 4. æ–°å¢ï¼šæ¨¡å‹æ’åå›¾
            fig, ax = plt.subplots(figsize=(14, 8))
            
            # è®¡ç®—å¹³å‡æ”¹è¿›
            avg_improvements = []
            model_names = []
            for _, row in comparison_df.iterrows():
                avg_imp = (row['1Level_Improvement'] + row['2Level_Improvement'] + 
                          row['3Level_Improvement'] + row['4Level_Improvement']) / 4
                avg_improvements.append(avg_imp)
                model_names.append(row['Model'])
            
            # æ’åº
            sorted_data = sorted(zip(model_names, avg_improvements), key=lambda x: x[1], reverse=True)
            sorted_models, sorted_improvements = zip(*sorted_data)
            
            colors = ['#2ecc71' if imp > 0 else '#e74c3c' for imp in sorted_improvements]
            bars = ax.barh(range(len(sorted_models)), sorted_improvements, color=colors, alpha=0.8)
            
            ax.set_yticks(range(len(sorted_models)))
            ax.set_yticklabels(sorted_models, fontsize=12, fontweight='bold')
            ax.set_xlabel('å¹³å‡å‡†ç¡®ç‡æ”¹è¿›', fontsize=12, fontweight='bold')
            ax.set_title('æ¨¡å‹æ•°æ®å¢å¼ºæ•ˆæœæ’å', fontsize=16, fontweight='bold', pad=20)
            ax.grid(True, alpha=0.3, axis='x')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (bar, imp) in enumerate(zip(bars, sorted_improvements)):
                width = bar.get_width()
                ax.text(width + (0.001 if width >= 0 else -0.001), bar.get_y() + bar.get_height()/2,
                       f'{imp:.4f} ({imp*100:+.2f}%)', 
                       ha='left' if width >= 0 else 'right', va='center', 
                       fontweight='bold', fontsize=11)
            
            plt.tight_layout()
            plt.savefig(results_dir / "model_ranking.png", dpi=300, bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            plt.close()
            
            print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ:")
            print(f"   - model_accuracy_comparison.png: å„çº§åˆ«å‡†ç¡®ç‡å¯¹æ¯” (å«æ”¹è¿›æ ‡æ³¨)")
            print(f"   - improvement_heatmap.png: æ”¹è¿›æ•ˆæœçƒ­åŠ›å›¾ (å«ç™¾åˆ†æ¯”)")
            print(f"   - level_accuracy_trends.png: çº§åˆ«å‡†ç¡®ç‡è¶‹åŠ¿å¯¹æ¯”")
            print(f"   - model_ranking.png: æ¨¡å‹æ•ˆæœæ’åå›¾")
            print(f"   å›¾è¡¨ç‰¹ç‚¹: ä¸­æ–‡æ”¯æŒã€é¢œè‰²åŒºåˆ†ã€æ•°å€¼æ ‡æ³¨ã€ä¸“ä¸šå¸ƒå±€")
            
        except Exception as e:
            print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
            print("è¯·æ£€æŸ¥æ˜¯å¦å®‰è£…äº†matplotlib: pip install matplotlib")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å¢å¼ºç‰ˆå¤šæ¨¡å‹æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®éªŒ")
    print("æ”¯æŒå¤šçº§åˆ«å‡†ç¡®ç‡åˆ†æå’Œå…¨é¢æ¨¡å‹å¯¹æ¯”")
    print("=" * 80)
    
    # é…ç½®å®éªŒ
    csv_path = "lunwenimpro/newjob1_sortall.csv"
    
    if not os.path.exists(csv_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {csv_path}")
        print("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹")
        return
    
    print("é€‰æ‹©å®éªŒè§„æ¨¡:")
    print("1. å¿«é€Ÿæµ‹è¯• (8Kæ ·æœ¬, 2ä¸ªæ¨¡å‹)")
    print("2. æ ‡å‡†å¯¹æ¯” (12Kæ ·æœ¬, å…¨éƒ¨æ¨¡å‹)")
    print("3. å®Œæ•´å¯¹æ¯” (æ— é™åˆ¶, å…¨éƒ¨æ¨¡å‹)")
    print("4. è‡ªå®šä¹‰é…ç½®")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1-4): ")
    
    if choice == "1":
        max_samples = 8000
        test_models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
    elif choice == "2":
        max_samples = 12000
        test_models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
    elif choice == "3":
        max_samples = None
        test_models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
    elif choice == "4":
        max_samples = int(input("æœ€å¤§æ ·æœ¬æ•° (ç•™ç©ºè¡¨ç¤ºæ— é™åˆ¶): ") or 0) or None
        print("é€‰æ‹©æµ‹è¯•æ¨¡å‹ (ç©ºæ ¼åˆ†éš”åºå·ï¼Œé»˜è®¤é€‰æ‹©æ‰€æœ‰):")
        models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
        model_names = [
            'Google Chinese BERT',
            'HFL Chinese BERT-wwm',
            'HFL Chinese RoBERTa', 
            'CKIP Chinese BERT',
            'BERT Multilingual'
        ]
        
        for i, (model, name) in enumerate(zip(models, model_names), 1):
            print(f"  {i}. {name} ({model})")
        
        selected = input("è¾“å…¥æ¨¡å‹åºå· (é»˜è®¤å…¨é€‰): ").split()
        if selected:
            test_models = [models[int(i)-1] for i in selected if i.isdigit() and 1 <= int(i) <= len(models)]
        else:
            test_models = models  # é»˜è®¤å…¨é€‰
        
        if not test_models:
            test_models = ['bert-base-chinese']
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨å¿«é€Ÿæµ‹è¯•")
        max_samples = 8000
        test_models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
    
    print(f"\nğŸ¯ å®éªŒé…ç½®:")
    print(f"   æ•°æ®æ–‡ä»¶: {csv_path}")
    print(f"   æ ·æœ¬é™åˆ¶: {max_samples if max_samples else 'æ— é™åˆ¶'}")
    print(f"   æµ‹è¯•æ¨¡å‹: {len(test_models)} ä¸ª")
    for i, model in enumerate(test_models, 1):
        print(f"     {i}. {model}")
    print(f"   å¯¹æ¯”ç»´åº¦: åŸå§‹æ•°æ® vs æ•°æ®å¢å¼º")
    print(f"   åˆ†æçº§åˆ«: ISCO 1-4çº§å±‚æ¬¡å‡†ç¡®ç‡")
    
    confirm = input(f"\nç¡®è®¤å¼€å§‹å¤šæ¨¡å‹å¯¹æ¯”å®éªŒ? (y/N): ")
    if confirm.lower() != 'y':
        print("å®éªŒå·²å–æ¶ˆ")
        return
    
    try:
        # åˆ›å»ºå®éªŒå¯¹è±¡
        experiment = EnhancedMultiModelComparison(csv_path, max_samples)
        experiment.test_models = test_models
        
        # è¿è¡Œå¯¹æ¯”å®éªŒ
        results, results_dir = experiment.run_comprehensive_comparison()
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        experiment.create_visualizations(results_dir)
        
        print(f"\nğŸ¯ å¤šæ¨¡å‹å¯¹æ¯”å®éªŒå®Œæˆ!")
        print(f"ğŸ“ æŸ¥çœ‹è¯¦ç»†ç»“æœ: {results_dir}")
        print(f"ğŸ” ä¸»è¦æ–‡ä»¶:")
        print(f"   - comprehensive_report.json: å®Œæ•´å®éªŒæŠ¥å‘Š")
        print(f"   - model_comparison_table.csv: æ¨¡å‹å¯¹æ¯”è¡¨æ ¼")
        print(f"   - level_accuracy_comparison.csv: çº§åˆ«å‡†ç¡®ç‡è¯¦æƒ…")
        print(f"   - *.png: å¯è§†åŒ–å›¾è¡¨")
        
    except Exception as e:
        print(f"âŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()