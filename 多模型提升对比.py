#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Â¢ûÂº∫ÁâàÂ§öÊ®°ÂûãÊï∞ÊçÆÂ¢ûÂº∫ÊïàÊûúÂØπÊØîÂÆûÈ™å
ÊîØÊåÅÂ§öÁ∫ßÂà´ÂáÜÁ°ÆÁéáÂàÜÊûêÂíåÂ§öÊ®°ÂûãÂØπÊØî
"""

import os
import time
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
import torch

warnings.filterwarnings('ignore')

# ÂØºÂÖ•ÂéüÊúâÊ®°Âùó
if __name__ == '__main__':
    # Ê∑ªÂä†Ë∑ØÂæÑ‰ª•‰æøÂØºÂÖ•lunwenimpro‰∏ãÁöÑÊ®°Âùó
    import sys
    sys.path.append('.')  # ÂΩìÂâçÁõÆÂΩï
    sys.path.append('lunwenimpro')  # lunwenimproÁõÆÂΩï
    
    from lunwenimpro.job_offers_classifier.job_offers_classfier import (
        ChineseTransformerJobOffersClassifier,
        get_recommended_chinese_models
    )
    from lunwenimpro.job_offers_classifier.job_offers_utils import create_hierarchy_node
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, accuracy_score
    
    # ÂØºÂÖ•Êï∞ÊçÆÂ¢ûÂº∫Ê®°Âùó - Á°Æ‰øùËøô‰∏™Êñá‰ª∂Âú®ÂΩìÂâçÁõÆÂΩï‰∏ã
    from chinese_job_data_augmentation import EnhancedJobDataProcessor
    from job_offers_classifier.job_offers_utils_old import create_hierarchy_node
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, accuracy_score
    
    # ÂØºÂÖ•Êï∞ÊçÆÂ¢ûÂº∫Ê®°Âùó
    from chinese_job_data_augmentation import EnhancedJobDataProcessor


class EnhancedMultiModelComparison:
    """Â¢ûÂº∫ÁâàÂ§öÊ®°ÂûãÊï∞ÊçÆÂ¢ûÂº∫ÊïàÊûúÂØπÊØîÂÆûÈ™å"""
    
    def __init__(self, csv_path: str, max_samples: int = 12000):
        self.csv_path = csv_path
        self.max_samples = max_samples
        
        # Â§öÊ®°ÂûãÈÖçÁΩÆ - ‰ΩøÁî®ÊîØÊåÅÁöÑ‰∏≠ÊñáÊ®°Âûã
        self.test_models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
        
        # Ê®°Âûã‰ø°ÊÅØÊò†Â∞Ñ
        self.model_info = {
            'bert-base-chinese': {
                'name': 'Google Chinese BERT',
                'description': 'GoogleÂÆòÊñπ‰∏≠ÊñáÊ®°Âûã',
                'params': '110M'
            },
            'hfl/chinese-bert-wwm-ext': {
                'name': 'HFL Chinese BERT-wwm',
                'description': 'ÂÖ®ËØçÊé©Á†ÅÈ¢ÑËÆ≠ÁªÉ',
                'params': '110M'
            },
            'hfl/chinese-roberta-wwm-ext': {
                'name': 'HFL Chinese RoBERTa',
                'description': 'RoBERTaÊû∂ÊûÑ‰ºòÂåñ',
                'params': '110M'
            },
            'ckiplab/bert-base-chinese': {
                'name': 'CKIP Chinese BERT',
                'description': 'Âè∞Êπæ‰∏≠Á†îÈô¢ÁâàÊú¨',
                'params': '110M'
            },
            'bert-base-multilingual-cased': {
                'name': 'BERT Multilingual',
                'description': 'Â§öËØ≠Ë®ÄÂü∫ÂáÜÊ®°Âûã',
                'params': '110M'
            }
        }
        
        # ËÆ≠ÁªÉÈÖçÁΩÆ
        self.training_config = {
            'max_epochs': 8,
            'patience': 4,
            'max_seq_length': 256,
            'batch_size': 16,
            'learning_rate': 2e-5
        }
        
        # ISCOÂ±ÇÁ∫ßÂÆö‰πâ
        self.isco_levels = {
            1: "‰∏ªË¶ÅËÅå‰∏öÁªÑ",
            2: "Ê¨°Ë¶ÅËÅå‰∏öÁªÑ", 
            3: "Ê¨°Á∫ßËÅå‰∏öÁªÑ",
            4: "Âü∫Êú¨ËÅå‰∏öÁªÑ"
        }
        
        print("üî¨ Â¢ûÂº∫ÁâàÂ§öÊ®°ÂûãÊï∞ÊçÆÂ¢ûÂº∫ÊïàÊûúÂØπÊØîÂÆûÈ™åÂàùÂßãÂåñ")
        print(f"   ÊµãËØïÊ®°ÂûãÊï∞Èáè: {len(self.test_models)}")
        print(f"   ÊúÄÂ§ßÊ†∑Êú¨Êï∞: {self.max_samples}")
        print(f"   ISCOÂ±ÇÁ∫ßÂàÜÊûê: {list(self.isco_levels.keys())}")

    def create_isco_hierarchy_from_codes(self, isco_codes):
        """‰ªéISCOÁºñÁ†ÅÂàõÂª∫Â±ÇÊ¨°ÁªìÊûÑ"""
        hierarchy = {}
        
        for code in isco_codes:
            code_str = str(code).zfill(4)
            
            for level in [1, 2, 3, 4]:
                level_code = code_str[:level]
                if level_code not in hierarchy:
                    hierarchy[level_code] = create_hierarchy_node(
                        level_code, 
                        f"ISCO-{level}‰Ωç-{level_code}"
                    )
        
        return hierarchy

    def load_data(self, enable_augmentation=False):
        """Âä†ËΩΩÊï∞ÊçÆ"""
        data_type = "Â¢ûÂº∫" if enable_augmentation else "ÂéüÂßã"
        print(f"\nüìä Âä†ËΩΩ{data_type}Êï∞ÊçÆ...")
        
        if enable_augmentation:
            # ‰ΩøÁî®Â¢ûÂº∫Êï∞ÊçÆ
            processor = EnhancedJobDataProcessor()
            texts, labels, processing_stats = processor.process_csv_data(
                csv_path=self.csv_path,
                enable_augmentation=True,
                balance_data=True,
                target_samples_per_class=8
            )
            
            # ÈôêÂà∂Ê†∑Êú¨Êï∞
            if self.max_samples and len(texts) > self.max_samples:
                indices = np.random.choice(len(texts), size=self.max_samples, replace=False)
                texts = [texts[i] for i in indices]
                labels = [labels[i] for i in indices]
            
            stats = processing_stats['final_stats']
            
        else:
            # ‰ΩøÁî®ÂéüÂßãÊï∞ÊçÆ
            try:
                df = pd.read_csv(self.csv_path, encoding='utf-8')
            except UnicodeDecodeError:
                df = pd.read_csv(self.csv_path, encoding='gbk')
            
            # Âü∫Á°ÄÊñáÊú¨ÁªÑÂêà
            def combine_features(row):
                parts = []
                for col in ['Â≤ó‰Ωç', 'Â≤ó‰ΩçÊèèËø∞', 'Â≤ó‰ΩçËÅåËÉΩ']:
                    if col in row and pd.notna(row[col]):
                        content = str(row[col])
                        if col == 'Â≤ó‰ΩçËÅåËÉΩ' and content.startswith('['):
                            try:
                                import ast
                                job_funcs = ast.literal_eval(content)
                                content = ' '.join(job_funcs) if isinstance(job_funcs, list) else content
                            except:
                                pass
                        parts.append(content)
                return ' '.join(parts)
            
            df['combined_text'] = df.apply(combine_features, axis=1)
            df['isco_code'] = df['ISCO_4_Digit_Code_Gemini'].astype(str).str.zfill(4)
            
            # ÁßªÈô§Á©∫ÊñáÊú¨
            df = df[df['combined_text'].str.strip() != ''].copy()
            
            # Êô∫ËÉΩÈááÊ†∑
            if self.max_samples and len(df) > self.max_samples:
                class_counts = df['isco_code'].value_counts()
                single_classes = class_counts[class_counts == 1].index.tolist()
                multi_classes = class_counts[class_counts > 1].index.tolist()
                
                sampled_data = []
                single_sample_data = df[df['isco_code'].isin(single_classes)]
                remaining_budget = self.max_samples - len(single_sample_data)
                
                if remaining_budget > 0:
                    sampled_data.append(single_sample_data)
                    multi_sample_data = df[df['isco_code'].isin(multi_classes)]
                    
                    if len(multi_sample_data) > remaining_budget:
                        try:
                            multi_sample_data = multi_sample_data.groupby('isco_code', group_keys=False).apply(
                                lambda x: x.sample(min(len(x), max(2, remaining_budget // len(multi_classes))), 
                                                 random_state=42)
                            ).reset_index(drop=True)
                            
                            if len(multi_sample_data) > remaining_budget:
                                multi_sample_data = multi_sample_data.sample(n=remaining_budget, random_state=42)
                        except ValueError:
                            multi_sample_data = multi_sample_data.sample(n=remaining_budget, random_state=42)
                    
                    sampled_data.append(multi_sample_data)
                else:
                    sampled_data.append(single_sample_data.sample(n=self.max_samples, random_state=42))
                
                df = pd.concat(sampled_data, ignore_index=True)
            
            texts = df['combined_text'].tolist()
            labels = df['isco_code'].tolist()
            
            # ËøáÊª§ÂçïÊ†∑Êú¨Á±ªÂà´
            final_class_counts = pd.Series(labels).value_counts()
            single_classes = final_class_counts[final_class_counts == 1].index.tolist()
            
            if len(single_classes) > len(final_class_counts) * 0.3:
                multi_classes = final_class_counts[final_class_counts > 1].index.tolist()
                filtered_data = [(text, label) for text, label in zip(texts, labels) if label in multi_classes]
                
                if len(filtered_data) > 100:
                    texts, labels = zip(*filtered_data)
                    texts, labels = list(texts), list(labels)
            
            # ÁªüËÆ°‰ø°ÊÅØ
            text_lengths = [len(text) for text in texts]
            stats = {
                'total_samples': len(texts),
                'unique_labels': len(set(labels)),
                'avg_text_length': np.mean(text_lengths),
                'avg_word_count': np.mean([len(text.split()) for text in texts]),
                'label_distribution': Counter(labels)
            }
        
        print(f"‚úÖ {data_type}Êï∞ÊçÆÂä†ËΩΩÂÆåÊàê:")
        print(f"   Ê†∑Êú¨Êï∞: {len(texts)}")
        print(f"   Á±ªÂà´Êï∞: {len(set(labels))}")
        print(f"   Âπ≥ÂùáÊñáÊú¨ÈïøÂ∫¶: {stats['avg_text_length']:.1f}")
        
        return texts, labels, stats

    def analyze_isco_levels(self, labels):
        """ÂàÜÊûêISCOÂêÑÁ∫ßÂà´ÂàÜÂ∏É"""
        level_stats = {}
        
        for level in [1, 2, 3, 4]:
            level_codes = [label[:level] for label in labels]
            level_unique = len(set(level_codes))
            level_stats[level] = {
                'unique_codes': level_unique,
                'description': self.isco_levels[level],
                'codes': list(set(level_codes))
            }
        
        return level_stats

    def calculate_hierarchical_accuracy(self, y_true, y_pred, top_k_preds=None):
        """ËÆ°ÁÆóÂ±ÇÊ¨°ÂåñÂáÜÁ°ÆÁéá"""
        results = {}
        
        # ÂêÑÁ∫ßÂà´ÂáÜÁ°ÆÁéá
        for level in [1, 2, 3, 4]:
            true_level = [label[:level] for label in y_true]
            pred_level = [label[:level] for label in y_pred]
            
            accuracy = accuracy_score(true_level, pred_level)
            results[f'level_{level}_accuracy'] = accuracy
            
            # Top-kÂ±ÇÊ¨°ÂåñÂáÜÁ°ÆÁéáÔºàÂ¶ÇÊûúÊúâtop_kÈ¢ÑÊµãÔºâ
            if top_k_preds is not None:
                top_k_level_acc = []
                for i, true_code in enumerate(true_level):
                    # Ê£ÄÊü•true_codeÊòØÂê¶Âú®top_kÈ¢ÑÊµãÁöÑ‰ªª‰Ωï‰∏Ä‰∏™ÁöÑÁõ∏Â∫îÁ∫ßÂà´‰∏≠
                    match_found = False
                    for pred_code in top_k_preds[i]:
                        if str(pred_code)[:level] == true_code:
                            match_found = True
                            break
                    top_k_level_acc.append(match_found)
                
                results[f'level_{level}_top5_accuracy'] = np.mean(top_k_level_acc)
        
        return results

    def safe_train_test_split_with_levels(self, texts, labels):
        """ËÄÉËôëÂ±ÇÊ¨°ÁªìÊûÑÁöÑÂÆâÂÖ®Êï∞ÊçÆÂàíÂàÜ"""
        print("   üìä ÂàÜÊûêÊï∞ÊçÆÂàÜÂ∏É...")
        label_counts = Counter(labels)
        
        # ÂàÜÊûêÂêÑÁ∫ßÂà´ÁöÑÂàÜÂ∏É
        level_analysis = {}
        for level in [1, 2, 3, 4]:
            level_codes = [label[:level] for label in labels]
            level_counts = Counter(level_codes)
            single_sample_classes = [code for code, count in level_counts.items() if count == 1]
            level_analysis[level] = {
                'total_classes': len(level_counts),
                'single_sample_classes': len(single_sample_classes),
                'multi_sample_classes': len(level_counts) - len(single_sample_classes)
            }
        
        # ‰ΩøÁî®4Á∫ßÁºñÁ†ÅËøõË°åÂàíÂàÜÔºàÊúÄÁªÜÁ≤íÂ∫¶Ôºâ
        single_sample_classes = [label for label, count in label_counts.items() if count == 1]
        multi_sample_classes = [label for label, count in label_counts.items() if count > 1]
        
        print(f"   4Á∫ßÁºñÁ†Å - ÂçïÊ†∑Êú¨Á±ªÂà´: {len(single_sample_classes)}, Â§öÊ†∑Êú¨Á±ªÂà´: {len(multi_sample_classes)}")
        
        train_indices = []
        test_indices = []
        
        # ÂçïÊ†∑Êú¨Á±ªÂà´ÂÖ®ÈÉ®ÊîæÂÖ•ËÆ≠ÁªÉÈõÜ
        for i, (text, label) in enumerate(zip(texts, labels)):
            if label in single_sample_classes:
                train_indices.append(i)
        
        # Â§öÊ†∑Êú¨Á±ªÂà´Ê≠£Â∏∏ÂàÜÂ±ÇÂàíÂàÜ
        if multi_sample_classes:
            multi_texts = []
            multi_labels = []
            multi_indices = []
            
            for i, (text, label) in enumerate(zip(texts, labels)):
                if label in multi_sample_classes:
                    multi_texts.append(text)
                    multi_labels.append(label)
                    multi_indices.append(i)
            
            if len(multi_texts) > 0:
                try:
                    multi_train_idx, multi_test_idx = train_test_split(
                        range(len(multi_texts)), 
                        test_size=0.2, 
                        random_state=42, 
                        stratify=multi_labels
                    )
                except ValueError:
                    # ÂàÜÂ±ÇÂ§±Ë¥•Ôºå‰ΩøÁî®ÈöèÊú∫ÂàíÂàÜ
                    multi_train_idx, multi_test_idx = train_test_split(
                        range(len(multi_texts)), 
                        test_size=0.2, 
                        random_state=42
                    )
                
                train_indices.extend([multi_indices[i] for i in multi_train_idx])
                test_indices.extend([multi_indices[i] for i in multi_test_idx])
        
        # ÊûÑÂª∫ÊúÄÁªàÁöÑËÆ≠ÁªÉÊµãËØïÈõÜ
        train_texts = [texts[i] for i in train_indices]
        train_labels = [labels[i] for i in train_indices]
        test_texts = [texts[i] for i in test_indices]
        test_labels = [labels[i] for i in test_indices]
        
        # È™åËØÅÊµãËØïÈõÜ
        if len(test_texts) == 0 and len(train_texts) >= 10:
            split_point = max(1, len(train_texts) // 5)
            test_texts = train_texts[-split_point:]
            test_labels = train_labels[-split_point:]
            train_texts = train_texts[:-split_point]
            train_labels = train_labels[:-split_point]
        
        print(f"   ÊúÄÁªàÂàíÂàÜ - ËÆ≠ÁªÉ: {len(train_texts)}, ÊµãËØï: {len(test_texts)}")
        
        return train_texts, test_texts, train_labels, test_labels, level_analysis

    def train_and_evaluate_model(self, model_name, texts, labels, experiment_name, results_dir):
        """ËÆ≠ÁªÉÂíåËØÑ‰º∞Âçï‰∏™Ê®°Âûã"""
        model_display_name = self.model_info[model_name]['name']
        print(f"\nü§ñ ÂºÄÂßãËÆ≠ÁªÉ: {model_display_name} ({model_name}) - {experiment_name}")
        
        # Êï∞ÊçÆÂàíÂàÜ
        train_texts, test_texts, train_labels, test_labels, level_analysis = self.safe_train_test_split_with_levels(texts, labels)
        
        if len(test_texts) < 5:
            print(f"   ‚ùå ÊµãËØïÈõÜÊ†∑Êú¨‰∏çË∂≥: {len(test_texts)}")
            return {
                'model_name': model_name,
                'model_display_name': model_display_name,
                'experiment_name': experiment_name,
                'status': 'failed',
                'error': 'Insufficient test data'
            }
        
        # ÂàõÂª∫Â±ÇÊ¨°ÁªìÊûÑ
        hierarchy = self.create_isco_hierarchy_from_codes(set(labels))
        
        # ÂàõÂª∫Ê®°ÂûãÁõÆÂΩï
        safe_model_name = model_name.replace('/', '_').replace('-', '_')
        model_dir = results_dir / f"model_{safe_model_name}_{experiment_name.lower().replace(' ', '_')}"
        
        start_time = time.time()
        
        try:
            # ‰∏∫‰∏çÂêåÊ®°Âûã‰ºòÂåñÂ≠¶‰π†Áéá
            learning_rate = self.training_config['learning_rate']
            if 'roberta' in model_name.lower():
                learning_rate = 2.5e-5  # RoBERTaÈÄöÂ∏∏ÈúÄË¶ÅÁ®çÈ´òÁöÑÂ≠¶‰π†Áéá
            elif 'multilingual' in model_name.lower():
                learning_rate = 1.5e-5  # Â§öËØ≠Ë®ÄÊ®°Âûã‰ΩøÁî®ËæÉ‰ΩéÂ≠¶‰π†Áéá
            
            # ÂàõÂª∫ÂàÜÁ±ªÂô®
            classifier = ChineseTransformerJobOffersClassifier(
                model_dir=str(model_dir),
                hierarchy=hierarchy,
                transformer_model=model_name,
                learning_rate=learning_rate,
                batch_size=self.training_config['batch_size'],
                max_epochs=self.training_config['max_epochs'],
                early_stopping=True,
                early_stopping_patience=self.training_config['patience'],
                max_sequence_length=self.training_config['max_seq_length'],
                devices=1,
                accelerator="gpu" if torch.cuda.is_available() else "cpu",
                precision="16-mixed" if torch.cuda.is_available() else 32,
                threads=0,
                verbose=True
            )
            
            # ÂáÜÂ§áÈ™åËØÅÈõÜ
            val_size = min(200, len(test_texts) // 3)
            if val_size > 0:
                val_texts = test_texts[:val_size]
                val_labels = test_labels[:val_size]
                final_test_texts = test_texts[val_size:]
                final_test_labels = test_labels[val_size:]
            else:
                val_texts, val_labels = None, None
                final_test_texts = test_texts
                final_test_labels = test_labels
            
            print(f"   ËÆ≠ÁªÉÊ†∑Êú¨: {len(train_texts)}")
            print(f"   È™åËØÅÊ†∑Êú¨: {val_size if val_size > 0 else 0}")
            print(f"   ÊµãËØïÊ†∑Êú¨: {len(final_test_texts)}")
            print(f"   Â≠¶‰π†Áéá: {learning_rate}")
            
            # ËÆ≠ÁªÉ
            print(f"   üéØ ÂºÄÂßãËÆ≠ÁªÉ...")
            classifier.fit(train_labels, train_texts, y_val=val_labels, X_val=val_texts)
            
            # È¢ÑÊµã
            print(f"   üîÆ È¢ÑÊµã‰∏≠...")
            predictions_df = classifier.predict(final_test_texts, format='dataframe', top_k=5)
            
            # ÊèêÂèñtop-kÈ¢ÑÊµãÁî®‰∫éÂ±ÇÊ¨°ÂåñÂàÜÊûê
            top_k_predictions = []
            for i in range(len(final_test_texts)):
                row_preds = []
                for k in range(1, 6):  # top-5
                    pred = predictions_df.iloc[i][f'class_{k}']
                    if pd.notna(pred):
                        row_preds.append(pred)
                top_k_predictions.append(row_preds)
            
            # ËÆ°ÁÆóÂü∫Êú¨ÊåáÊ†á
            y_true = final_test_labels
            y_pred = predictions_df['class_1'].tolist()
            
            accuracy = accuracy_score(y_true, y_pred)
            top_3_acc = sum(
                true_label in [predictions_df.iloc[i][f'class_{j}'] for j in range(1, 4)]
                for i, true_label in enumerate(y_true)
            ) / len(y_true)
            top_5_acc = sum(
                true_label in [predictions_df.iloc[i][f'class_{j}'] for j in range(1, 6)]
                for i, true_label in enumerate(y_true)
            ) / len(y_true)
            
            # ËÆ°ÁÆóÂ±ÇÊ¨°ÂåñÂáÜÁ°ÆÁéá
            hierarchical_results = self.calculate_hierarchical_accuracy(y_true, y_pred, top_k_predictions)
            
            # ÂàÜÊûêISCOÁ∫ßÂà´ÂàÜÂ∏É
            test_level_stats = self.analyze_isco_levels(final_test_labels)
            
            training_time = time.time() - start_time
            
            result = {
                'model_name': model_name,
                'model_display_name': model_display_name,
                'model_description': self.model_info[model_name]['description'],
                'experiment_name': experiment_name,
                'train_samples': len(train_texts),
                'test_samples': len(final_test_texts),
                'learning_rate': learning_rate,
                'accuracy': accuracy,
                'top_3_accuracy': top_3_acc,
                'top_5_accuracy': top_5_acc,
                'training_time_minutes': training_time / 60,
                'status': 'success',
                'hierarchical_accuracy': hierarchical_results,
                'level_analysis': {
                    'training_distribution': level_analysis,
                    'test_distribution': test_level_stats
                }
            }
            
            print(f"‚úÖ {model_display_name} - {experiment_name} ËÆ≠ÁªÉÂÆåÊàê!")
            print(f"   4Á∫ßÂáÜÁ°ÆÁéá: {accuracy:.4f} ({accuracy*100:.2f}%)")
            print(f"   3Á∫ßÂáÜÁ°ÆÁéá: {hierarchical_results['level_3_accuracy']:.4f}")
            print(f"   2Á∫ßÂáÜÁ°ÆÁéá: {hierarchical_results['level_2_accuracy']:.4f}")
            print(f"   1Á∫ßÂáÜÁ°ÆÁéá: {hierarchical_results['level_1_accuracy']:.4f}")
            print(f"   ËÆ≠ÁªÉÊó∂Èó¥: {training_time/60:.1f} ÂàÜÈíü")
            
            # ‰øùÂ≠òËØ¶ÁªÜÁªìÊûú
            detailed_results = pd.DataFrame({
                'true_label': y_true,
                'predicted_label': y_pred,
                'confidence': predictions_df['prob_1'].tolist(),
                'correct': [t == p for t, p in zip(y_true, y_pred)],
                'true_level_1': [label[:1] for label in y_true],
                'pred_level_1': [label[:1] for label in y_pred],
                'true_level_2': [label[:2] for label in y_true],
                'pred_level_2': [label[:2] for label in y_pred],
                'true_level_3': [label[:3] for label in y_true],
                'pred_level_3': [label[:3] for label in y_pred]
            })
            
            detailed_results.to_csv(
                results_dir / f"{safe_model_name}_{experiment_name.lower().replace(' ', '_')}_detailed.csv", 
                index=False, encoding='utf-8'
            )
            
            return result
            
        except Exception as e:
            print(f"‚ùå {model_display_name} - {experiment_name} ËÆ≠ÁªÉÂ§±Ë¥•: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'model_name': model_name,
                'model_display_name': model_display_name,
                'experiment_name': experiment_name,
                'accuracy': 0.0,
                'training_time_minutes': 0.0,
                'status': 'failed',
                'error': str(e)
            }

    def run_comprehensive_comparison(self):
        """ËøêË°åÂÖ®Èù¢ÁöÑÂ§öÊ®°ÂûãÂØπÊØîÂÆûÈ™å"""
        print("üî¨ ÂºÄÂßãÂ¢ûÂº∫ÁâàÂ§öÊ®°ÂûãÊï∞ÊçÆÂ¢ûÂº∫ÊïàÊûúÂØπÊØîÂÆûÈ™å")
        print("=" * 80)
        
        # ÂàõÂª∫ÁªìÊûúÁõÆÂΩï
        results_dir = Path("enhanced_multi_model_comparison_results")
        results_dir.mkdir(exist_ok=True)
        
        experiment_start_time = time.time()
        all_results = []
        data_stats = {}
        
        # Âä†ËΩΩÂéüÂßãÂíåÂ¢ûÂº∫Êï∞ÊçÆ
        print(f"\n{'='*80}")
        print(f"üìä Êï∞ÊçÆÂáÜÂ§áÈò∂ÊÆµ")
        print(f"{'='*80}")
        
        original_texts, original_labels, original_stats = self.load_data(enable_augmentation=False)
        augmented_texts, augmented_labels, augmented_stats = self.load_data(enable_augmentation=True)
        
        data_stats['original'] = original_stats
        data_stats['augmented'] = augmented_stats
        
        # ÂàÜÊûêISCOÁ∫ßÂà´ÂàÜÂ∏É
        original_level_analysis = self.analyze_isco_levels(original_labels)
        augmented_level_analysis = self.analyze_isco_levels(augmented_labels)
        
        print(f"\nüìà ISCOÁ∫ßÂà´ÂàÜÂ∏ÉÂØπÊØî:")
        for level in [1, 2, 3, 4]:
            orig_count = original_level_analysis[level]['unique_codes']
            aug_count = augmented_level_analysis[level]['unique_codes']
            print(f"   {level}Á∫ß({self.isco_levels[level]}): ÂéüÂßã{orig_count} ‚Üí Â¢ûÂº∫{aug_count}")
        
        # ÂØπÊØè‰∏™Ê®°ÂûãËøõË°åÂéüÂßãvsÂ¢ûÂº∫ÂØπÊØî
        for model_idx, model_name in enumerate(self.test_models):
            model_display_name = self.model_info[model_name]['name']
            print(f"\n{'='*80}")
            print(f"ü§ñ Ê®°Âûã {model_idx+1}/{len(self.test_models)}: {model_display_name}")
            print(f"   Ê®°Âûã‰ª£Á†Å: {model_name}")
            print(f"   Ê®°ÂûãÁâπÁÇπ: {self.model_info[model_name]['description']}")
            print(f"   ÂèÇÊï∞Èáè: {self.model_info[model_name]['params']}")
            print(f"{'='*80}")
            
            # ÂéüÂßãÊï∞ÊçÆÂÆûÈ™å
            original_result = self.train_and_evaluate_model(
                model_name, original_texts, original_labels, "Original", results_dir
            )
            all_results.append(original_result)
            
            # Â¢ûÂº∫Êï∞ÊçÆÂÆûÈ™å
            augmented_result = self.train_and_evaluate_model(
                model_name, augmented_texts, augmented_labels, "Augmented", results_dir
            )
            all_results.append(augmented_result)
            
            # ÊâìÂç∞ÂΩìÂâçÊ®°ÂûãÁöÑÂØπÊØîÁªìÊûú
            if original_result['status'] == 'success' and augmented_result['status'] == 'success':
                print(f"\nüìä {model_display_name} ÂØπÊØîÁªìÊûú:")
                print(f"   4Á∫ßÂáÜÁ°ÆÁéá: {original_result['accuracy']:.4f} ‚Üí {augmented_result['accuracy']:.4f} ({(augmented_result['accuracy']-original_result['accuracy'])*100:+.2f}%)")
                for level in [1, 2, 3]:
                    orig_acc = original_result['hierarchical_accuracy'][f'level_{level}_accuracy']
                    aug_acc = augmented_result['hierarchical_accuracy'][f'level_{level}_accuracy']
                    print(f"   {level}Á∫ßÂáÜÁ°ÆÁéá: {orig_acc:.4f} ‚Üí {aug_acc:.4f} ({(aug_acc-orig_acc)*100:+.2f}%)")
                
                training_time_diff = augmented_result['training_time_minutes'] - original_result['training_time_minutes']
                print(f"   ËÆ≠ÁªÉÊó∂Èó¥: {original_result['training_time_minutes']:.1f}ÂàÜ ‚Üí {augmented_result['training_time_minutes']:.1f}ÂàÜ ({training_time_diff:+.1f}ÂàÜ)")
            else:
                print(f"\n‚ö†Ô∏è {model_display_name} ÈÉ®ÂàÜÂÆûÈ™åÂ§±Ë¥•")
                if original_result['status'] != 'success':
                    print(f"   ÂéüÂßãÊï∞ÊçÆÂÆûÈ™åÂ§±Ë¥•: {original_result.get('error', 'Unknown error')}")
                if augmented_result['status'] != 'success':
                    print(f"   Â¢ûÂº∫Êï∞ÊçÆÂÆûÈ™åÂ§±Ë¥•: {augmented_result.get('error', 'Unknown error')}")
        total_time = time.time() - experiment_start_time
        
        # ÁîüÊàêÁªºÂêàÊä•Âëä
        self.generate_comprehensive_report(all_results, data_stats, total_time, results_dir)
        
        return all_results, results_dir

    def generate_comprehensive_report(self, results, data_stats, total_time, results_dir):
        """ÁîüÊàêÁªºÂêàÂØπÊØîÊä•Âëä"""
        print(f"\nüìä ÁîüÊàêÁªºÂêàÂàÜÊûêÊä•Âëä...")
        
        # ÂàõÂª∫ÁªìÊûúDataFrame
        results_df = pd.DataFrame(results)
        results_df.to_csv(results_dir / "comprehensive_results.csv", index=False, encoding='utf-8')
        
        # ÂàÜÁªÑÂàÜÊûêÁªìÊûú
        successful_results = [r for r in results if r['status'] == 'success']
        
        if len(successful_results) == 0:
            print("‚ùå Ê≤°ÊúâÊàêÂäüÁöÑÂÆûÈ™åÁªìÊûú")
            return
        
        # ÊåâÊ®°ÂûãÂíåÂÆûÈ™åÁ±ªÂûãÂàÜÁªÑ
        model_comparison = defaultdict(dict)
        for result in successful_results:
            model_name = result['model_name']
            exp_type = result['experiment_name']
            model_comparison[model_name][exp_type] = result
        
        # ËÆ°ÁÆóÊîπËøõÊÉÖÂÜµ
        improvements = []
        level_improvements = {1: [], 2: [], 3: [], 4: []}
        
        for model_name, experiments in model_comparison.items():
            if 'Original' in experiments and 'Augmented' in experiments:
                orig = experiments['Original']
                aug = experiments['Augmented']
                
                # 4Á∫ßÂáÜÁ°ÆÁéáÊîπËøõ
                improvement = aug['accuracy'] - orig['accuracy']
                improvements.append({
                    'model_name': model_name,
                    'original_accuracy': orig['accuracy'],
                    'augmented_accuracy': aug['accuracy'],
                    'improvement': improvement,
                    'improvement_percentage': (improvement / orig['accuracy'] * 100) if orig['accuracy'] > 0 else 0
                })
                
                # ÂêÑÁ∫ßÂà´ÂáÜÁ°ÆÁéáÊîπËøõ
                for level in [1, 2, 3, 4]:
                    orig_acc = orig['hierarchical_accuracy'][f'level_{level}_accuracy']
                    aug_acc = aug['hierarchical_accuracy'][f'level_{level}_accuracy']
                    level_improvements[level].append({
                        'model_name': model_name,
                        'original': orig_acc,
                        'augmented': aug_acc,
                        'improvement': aug_acc - orig_acc
                    })
        
        # ÁîüÊàêËØ¶ÁªÜÊä•Âëä
        report = {
            'experiment_info': {
                'experiment_type': 'Enhanced Multi-Model Data Augmentation Comparison',
                'timestamp': datetime.now().isoformat(),
                'total_time_hours': total_time / 3600,
                'models_tested': self.test_models,
                'max_samples': self.max_samples
            },
            'data_statistics': data_stats,
            'results': results,
            'model_improvements': improvements,
            'level_improvements': level_improvements,
            'summary_statistics': {
                'avg_improvement': np.mean([imp['improvement'] for imp in improvements]) if improvements else 0,
                'best_model': max(improvements, key=lambda x: x['improvement'])['model_name'] if improvements else None,
                'worst_model': min(improvements, key=lambda x: x['improvement'])['model_name'] if improvements else None
            }
        }
        
        # ‰øùÂ≠òÊä•Âëä
        with open(results_dir / "comprehensive_report.json", 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # ÁîüÊàêÂèØËßÜÂåñÁªìÊûúË°®Ê†º
        self.create_visualization_tables(model_comparison, results_dir)
        
        # ÊâìÂç∞ÁªºÂêàÁªìÊûú
        print(f"\nüéâ Â¢ûÂº∫ÁâàÂ§öÊ®°ÂûãÂØπÊØîÂÆûÈ™åÂÆåÊàê!")
        print(f"üìà ÂÆûÈ™åÊÄªÁªì:")
        print(f"   ÊÄªËÄóÊó∂: {total_time/3600:.2f} Â∞èÊó∂")
        print(f"   ÊµãËØïÊ®°Âûã: {len(self.test_models)} ‰∏™")
        print(f"   ÊàêÂäüÂÆûÈ™å: {len(successful_results)}/{len(results)}")
        
        if improvements:
            print(f"\nüèÜ ÂêÑÁ∫ßÂà´ÂáÜÁ°ÆÁéáÊîπËøõÊéíÂêç:")
            
            # 4Á∫ßÂáÜÁ°ÆÁéáÊéíÂêç
            improvements.sort(key=lambda x: x['improvement'], reverse=True)
            print(f"\n   4Á∫ßÂáÜÁ°ÆÁéáÊîπËøõ:")
            for i, imp in enumerate(improvements, 1):
                model_short = imp['model_name'].split('/')[-1]
                print(f"     {i}. {model_short}: {imp['improvement']:.4f} ({imp['improvement_percentage']:+.2f}%)")
            
            # ÂêÑÁ∫ßÂà´ÊîπËøõÊéíÂêç
            for level in [1, 2, 3]:
                level_imps = level_improvements[level]
                level_imps.sort(key=lambda x: x['improvement'], reverse=True)
                print(f"\n   {level}Á∫ßÂáÜÁ°ÆÁéáÊîπËøõ:")
                for i, imp in enumerate(level_imps, 1):
                    model_short = imp['model_name'].split('/')[-1]
                    improvement_pct = (imp['improvement'] / imp['original'] * 100) if imp['original'] > 0 else 0
                    print(f"     {i}. {model_short}: {imp['improvement']:.4f} ({improvement_pct:+.2f}%)")
            
            # ÊúÄ‰Ω≥Ê®°ÂûãÊé®Ëçê
            best_model = report['summary_statistics']['best_model']
            avg_improvement = report['summary_statistics']['avg_improvement']
            print(f"\nüéØ ÂÆûÈ™åÁªìËÆ∫:")
            print(f"   ÊúÄ‰Ω≥ÊîπËøõÊ®°Âûã: {best_model.split('/')[-1]}")
            print(f"   Âπ≥ÂùáÂáÜÁ°ÆÁéáÊèêÂçá: {avg_improvement:.4f} ({avg_improvement*100:+.2f}%)")
            
            if avg_improvement > 0.01:
                print(f"   üéâ Êï∞ÊçÆÂ¢ûÂº∫ÊïàÊûúÊòæËëóÔºÅ")
            elif avg_improvement > 0:
                print(f"   ‚öñÔ∏è Êï∞ÊçÆÂ¢ûÂº∫ÊïàÊûú‰∏≠ÊÄß")
            else:
                print(f"   ‚ö†Ô∏è Êï∞ÊçÆÂ¢ûÂº∫ÈúÄË¶Å‰ºòÂåñ")
        
        print(f"\nüìÅ ËØ¶ÁªÜÁªìÊûú‰øùÂ≠òÂú®: {results_dir}")
        
        # ÂêéÁª≠ÂàÜÊûêÂª∫ËÆÆ
        print(f"\nüí° Ê∑±Â∫¶ÂàÜÊûêÂª∫ËÆÆ:")
        print(f"   1. Êü•Áúã comprehensive_results.csv ‰∫ÜËß£ÊâÄÊúâÊ®°ÂûãËØ¶ÁªÜÊåáÊ†á")
        print(f"   2. Êü•Áúã model_comparison_table.csv ‰∫ÜËß£Ê®°ÂûãÂØπÊØî")
        print(f"   3. Êü•Áúã level_accuracy_comparison.csv ‰∫ÜËß£ÂêÑÁ∫ßÂà´ÂáÜÁ°ÆÁéá")
        print(f"   4. ‰ΩøÁî® create_visualizations() ÁîüÊàêÂõæË°®")

    def create_visualization_tables(self, model_comparison, results_dir):
        """ÂàõÂª∫ÂèØËßÜÂåñÂØπÊØîË°®Ê†º"""
        
        # Ê®°ÂûãÂØπÊØîË°®
        comparison_data = []
        for model_name, experiments in model_comparison.items():
            if 'Original' in experiments and 'Augmented' in experiments:
                orig = experiments['Original']
                aug = experiments['Augmented']
                
                row = {
                    'Model': orig.get('model_display_name', model_name.split('/')[-1]),
                    'Model_Code': model_name,
                    'Description': orig.get('model_description', ''),
                    'Original_4Level_Acc': orig['accuracy'],
                    'Augmented_4Level_Acc': aug['accuracy'],
                    '4Level_Improvement': aug['accuracy'] - orig['accuracy'],
                    '4Level_Improvement_Pct': ((aug['accuracy'] - orig['accuracy']) / orig['accuracy'] * 100) if orig['accuracy'] > 0 else 0,
                    'Original_3Level_Acc': orig['hierarchical_accuracy']['level_3_accuracy'],
                    'Augmented_3Level_Acc': aug['hierarchical_accuracy']['level_3_accuracy'],
                    '3Level_Improvement': aug['hierarchical_accuracy']['level_3_accuracy'] - orig['hierarchical_accuracy']['level_3_accuracy'],
                    'Original_2Level_Acc': orig['hierarchical_accuracy']['level_2_accuracy'],
                    'Augmented_2Level_Acc': aug['hierarchical_accuracy']['level_2_accuracy'],
                    '2Level_Improvement': aug['hierarchical_accuracy']['level_2_accuracy'] - orig['hierarchical_accuracy']['level_2_accuracy'],
                    'Original_1Level_Acc': orig['hierarchical_accuracy']['level_1_accuracy'],
                    'Augmented_1Level_Acc': aug['hierarchical_accuracy']['level_1_accuracy'],
                    '1Level_Improvement': aug['hierarchical_accuracy']['level_1_accuracy'] - orig['hierarchical_accuracy']['level_1_accuracy'],
                    'Training_Time_Increase': aug['training_time_minutes'] - orig['training_time_minutes'],
                    'Original_LR': orig.get('learning_rate', 'N/A'),
                    'Augmented_LR': aug.get('learning_rate', 'N/A')
                }
                comparison_data.append(row)
        
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df.to_csv(results_dir / "model_comparison_table.csv", index=False)
        
        # Á∫ßÂà´ÂáÜÁ°ÆÁéáÂØπÊØîË°®
        level_data = []
        for model_name, experiments in model_comparison.items():
            if 'Original' in experiments and 'Augmented' in experiments:
                orig = experiments['Original']
                aug = experiments['Augmented']
                
                for level in [1, 2, 3, 4]:
                    level_data.append({
                        'Model': orig.get('model_display_name', model_name.split('/')[-1]),
                        'Model_Code': model_name,
                        'Level': level,
                        'Level_Name': self.isco_levels[level],
                        'Original_Accuracy': orig['hierarchical_accuracy'][f'level_{level}_accuracy'],
                        'Augmented_Accuracy': aug['hierarchical_accuracy'][f'level_{level}_accuracy'],
                        'Improvement': aug['hierarchical_accuracy'][f'level_{level}_accuracy'] - orig['hierarchical_accuracy'][f'level_{level}_accuracy'],
                        'Improvement_Percentage': ((aug['hierarchical_accuracy'][f'level_{level}_accuracy'] - orig['hierarchical_accuracy'][f'level_{level}_accuracy']) / orig['hierarchical_accuracy'][f'level_{level}_accuracy'] * 100) if orig['hierarchical_accuracy'][f'level_{level}_accuracy'] > 0 else 0
                    })
        
        level_df = pd.DataFrame(level_data)
        level_df.to_csv(results_dir / "level_accuracy_comparison.csv", index=False)
        
        print(f"‚úÖ ÂèØËßÜÂåñË°®Ê†ºÂ∑≤ÁîüÊàê:")
        print(f"   - model_comparison_table.csv: Ê®°ÂûãÊ®™ÂêëÂØπÊØî")
        print(f"   - level_accuracy_comparison.csv: Á∫ßÂà´ÂáÜÁ°ÆÁéáËØ¶ÁªÜÂØπÊØî")

    def create_visualizations(self, results_dir):
        """ÂàõÂª∫ÂèØËßÜÂåñÂõæË°®"""
        try:
            # ËØªÂèñÂØπÊØîÊï∞ÊçÆ
            comparison_df = pd.read_csv(results_dir / "model_comparison_table.csv")
            level_df = pd.read_csv(results_dir / "level_accuracy_comparison.csv")
            
            # ËÆæÁΩÆmatplotlib‰∏≠ÊñáÊîØÊåÅÂíåÊõ¥Â•ΩÁöÑÂ≠ó‰Ωì
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'sans-serif']
            plt.rcParams['axes.unicode_minus'] = False
            plt.rcParams['figure.facecolor'] = 'white'
            plt.rcParams['axes.facecolor'] = 'white'
            
            # 1. Ê®°ÂûãÂáÜÁ°ÆÁéáÂØπÊØîÂõæ - Êõ¥Á¥ßÂáëÁöÑÂ∏ÉÂ±Ä
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            
            colors_original = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']
            colors_augmented = ['#2980b9', '#c0392b', '#27ae60', '#d68910', '#8e44ad']
            
            for i, level in enumerate([4, 3, 2, 1]):
                ax = axes[i//2, i%2]
                
                models = comparison_df['Model']
                original_acc = comparison_df[f'Original_{level}Level_Acc']
                augmented_acc = comparison_df[f'Augmented_{level}Level_Acc']
                
                x = np.arange(len(models))
                width = 0.35
                
                bars1 = ax.bar(x - width/2, original_acc, width, label='ÂéüÂßãÊï∞ÊçÆ', 
                              color=colors_original[:len(models)], alpha=0.8, edgecolor='white', linewidth=1)
                bars2 = ax.bar(x + width/2, augmented_acc, width, label='Êï∞ÊçÆÂ¢ûÂº∫', 
                              color=colors_augmented[:len(models)], alpha=0.8, edgecolor='white', linewidth=1)
                
                ax.set_ylabel('ÂáÜÁ°ÆÁéá', fontsize=12, fontweight='bold')
                ax.set_title(f'{level}Á∫ßÂáÜÁ°ÆÁéáÂØπÊØî\n{self.isco_levels[level]}', fontsize=14, fontweight='bold', pad=20)
                ax.set_xticks(x)
                
                # Áº©Áü≠Ê®°ÂûãÂêçÁß∞ÊòæÁ§∫
                short_names = []
                for model in models:
                    if 'Google' in model:
                        short_names.append('Google BERT')
                    elif 'HFL' in model and 'RoBERTa' in model:
                        short_names.append('HFL RoBERTa')
                    elif 'HFL' in model and 'BERT' in model:
                        short_names.append('HFL BERT')
                    elif 'CKIP' in model:
                        short_names.append('CKIP BERT')
                    elif 'Multilingual' in model:
                        short_names.append('Multilingual')
                    else:
                        short_names.append(model)
                
                ax.set_xticklabels(short_names, rotation=45, ha='right', fontsize=10)
                ax.legend(fontsize=10, loc='upper left')
                ax.grid(True, alpha=0.3, axis='y')
                ax.set_ylim(0, max(max(original_acc), max(augmented_acc)) * 1.1)
                
                # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
                for bar in bars1:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
                
                for bar in bars2:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
                
                # Ê∑ªÂä†ÊîπËøõÂπÖÂ∫¶Ê†áÊ≥®
                for j, (orig, aug) in enumerate(zip(original_acc, augmented_acc)):
                    improvement = aug - orig
                    if improvement > 0:
                        ax.annotate(f'+{improvement:.3f}', 
                                  xy=(j, max(orig, aug) + 0.02), 
                                  ha='center', va='bottom', 
                                  fontsize=8, color='green', fontweight='bold',
                                  bbox=dict(boxstyle='round,pad=0.2', facecolor='lightgreen', alpha=0.7))
                    elif improvement < 0:
                        ax.annotate(f'{improvement:.3f}', 
                                  xy=(j, max(orig, aug) + 0.02), 
                                  ha='center', va='bottom', 
                                  fontsize=8, color='red', fontweight='bold',
                                  bbox=dict(boxstyle='round,pad=0.2', facecolor='lightcoral', alpha=0.7))
            
            plt.suptitle('ÂêÑÁ∫ßÂà´ÂáÜÁ°ÆÁéáÂØπÊØî - ÂéüÂßãÊï∞ÊçÆ vs Êï∞ÊçÆÂ¢ûÂº∫', fontsize=16, fontweight='bold', y=0.98)
            plt.tight_layout()
            plt.subplots_adjust(top=0.93)
            plt.savefig(results_dir / "model_accuracy_comparison.png", dpi=300, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
            plt.close()
            
            # 2. ÊîπËøõÂπÖÂ∫¶ÁÉ≠ÂäõÂõæ - Â¢ûÂº∫Áâà
            fig, ax = plt.subplots(figsize=(12, 8))
            
            improvement_matrix = []
            model_display_names = []
            level_names = ['1Á∫ß\n‰∏ªË¶ÅËÅå‰∏öÁªÑ', '2Á∫ß\nÊ¨°Ë¶ÅËÅå‰∏öÁªÑ', '3Á∫ß\nÊ¨°Á∫ßËÅå‰∏öÁªÑ', '4Á∫ß\nÂü∫Êú¨ËÅå‰∏öÁªÑ']
            
            for _, row in comparison_df.iterrows():
                model_display_names.append(row['Model'])
                improvements = [
                    row['1Level_Improvement'],
                    row['2Level_Improvement'], 
                    row['3Level_Improvement'],
                    row['4Level_Improvement']
                ]
                improvement_matrix.append(improvements)
            
            improvement_matrix = np.array(improvement_matrix)
            
            # ‰ΩøÁî®Êõ¥Â•ΩÁöÑÈÖçËâ≤ÊñπÊ°à
            im = ax.imshow(improvement_matrix, cmap='RdYlGn', aspect='auto', vmin=-0.02, vmax=0.05)
            
            # ËÆæÁΩÆÊ†áÁ≠æ
            ax.set_xticks(np.arange(len(level_names)))
            ax.set_yticks(np.arange(len(model_display_names)))
            ax.set_xticklabels(level_names, fontsize=12, fontweight='bold')
            ax.set_yticklabels(model_display_names, fontsize=12, fontweight='bold')
            
            # Ê∑ªÂä†Êï∞ÂÄºÂíåÁôæÂàÜÊØî
            for i in range(len(model_display_names)):
                for j in range(len(level_names)):
                    improvement = improvement_matrix[i, j]
                    percentage = improvement * 100
                    text_color = 'white' if abs(improvement) > 0.02 else 'black'
                    ax.text(j, i, f'{improvement:.3f}\n({percentage:+.1f}%)',
                           ha="center", va="center", color=text_color, 
                           fontweight='bold', fontsize=10)
            
            ax.set_title("Êï∞ÊçÆÂ¢ûÂº∫ÊîπËøõÊïàÊûúÁÉ≠ÂäõÂõæ", fontsize=16, fontweight='bold', pad=20)
            
            # Ê∑ªÂä†È¢úËâ≤Êù°
            cbar = plt.colorbar(im, ax=ax, shrink=0.8)
            cbar.set_label('ÂáÜÁ°ÆÁéáÊîπËøõ', fontsize=12, fontweight='bold')
            cbar.ax.tick_params(labelsize=10)
            
            plt.tight_layout()
            plt.savefig(results_dir / "improvement_heatmap.png", dpi=300, bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            plt.close()
            
            # 3. Á∫ßÂà´ÂáÜÁ°ÆÁéáË∂ãÂäøÂõæ - ÊîπËøõÁâà
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
            
            # Â∑¶ÂõæÔºöÂéüÂßãÊï∞ÊçÆË∂ãÂäø
            for i, model in enumerate(comparison_df['Model'].unique()):
                model_data = level_df[level_df['Model'] == model]
                levels = model_data['Level']
                original_acc = model_data['Original_Accuracy']
                
                ax1.plot(levels, original_acc, 'o-', label=model, linewidth=2.5, 
                        markersize=8, color=colors_original[i], alpha=0.8)
            
            ax1.set_xlabel('ISCO Á∫ßÂà´', fontsize=12, fontweight='bold')
            ax1.set_ylabel('ÂáÜÁ°ÆÁéá', fontsize=12, fontweight='bold')
            ax1.set_title('ÂéüÂßãÊï∞ÊçÆ - ÂêÑÁ∫ßÂà´ÂáÜÁ°ÆÁéáË∂ãÂäø', fontsize=14, fontweight='bold')
            ax1.set_xticks([1, 2, 3, 4])
            ax1.set_xticklabels(['1Á∫ß\n‰∏ªË¶ÅËÅå‰∏öÁªÑ', '2Á∫ß\nÊ¨°Ë¶ÅËÅå‰∏öÁªÑ', '3Á∫ß\nÊ¨°Á∫ßËÅå‰∏öÁªÑ', '4Á∫ß\nÂü∫Êú¨ËÅå‰∏öÁªÑ'])
            ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
            ax1.grid(True, alpha=0.3)
            
            # Âè≥ÂõæÔºöÊï∞ÊçÆÂ¢ûÂº∫Ë∂ãÂäø
            for i, model in enumerate(comparison_df['Model'].unique()):
                model_data = level_df[level_df['Model'] == model]
                levels = model_data['Level']
                augmented_acc = model_data['Augmented_Accuracy']
                
                ax2.plot(levels, augmented_acc, 's-', label=model, linewidth=2.5, 
                        markersize=8, color=colors_augmented[i], alpha=0.8)
            
            ax2.set_xlabel('ISCO Á∫ßÂà´', fontsize=12, fontweight='bold')
            ax2.set_ylabel('ÂáÜÁ°ÆÁéá', fontsize=12, fontweight='bold')
            ax2.set_title('Êï∞ÊçÆÂ¢ûÂº∫ - ÂêÑÁ∫ßÂà´ÂáÜÁ°ÆÁéáË∂ãÂäø', fontsize=14, fontweight='bold')
            ax2.set_xticks([1, 2, 3, 4])
            ax2.set_xticklabels(['1Á∫ß\n‰∏ªË¶ÅËÅå‰∏öÁªÑ', '2Á∫ß\nÊ¨°Ë¶ÅËÅå‰∏öÁªÑ', '3Á∫ß\nÊ¨°Á∫ßËÅå‰∏öÁªÑ', '4Á∫ß\nÂü∫Êú¨ËÅå‰∏öÁªÑ'])
            ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
            ax2.grid(True, alpha=0.3)
            
            plt.suptitle('ÂêÑÁ∫ßÂà´ÂáÜÁ°ÆÁéáË∂ãÂäøÂØπÊØî', fontsize=16, fontweight='bold')
            plt.tight_layout()
            plt.subplots_adjust(top=0.93)
            plt.savefig(results_dir / "level_accuracy_trends.png", dpi=300, bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            plt.close()
            
            # 4. Êñ∞Â¢ûÔºöÊ®°ÂûãÊéíÂêçÂõæ
            fig, ax = plt.subplots(figsize=(14, 8))
            
            # ËÆ°ÁÆóÂπ≥ÂùáÊîπËøõ
            avg_improvements = []
            model_names = []
            for _, row in comparison_df.iterrows():
                avg_imp = (row['1Level_Improvement'] + row['2Level_Improvement'] + 
                          row['3Level_Improvement'] + row['4Level_Improvement']) / 4
                avg_improvements.append(avg_imp)
                model_names.append(row['Model'])
            
            # ÊéíÂ∫è
            sorted_data = sorted(zip(model_names, avg_improvements), key=lambda x: x[1], reverse=True)
            sorted_models, sorted_improvements = zip(*sorted_data)
            
            colors = ['#2ecc71' if imp > 0 else '#e74c3c' for imp in sorted_improvements]
            bars = ax.barh(range(len(sorted_models)), sorted_improvements, color=colors, alpha=0.8)
            
            ax.set_yticks(range(len(sorted_models)))
            ax.set_yticklabels(sorted_models, fontsize=12, fontweight='bold')
            ax.set_xlabel('Âπ≥ÂùáÂáÜÁ°ÆÁéáÊîπËøõ', fontsize=12, fontweight='bold')
            ax.set_title('Ê®°ÂûãÊï∞ÊçÆÂ¢ûÂº∫ÊïàÊûúÊéíÂêç', fontsize=16, fontweight='bold', pad=20)
            ax.grid(True, alpha=0.3, axis='x')
            
            # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
            for i, (bar, imp) in enumerate(zip(bars, sorted_improvements)):
                width = bar.get_width()
                ax.text(width + (0.001 if width >= 0 else -0.001), bar.get_y() + bar.get_height()/2,
                       f'{imp:.4f} ({imp*100:+.2f}%)', 
                       ha='left' if width >= 0 else 'right', va='center', 
                       fontweight='bold', fontsize=11)
            
            plt.tight_layout()
            plt.savefig(results_dir / "model_ranking.png", dpi=300, bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            plt.close()
            
            print(f"‚úÖ ÂèØËßÜÂåñÂõæË°®Â∑≤ÁîüÊàê:")
            print(f"   - model_accuracy_comparison.png: ÂêÑÁ∫ßÂà´ÂáÜÁ°ÆÁéáÂØπÊØî (Âê´ÊîπËøõÊ†áÊ≥®)")
            print(f"   - improvement_heatmap.png: ÊîπËøõÊïàÊûúÁÉ≠ÂäõÂõæ (Âê´ÁôæÂàÜÊØî)")
            print(f"   - level_accuracy_trends.png: Á∫ßÂà´ÂáÜÁ°ÆÁéáË∂ãÂäøÂØπÊØî")
            print(f"   - model_ranking.png: Ê®°ÂûãÊïàÊûúÊéíÂêçÂõæ")
            print(f"   ÂõæË°®ÁâπÁÇπ: ‰∏≠ÊñáÊîØÊåÅ„ÄÅÈ¢úËâ≤Âå∫ÂàÜ„ÄÅÊï∞ÂÄºÊ†áÊ≥®„ÄÅ‰∏ì‰∏öÂ∏ÉÂ±Ä")
            
        except Exception as e:
            print(f"‚ö†Ô∏è ÂõæË°®ÁîüÊàêÂ§±Ë¥•: {e}")
            print("ËØ∑Ê£ÄÊü•ÊòØÂê¶ÂÆâË£Ö‰∫Ümatplotlib: pip install matplotlib")
            import traceback
            traceback.print_exc()


def main():
    """‰∏ªÂáΩÊï∞"""
    print("üî¨ Â¢ûÂº∫ÁâàÂ§öÊ®°ÂûãÊï∞ÊçÆÂ¢ûÂº∫ÊïàÊûúÂØπÊØîÂÆûÈ™å")
    print("ÊîØÊåÅÂ§öÁ∫ßÂà´ÂáÜÁ°ÆÁéáÂàÜÊûêÂíåÂÖ®Èù¢Ê®°ÂûãÂØπÊØî")
    print("=" * 80)
    
    # ÈÖçÁΩÆÂÆûÈ™å
    csv_path = "lunwenimpro/newjob1_sortall.csv"
    
    if not os.path.exists(csv_path):
        print(f"‚ùå Êâæ‰∏çÂà∞Êï∞ÊçÆÊñá‰ª∂: {csv_path}")
        print("ËØ∑Á°Æ‰øùÊï∞ÊçÆÊñá‰ª∂Âú®ÂΩìÂâçÁõÆÂΩï‰∏ã")
        return
    
    print("ÈÄâÊã©ÂÆûÈ™åËßÑÊ®°:")
    print("1. Âø´ÈÄüÊµãËØï (8KÊ†∑Êú¨, 2‰∏™Ê®°Âûã)")
    print("2. Ê†áÂáÜÂØπÊØî (12KÊ†∑Êú¨, ÂÖ®ÈÉ®Ê®°Âûã)")
    print("3. ÂÆåÊï¥ÂØπÊØî (Êó†ÈôêÂà∂, ÂÖ®ÈÉ®Ê®°Âûã)")
    print("4. Ëá™ÂÆö‰πâÈÖçÁΩÆ")
    
    choice = input("ËØ∑ËæìÂÖ•ÈÄâÊã© (1-4): ")
    
    if choice == "1":
        max_samples = 8000
        test_models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
    elif choice == "2":
        max_samples = 12000
        test_models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
    elif choice == "3":
        max_samples = None
        test_models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
    elif choice == "4":
        max_samples = int(input("ÊúÄÂ§ßÊ†∑Êú¨Êï∞ (ÁïôÁ©∫Ë°®Á§∫Êó†ÈôêÂà∂): ") or 0) or None
        print("ÈÄâÊã©ÊµãËØïÊ®°Âûã (Á©∫Ê†ºÂàÜÈöîÂ∫èÂè∑ÔºåÈªòËÆ§ÈÄâÊã©ÊâÄÊúâ):")
        models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
        model_names = [
            'Google Chinese BERT',
            'HFL Chinese BERT-wwm',
            'HFL Chinese RoBERTa', 
            'CKIP Chinese BERT',
            'BERT Multilingual'
        ]
        
        for i, (model, name) in enumerate(zip(models, model_names), 1):
            print(f"  {i}. {name} ({model})")
        
        selected = input("ËæìÂÖ•Ê®°ÂûãÂ∫èÂè∑ (ÈªòËÆ§ÂÖ®ÈÄâ): ").split()
        if selected:
            test_models = [models[int(i)-1] for i in selected if i.isdigit() and 1 <= int(i) <= len(models)]
        else:
            test_models = models  # ÈªòËÆ§ÂÖ®ÈÄâ
        
        if not test_models:
            test_models = ['bert-base-chinese']
    else:
        print("Êó†ÊïàÈÄâÊã©Ôºå‰ΩøÁî®Âø´ÈÄüÊµãËØï")
        max_samples = 8000
        test_models = [
            'bert-base-chinese',                    # Google Chinese BERT
            'hfl/chinese-bert-wwm-ext',            # HFL Chinese BERT-wwm  
            'hfl/chinese-roberta-wwm-ext',         # HFL Chinese RoBERTa
            'ckiplab/bert-base-chinese',           # CKIP Chinese BERT
            'bert-base-multilingual-cased'        # BERT Multilingual
        ]
    
    print(f"\nüéØ ÂÆûÈ™åÈÖçÁΩÆ:")
    print(f"   Êï∞ÊçÆÊñá‰ª∂: {csv_path}")
    print(f"   Ê†∑Êú¨ÈôêÂà∂: {max_samples if max_samples else 'Êó†ÈôêÂà∂'}")
    print(f"   ÊµãËØïÊ®°Âûã: {len(test_models)} ‰∏™")
    for i, model in enumerate(test_models, 1):
        print(f"     {i}. {model}")
    print(f"   ÂØπÊØîÁª¥Â∫¶: ÂéüÂßãÊï∞ÊçÆ vs Êï∞ÊçÆÂ¢ûÂº∫")
    print(f"   ÂàÜÊûêÁ∫ßÂà´: ISCO 1-4Á∫ßÂ±ÇÊ¨°ÂáÜÁ°ÆÁéá")
    
    confirm = input(f"\nÁ°ÆËÆ§ÂºÄÂßãÂ§öÊ®°ÂûãÂØπÊØîÂÆûÈ™å? (y/N): ")
    if confirm.lower() != 'y':
        print("ÂÆûÈ™åÂ∑≤ÂèñÊ∂à")
        return
    
    try:
        # ÂàõÂª∫ÂÆûÈ™åÂØπË±°
        experiment = EnhancedMultiModelComparison(csv_path, max_samples)
        experiment.test_models = test_models
        
        # ËøêË°åÂØπÊØîÂÆûÈ™å
        results, results_dir = experiment.run_comprehensive_comparison()
        
        # ÁîüÊàêÂèØËßÜÂåñÂõæË°®
        print(f"\nüìä ÁîüÊàêÂèØËßÜÂåñÂõæË°®...")
        experiment.create_visualizations(results_dir)
        
        print(f"\nüéØ Â§öÊ®°ÂûãÂØπÊØîÂÆûÈ™åÂÆåÊàê!")
        print(f"üìÅ Êü•ÁúãËØ¶ÁªÜÁªìÊûú: {results_dir}")
        print(f"üîç ‰∏ªË¶ÅÊñá‰ª∂:")
        print(f"   - comprehensive_report.json: ÂÆåÊï¥ÂÆûÈ™åÊä•Âëä")
        print(f"   - model_comparison_table.csv: Ê®°ÂûãÂØπÊØîË°®Ê†º")
        print(f"   - level_accuracy_comparison.csv: Á∫ßÂà´ÂáÜÁ°ÆÁéáËØ¶ÊÉÖ")
        print(f"   - *.png: ÂèØËßÜÂåñÂõæË°®")
        
    except Exception as e:
        print(f"‚ùå ÂÆûÈ™åËøáÁ®ã‰∏≠Âá∫Áé∞ÈîôËØØ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()