#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆTransformeræ¨¡å—
é›†æˆå±‚æ¬¡åŒ–æŸå¤±å‡½æ•°å’Œå¤šä»»åŠ¡å­¦ä¹ 
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoConfig
import pytorch_lightning as pl
from typing import Dict, List, Optional, Tuple, Union
import numpy as np

# å¯¼å…¥åŸºç¡€æ¨¡å—
from job_offers_classifier.transformer_module import TransformerClassifier

# å¯¼å…¥å±‚æ¬¡åŒ–æŸå¤±å‡½æ•°
from hierarchical_loss import HierarchicalISCOLoss, HierarchicalMultitaskLoss


class EnhancedTextDataset(torch.utils.data.Dataset):
    """å¢å¼ºç‰ˆæ–‡æœ¬æ•°æ®é›†ï¼Œæ”¯æŒISCOç¼–ç """
    
    def __init__(self, texts, labels=None, isco_codes=None, num_labels=None, 
                 lazy_encode=True, labels_dense_vec=False):
        self.texts = texts
        self.labels = labels
        self.isco_codes = isco_codes  # åŸå§‹ISCOç¼–ç å­—ç¬¦ä¸²
        self.num_labels = num_labels
        self.lazy_encode = lazy_encode
        self.labels_dense_vec = labels_dense_vec
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        item = {'text': self.texts[idx]}
        
        if self.labels is not None:
            if self.labels_dense_vec:
                item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
            else:
                item['labels'] = self.labels[idx]
                
        if self.isco_codes is not None:
            item['isco_code'] = self.isco_codes[idx]
            
        return item


class HierarchicalClassificationHead(nn.Module):
    """å±‚æ¬¡åŒ–åˆ†ç±»å¤´ï¼Œæ”¯æŒå¤šçº§åˆ«è¾“å‡º"""
    
    def __init__(self, 
                 hidden_size: int,
                 hierarchy: Dict,
                 dropout_rate: float = 0.1):
        super().__init__()
        
        self.hierarchy = hierarchy
        self.dropout = nn.Dropout(dropout_rate)
        
        # æ„å»ºå„çº§åˆ«çš„æ˜ å°„
        self._build_level_mappings()
        
        # ä¸ºæ¯ä¸ªçº§åˆ«åˆ›å»ºåˆ†ç±»å¤´
        self.level_classifiers = nn.ModuleDict()
        
        for level in [1, 2, 3, 4]:
            num_classes = self.level_info[level]['num_classes']
            self.level_classifiers[str(level)] = nn.Linear(hidden_size, num_classes)
        
        print(f"âœ… å±‚æ¬¡åŒ–åˆ†ç±»å¤´åˆå§‹åŒ–å®Œæˆ")
        for level in [1, 2, 3, 4]:
            print(f"   Level {level}: {self.level_info[level]['num_classes']} classes")

    def _build_level_mappings(self):
        """æ„å»ºå„çº§åˆ«çš„ç±»åˆ«ä¿¡æ¯"""
        self.level_info = {}
        
        # è·å–æ‰€æœ‰4çº§ç¼–ç 
        level_4_codes = sorted([
            code for code, info in self.hierarchy.items() 
            if info['level'] == 4
        ])
        
        # ä¸ºæ¯ä¸ªçº§åˆ«æ„å»ºç±»åˆ«é›†åˆ
        for level in [1, 2, 3, 4]:
            level_codes = sorted(list(set([
                code[:level] for code in level_4_codes
            ])))
            
            self.level_info[level] = {
                'codes': level_codes,
                'code_to_idx': {code: idx for idx, code in enumerate(level_codes)},
                'idx_to_code': {idx: code for idx, code in enumerate(level_codes)},
                'num_classes': len(level_codes)
            }

    def forward(self, hidden_states: torch.Tensor) -> Dict[int, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            hidden_states: BERTè¾“å‡ºçš„éšè—çŠ¶æ€ [batch_size, hidden_size]
            
        Returns:
            logits_dict: å„çº§åˆ«çš„logits {1: tensor, 2: tensor, 3: tensor, 4: tensor}
        """
        # åº”ç”¨dropout
        hidden_states = self.dropout(hidden_states)
        
        # è®¡ç®—å„çº§åˆ«çš„logits
        logits_dict = {}
        for level in [1, 2, 3, 4]:
            logits_dict[level] = self.level_classifiers[str(level)](hidden_states)
        
        return logits_dict


class EnhancedTransformerClassifier(TransformerClassifier):
    """
    å¢å¼ºç‰ˆTransformeråˆ†ç±»å™¨
    æ”¯æŒå±‚æ¬¡åŒ–æŸå¤±å’Œå¤šä»»åŠ¡å­¦ä¹ 
    """
    
    def __init__(self,
                 model_name_or_path: str,
                 output_size: int,
                 isco_hierarchy: Dict = None,
                 use_hierarchical_loss: bool = True,
                 use_multitask_learning: bool = False,
                 hierarchical_loss_weights: Dict[int, float] = None,
                 task_weights: Dict[int, float] = None,
                 learning_rate: float = 2e-5,
                 adam_epsilon: float = 1e-8,
                 warmup_steps: int = 0,
                 weight_decay: float = 0.01,
                 train_batch_size: int = 16,
                 eval_batch_size: int = 16,
                 accumulate_grad_batches: int = 1,
                 max_seq_length: int = 512,
                 verbose: bool = True,
                 **kwargs):
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(
            model_name_or_path=model_name_or_path,
            output_size=output_size,
            learning_rate=learning_rate,
            adam_epsilon=adam_epsilon,
            warmup_steps=warmup_steps,
            weight_decay=weight_decay,
            train_batch_size=train_batch_size,
            eval_batch_size=eval_batch_size,
            accumulate_grad_batches=accumulate_grad_batches,
            max_seq_length=max_seq_length,
            verbose=verbose,
            **kwargs
        )
        
        # ä¿å­˜å±‚æ¬¡åŒ–é…ç½®
        self.isco_hierarchy = isco_hierarchy
        
        self.use_hierarchical_loss = use_hierarchical_loss and isco_hierarchy is not None
        self.use_multitask_learning = use_multitask_learning and isco_hierarchy is not None
        
        # æŸå¤±æƒé‡
        self.hierarchical_loss_weights = hierarchical_loss_weights
        self.task_weights = task_weights
        
        # æ›¿æ¢è¾“å‡ºå±‚
        if self.use_multitask_learning:
            # ç§»é™¤åŸæœ‰çš„è¾“å‡ºå±‚
            self.fc_head = None
            # åˆ›å»ºå±‚æ¬¡åŒ–åˆ†ç±»å¤´
            self.hierarchical_head = HierarchicalClassificationHead(
                hidden_size=self.transformer.config.hidden_size,
                hierarchy=self.isco_hierarchy,
                dropout_rate=0.1
            )
            
        # åˆ›å»ºæŸå¤±å‡½æ•°
        if self.use_multitask_learning:
            self.criterion = HierarchicalMultitaskLoss(
                hierarchy=self.isco_hierarchy,
                task_weights=self.task_weights,
                level_weights=self.hierarchical_loss_weights
            )
        elif self.use_hierarchical_loss:
            self.criterion = HierarchicalISCOLoss(
                hierarchy=self.isco_hierarchy,
                level_weights=self.hierarchical_loss_weights
            )
        else:
            self.criterion = nn.CrossEntropyLoss()
        
        # ç”¨äºè®°å½•å„çº§åˆ«çš„å‡†ç¡®ç‡
        self.level_correct = {1: 0, 2: 0, 3: 0, 4: 0}
        self.level_total = {1: 0, 2: 0, 3: 0, 4: 0}
        
        if self.verbose:
            print(f"âœ… å¢å¼ºç‰ˆTransformeråˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ")
            print(f"   å±‚æ¬¡åŒ–æŸå¤±: {'å¯ç”¨' if self.use_hierarchical_loss else 'ç¦ç”¨'}")
            print(f"   å¤šä»»åŠ¡å­¦ä¹ : {'å¯ç”¨' if self.use_multitask_learning else 'ç¦ç”¨'}")

    # def forward(self, input_ids, attention_mask, token_type_ids=None):
    #     """å‰å‘ä¼ æ’­"""
    #     # è·å–BERTè¾“å‡º
    #     outputs = self.transformer(
    #         input_ids=input_ids,
    #         attention_mask=attention_mask,
    #         token_type_ids=token_type_ids
    #     )
        
    #     # ä½¿ç”¨CLS tokençš„è¡¨ç¤º
    #     pooled_output = outputs.last_hidden_state[:, 0]
        
    #     if self.use_multitask_learning:
    #         # å¤šä»»åŠ¡è¾“å‡º
    #         logits_dict = self.hierarchical_head(pooled_output)
    #         return logits_dict
    #     else:
    #         # å•ä»»åŠ¡è¾“å‡º
    #         pooled_output = self.dropout(pooled_output)
    #         logits = self.fc_head(pooled_output)
    #         return logits

    def forward(self, batch, labels=None):
        
        transformer_output = self.transformer(batch['input_ids'], attention_mask=batch['attention_mask'])
        # ä½¿ç”¨[CLS] tokençš„è¾“å‡º
        transformer_output = transformer_output.last_hidden_state[:, 0, :]
        
        if self.use_multitask_learning:
            return self.hierarchical_head(transformer_output)
        elif self.fc_head is not None:
            output_ = self.fc_head(transformer_output, labels=labels)
            return output_
        else:
            print("Fault in EnhancedTransformerClassifier Forward")
            return transformer_output

    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        if batch['labels'] is not None:  # Windows fix
            batch['labels'] = batch['labels'].type(torch.LongTensor)
            batch['labels'] = batch['labels'].to(self.device)
        
        # å‰å‘ä¼ æ’­
        outputs = self.forward(batch)
        
        # è®¡ç®—æŸå¤±
        if self.use_multitask_learning:
            loss, loss_dict = self.criterion(outputs, batch['labels'])
            
            # è®°å½•å„çº§åˆ«æŸå¤±
            for level in [1, 2, 3, 4]:
                if f'level_{level}_loss' in loss_dict:
                    self.log(f'train_level_{level}_loss', loss_dict[f'level_{level}_loss'], 
                            on_step=True, on_epoch=True, prog_bar=False)
        else:
            loss = self.criterion(outputs, batch['labels'])
        
        # è®°å½•æ€»æŸå¤±
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        
        return loss

    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤"""
        # è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡
        if self.use_multitask_learning:
            # # è¿™é‡Œæœªå®ç°
            # pass
            
            # if batch['labels'] is not None:  # Windows fix
            #     batch['labels'] = batch['labels'].type(torch.LongTensor)
            #     batch['labels'] = batch['labels'].to(self.device)
            # labels = batch['labels']
        
            # # å‰å‘ä¼ æ’­
            # outputs = self.forward(batch)
            
            # loss, loss_dict = self.criterion(outputs, labels)
            
            # # è®¡ç®—å„çº§åˆ«å‡†ç¡®ç‡
            # self._calculate_hierarchical_accuracy(outputs, labels)
            
            # # è®°å½•æŸå¤±
            # self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)
            
            # predictions = torch.argmax(outputs[4], dim=1)
            # return loss
        
            if batch['labels'] is not None:  # Windows fix
                batch['labels'] = batch['labels'].type(torch.LongTensor)
                batch['labels'] = batch['labels'].to(self.device)

            outputs = self.forward(batch)
            loss, loss_dict = self.criterion(outputs, batch['labels'])
            scores = F.softmax(outputs[4], dim=-1)
            
            if self.metrics is None:
                return loss
            
            # å¤„ç†å±‚æ¬¡åŒ–é¢„æµ‹
            if self.hparams.labels_paths is not None:
                new_scores = torch.zeros((scores.shape[0], self.eval_num_labels)).to(self.device)
                for i, path in enumerate(self.hparams.labels_paths):
                    path_score = scores[:, path]
                    new_scores[:, i] = path_score.prod(axis=1)
                scores = new_scores
                assert scores.shape == (scores.shape[0], self.eval_num_labels)
                true_loss = F.cross_entropy(scores, batch['labels'])
                self.log(f'val_true_loss', true_loss, on_epoch=True, logger=True)

            # ä¿®å¤ï¼šåˆ†åˆ«è®°å½•æ¯ä¸ªæŒ‡æ ‡ï¼Œè€Œä¸æ˜¯è®°å½•æ•´ä¸ªå­—å…¸
            try:
                metrics_dict = self.metrics(scores, batch['labels'])
                for metric_name, metric_value in metrics_dict.items():
                    self.log(f'val_{metric_name}', metric_value, on_epoch=True, logger=True)
            except Exception as e:
                # å¦‚æœæŒ‡æ ‡è®¡ç®—å¤±è´¥ï¼Œåªè®°å½•æŸå¤±
                if self.hparams.verbose:
                    print(f"Warning: Metrics calculation failed: {e}")
            
            self.log(f'val_loss', loss, on_epoch=True, prog_bar=True, logger=True)
            
            return loss
        
        else:
            return self._eval_step(batch, eval_name='val')

    def validation_epoch_end(self, outputs):
        """éªŒè¯epochç»“æŸæ—¶çš„å¤„ç†"""
        if self.use_multitask_learning and sum(self.level_total.values()) > 0:
            # è®¡ç®—å¹¶è®°å½•å„çº§åˆ«å‡†ç¡®ç‡
            for level in [1, 2, 3, 4]:
                if self.level_total[level] > 0:
                    accuracy = self.level_correct[level] / self.level_total[level]
                    self.log(f'val_level_{level}_accuracy', accuracy, on_epoch=True)
            
            # é‡ç½®è®¡æ•°å™¨
            self.level_correct = {1: 0, 2: 0, 3: 0, 4: 0}
            self.level_total = {1: 0, 2: 0, 3: 0, 4: 0}

    def _calculate_hierarchical_accuracy(self, logits_dict: Dict[int, torch.Tensor], 
                                        targets: torch.Tensor):
        """è®¡ç®—å„çº§åˆ«çš„å‡†ç¡®ç‡"""
        if not hasattr(self.criterion, 'hierarchical_loss'):
            return
        
        # è·å–ç›®æ ‡ISCOç¼–ç 
        target_codes = []
        for idx in targets:
            if hasattr(self.criterion.hierarchical_loss, 'idx_to_code'):
                target_codes.append(self.criterion.hierarchical_loss.idx_to_code[idx.item()])
            else:
                # å¦‚æœæ²¡æœ‰æ˜ å°„ï¼Œè·³è¿‡
                return
        
        # è®¡ç®—å„çº§åˆ«å‡†ç¡®ç‡
        for level in [1, 2, 3, 4]:
            if level in logits_dict:
                predictions = torch.argmax(logits_dict[level], dim=1)
                
                for i, pred_idx in enumerate(predictions):
                    # è·å–é¢„æµ‹çš„ç¼–ç 
                    if hasattr(self.hierarchical_head, 'level_info'):
                        pred_code = self.hierarchical_head.level_info[level]['idx_to_code'][pred_idx.item()]
                        true_code = target_codes[i][:level]
                        
                        self.level_total[level] += 1
                        if pred_code == true_code:
                            self.level_correct[level] += 1

    def predict_step(self, batch, batch_idx):
        """é¢„æµ‹æ­¥éª¤"""
        # input_ids = batch['input_ids']
        # attention_mask = batch['attention_mask']
        # token_type_ids = batch.get('token_type_ids', None)
        
        # å‰å‘ä¼ æ’­
        outputs = self.forward(batch)
        
        if self.use_multitask_learning:
            # è¿”å›4çº§é¢„æµ‹çš„æ¦‚ç‡
            logits = outputs[4]
        else:
            if isinstance(outputs, dict):
                logits = outputs[4]
            else:
                logits = outputs
        
        # è¿”å›softmaxæ¦‚ç‡
        probs = F.softmax(logits, dim=-1)
        return probs

    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨ï¼Œæ”¯æŒä¸åŒå­¦ä¹ ç‡"""
        # åˆ†ç»„å‚æ•°
        no_decay = ['bias', 'LayerNorm.weight']
        
        # åŸºç¡€å‚æ•°ç»„
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in self.transformer.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                'weight_decay': self.hparams.weight_decay,
                'lr': self.hparams.learning_rate
            },
            {
                'params': [p for n, p in self.transformer.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0,
                'lr': self.hparams.learning_rate
            }
        ]
        
        # å¦‚æœä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ ï¼Œä¸ºåˆ†ç±»å¤´æ·»åŠ æ›´é«˜çš„å­¦ä¹ ç‡
        if self.use_multitask_learning:
            classifier_params = []
            for level_classifier in self.hierarchical_head.level_classifiers.values():
                classifier_params.extend(list(level_classifier.parameters()))
            
            optimizer_grouped_parameters.append({
                'params': classifier_params,
                'weight_decay': self.hparams.weight_decay,
                'lr': self.hparams.learning_rate * 2  # åˆ†ç±»å¤´ä½¿ç”¨2å€å­¦ä¹ ç‡
            })
        elif hasattr(self, 'fc_head') and self.fc_head is not None:
            optimizer_grouped_parameters.append({
                'params': self.fc_head.parameters(),
                'weight_decay': self.hparams.weight_decay,
                'lr': self.hparams.learning_rate * 2
            })
        
        optimizer = torch.optim.AdamW(
            optimizer_grouped_parameters,
            eps=self.hparams.adam_epsilon
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = self._get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=self.hparams.warmup_steps,
            num_training_steps=self.trainer.estimated_stepping_batches
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'interval': 'step',
                'frequency': 1
            }
        }

    def _get_linear_schedule_with_warmup(self, optimizer, num_warmup_steps, num_training_steps):
        """çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        def lr_lambda(current_step):
            if current_step < num_warmup_steps:
                return float(current_step) / float(max(1, num_warmup_steps))
            return max(
                0.0,
                float(num_training_steps - current_step) / 
                float(max(1, num_training_steps - num_warmup_steps))
            )
        
        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    def get_hierarchical_predictions(self, texts: List[str], top_k: int = 5) -> Dict:
        """
        è·å–å±‚æ¬¡åŒ–é¢„æµ‹ç»“æœ
        
        Args:
            texts: è¾“å…¥æ–‡æœ¬åˆ—è¡¨
            top_k: æ¯ä¸ªçº§åˆ«è¿”å›çš„top-ké¢„æµ‹
            
        Returns:
            predictions: åŒ…å«å„çº§åˆ«é¢„æµ‹çš„å­—å…¸
        """
        self.eval()
        
        # å‡†å¤‡æ•°æ®
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(self.hparams.model_name_or_path)
        
        all_predictions = {1: [], 2: [], 3: [], 4: []}
        
        with torch.no_grad():
            for text in texts:
                # ç¼–ç æ–‡æœ¬
                inputs = tokenizer(
                    text,
                    padding='max_length',
                    truncation=True,
                    max_length=self.hparams.max_seq_length,
                    return_tensors='pt'
                )
                
                # ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # å‰å‘ä¼ æ’­
                outputs = self.forward(**inputs)
                
                if self.use_multitask_learning:
                    # å¤„ç†å¤šä»»åŠ¡è¾“å‡º
                    for level in [1, 2, 3, 4]:
                        if level in outputs:
                            probs = F.softmax(outputs[level], dim=-1)
                            top_k_probs, top_k_indices = torch.topk(probs[0], k=min(top_k, probs.shape[1]))
                            
                            # è½¬æ¢ä¸ºç¼–ç 
                            predictions = []
                            for idx, prob in zip(top_k_indices, top_k_probs):
                                code = self.hierarchical_head.level_info[level]['idx_to_code'][idx.item()]
                                predictions.append({
                                    'code': code,
                                    'probability': prob.item()
                                })
                            
                            all_predictions[level].append(predictions)
                else:
                    # å•ä»»åŠ¡è¾“å‡ºï¼Œåªæœ‰4çº§é¢„æµ‹
                    probs = F.softmax(outputs, dim=-1)
                    top_k_probs, top_k_indices = torch.topk(probs[0], k=min(top_k, probs.shape[1]))
                    
                    predictions = []
                    for idx, prob in zip(top_k_indices, top_k_probs):
                        predictions.append({
                            'index': idx.item(),
                            'probability': prob.item()
                        })
                    
                    all_predictions[4].append(predictions)
        
        return all_predictions


# é›†æˆåˆ°ä¸»åˆ†ç±»å™¨çš„è¾…åŠ©å‡½æ•°
def integrate_hierarchical_loss(classifier, hierarchy, use_hierarchical=True, use_multitask=False):
    """
    å°†å±‚æ¬¡åŒ–æŸå¤±é›†æˆåˆ°ç°æœ‰åˆ†ç±»å™¨
    
    Args:
        classifier: ç°æœ‰çš„åˆ†ç±»å™¨å®ä¾‹
        hierarchy: ISCOå±‚æ¬¡ç»“æ„
        use_hierarchical: æ˜¯å¦ä½¿ç”¨å±‚æ¬¡åŒ–æŸå¤±
        use_multitask: æ˜¯å¦ä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ 
    """
    if use_multitask:
        # æ›¿æ¢ä¸ºå¤šä»»åŠ¡æŸå¤±
        classifier.criterion = HierarchicalMultitaskLoss(
            hierarchy=hierarchy,
            task_weights={1: 0.1, 2: 0.2, 3: 0.3, 4: 0.4},
            level_weights={1: 8.0, 2: 4.0, 3: 2.0, 4: 1.0}
        )
        print("âœ… å·²å¯ç”¨å¤šä»»åŠ¡å±‚æ¬¡åŒ–å­¦ä¹ ")
    elif use_hierarchical:
        # æ›¿æ¢ä¸ºå±‚æ¬¡åŒ–æŸå¤±
        classifier.criterion = HierarchicalISCOLoss(
            hierarchy=hierarchy,
            level_weights={1: 8.0, 2: 4.0, 3: 2.0, 4: 1.0}
        )
        print("âœ… å·²å¯ç”¨å±‚æ¬¡åŒ–æŸå¤±å‡½æ•°")
    
    return classifier


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸš€ å¢å¼ºç‰ˆTransformeråˆ†ç±»å™¨æ¨¡å—")
    print("=" * 50)
    
    # ç¤ºä¾‹ï¼šåˆ›å»ºå¢å¼ºç‰ˆåˆ†ç±»å™¨
    print("\nåˆ›å»ºç¤ºä¾‹åˆ†ç±»å™¨...")
    
    # æ¨¡æ‹ŸISCOå±‚æ¬¡ç»“æ„
    example_hierarchy = {
        '1': {'label': '1', 'level': 1, 'parents': []},
        '11': {'label': '11', 'level': 2, 'parents': ['1']},
        '112': {'label': '112', 'level': 3, 'parents': ['1', '11']},
        '1121': {'label': '1121', 'level': 4, 'parents': ['1', '11', '112']},
        # ... æ›´å¤šç¼–ç 
    }
    
    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = EnhancedTransformerClassifier(
        model_name_or_path='bert-base-chinese',
        output_size=100,  # å‡è®¾æœ‰100ä¸ª4çº§ç±»åˆ«
        isco_hierarchy=example_hierarchy,
        use_hierarchical_loss=True,
        use_multitask_learning=True,
        hierarchical_loss_weights={1: 8.0, 2: 4.0, 3: 2.0, 4: 1.0},
        task_weights={1: 0.1, 2: 0.2, 3: 0.3, 4: 0.4}
    )
    
    print("\nâœ… å¢å¼ºç‰ˆåˆ†ç±»å™¨åˆ›å»ºæˆåŠŸ!")
    print("\nç‰¹æ€§:")
    print("- å±‚æ¬¡åŒ–æŸå¤±å‡½æ•°ï¼šæ ¹æ®é”™è¯¯çº§åˆ«ç»™äºˆä¸åŒæƒ©ç½š")
    print("- å¤šä»»åŠ¡å­¦ä¹ ï¼šåŒæ—¶é¢„æµ‹1-4çº§ISCOç¼–ç ")
    print("- çº§åˆ«æƒé‡ï¼š1çº§é”™è¯¯æƒ©ç½šæœ€é‡(8.0)ï¼Œ4çº§æœ€è½»(1.0)")
    print("- ä»»åŠ¡æƒé‡ï¼š4çº§é¢„æµ‹æƒé‡æœ€é«˜(0.4)ï¼Œ1çº§æœ€ä½(0.1)")