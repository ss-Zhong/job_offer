#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®éªŒ
åŒæ—¶è¿è¡Œå¢å¼ºç‰ˆå’Œéå¢å¼ºç‰ˆæ¨¡å‹ï¼Œå¯¹æ¯”æ€§èƒ½å·®å¼‚
"""

import os
import time
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

import torch
torch.set_float32_matmul_precision('medium')

# å¯¼å…¥åŸæœ‰æ¨¡å—
if __name__ == '__main__':
    from job_offers_classifier.job_offers_classfier_old import (
        ChineseTransformerJobOffersClassifier,
        get_recommended_chinese_models
    )
    from job_offers_classifier.job_offers_utils_old import create_hierarchy_node
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, accuracy_score
    
    # å¯¼å…¥æ•°æ®å¢å¼ºæ¨¡å—
    from chinese_job_data_augmentation import EnhancedJobDataProcessor


class AugmentationComparisonExperiment:
    """æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®éªŒç±»"""
    
    def __init__(self, csv_path: str, max_samples: int = 8000):
        self.csv_path = csv_path
        self.max_samples = max_samples
        
        # å®éªŒé…ç½®
        self.test_model = 'hfl/chinese-roberta-wwm-ext'  # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œå¯¹æ¯”
        self.training_config = {
            'max_epochs': 5,
            'patience': 3,
            'max_seq_length': 256,
            'batch_size': 16,
            'learning_rate': 2e-5
        }
        
        print("ğŸ”¬ æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®éªŒåˆå§‹åŒ–")
        print(f"   æµ‹è¯•æ¨¡å‹: {self.test_model}")
        print(f"   æœ€å¤§æ ·æœ¬æ•°: {self.max_samples}")

    def create_isco_hierarchy_from_codes(self, isco_codes):
        """ä»ISCOç¼–ç åˆ›å»ºå±‚æ¬¡ç»“æ„"""
        hierarchy = {}
        
        for code in isco_codes:
            code_str = str(code).zfill(4)
            
            for level in [1, 2, 3, 4]:
                level_code = code_str[:level]
                if level_code not in hierarchy:
                    hierarchy[level_code] = create_hierarchy_node(
                        level_code, 
                        f"ISCO-{level}ä½-{level_code}"
                    )
        
        return hierarchy

    def load_original_data(self):
        """åŠ è½½åŸå§‹æ•°æ®ï¼ˆä¸å¢å¼ºï¼‰"""
        print(f"\nğŸ“Š åŠ è½½åŸå§‹æ•°æ®ï¼ˆä¸å¢å¼ºï¼‰...")
        
        # åŠ è½½CSV
        try:
            df = pd.read_csv(self.csv_path, encoding='utf-8')
        except UnicodeDecodeError:
            df = pd.read_csv(self.csv_path, encoding='gbk')
        
        print(f"   åŸå§‹æ•°æ®: {len(df)} è¡Œ")
        
        # åŸºç¡€æ–‡æœ¬ç»„åˆï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        def combine_features(row):
            parts = []
            for col in ['å²—ä½', 'å²—ä½æè¿°', 'å²—ä½èŒèƒ½']:
                if col in row and pd.notna(row[col]):
                    content = str(row[col])
                    # å¤„ç†å²—ä½èŒèƒ½åˆ—è¡¨æ ¼å¼
                    if col == 'å²—ä½èŒèƒ½' and content.startswith('['):
                        try:
                            import ast
                            job_funcs = ast.literal_eval(content)
                            content = ' '.join(job_funcs) if isinstance(job_funcs, list) else content
                        except:
                            pass
                    parts.append(content)
            return ' '.join(parts)
        
        df['combined_text'] = df.apply(combine_features, axis=1)
        df['isco_code'] = df['ISCO_4_Digit_Code_Gemini'].astype(str).str.zfill(4)
        
        # ç§»é™¤ç©ºæ–‡æœ¬
        df = df[df['combined_text'].str.strip() != ''].copy()
        
        # æ™ºèƒ½é‡‡æ · - ä¼˜å…ˆä¿ç•™å¤šæ ·æœ¬ç±»åˆ«
        if self.max_samples and len(df) > self.max_samples:
            print(f"   éœ€è¦ä» {len(df)} è¡Œä¸­é‡‡æ · {self.max_samples} è¡Œ")
            
            # åˆ†æç±»åˆ«åˆ†å¸ƒ
            class_counts = df['isco_code'].value_counts()
            
            # ä¼˜å…ˆä¿ç•™æœ‰è¶³å¤Ÿæ ·æœ¬çš„ç±»åˆ«
            sufficient_classes = class_counts[class_counts >= 2].index.tolist()
            insufficient_classes = class_counts[class_counts == 1].index.tolist()
            
            print(f"   è¶³å¤Ÿæ ·æœ¬ç±»åˆ«: {len(sufficient_classes)}")
            print(f"   å•æ ·æœ¬ç±»åˆ«: {len(insufficient_classes)}")
            
            # åˆ†åˆ«é‡‡æ ·
            sampled_data = []
            
            # 1. ä¿ç•™æ‰€æœ‰å•æ ·æœ¬ç±»åˆ«ï¼ˆå¦‚æœç©ºé—´å…è®¸ï¼‰
            single_sample_data = df[df['isco_code'].isin(insufficient_classes)]
            remaining_budget = self.max_samples - len(single_sample_data)
            
            if remaining_budget > 0:
                sampled_data.append(single_sample_data)
                
                # 2. ä»å¤šæ ·æœ¬ç±»åˆ«ä¸­æŒ‰æ¯”ä¾‹é‡‡æ ·
                multi_sample_data = df[df['isco_code'].isin(sufficient_classes)]
                
                if len(multi_sample_data) > remaining_budget:
                    # åˆ†å±‚é‡‡æ ·ä¿æŒç±»åˆ«å¹³è¡¡
                    try:
                        multi_sample_data = multi_sample_data.groupby('isco_code', group_keys=False).apply(
                            lambda x: x.sample(min(len(x), max(2, remaining_budget // len(sufficient_classes))), 
                                             random_state=42)
                        ).reset_index(drop=True)
                        
                        # å¦‚æœè¿˜æ˜¯è¶…å‡ºé¢„ç®—ï¼Œéšæœºé‡‡æ ·
                        if len(multi_sample_data) > remaining_budget:
                            multi_sample_data = multi_sample_data.sample(n=remaining_budget, random_state=42)
                            
                    except ValueError:
                        # å¦‚æœåˆ†å±‚é‡‡æ ·å¤±è´¥ï¼Œç›´æ¥éšæœºé‡‡æ ·
                        multi_sample_data = multi_sample_data.sample(n=remaining_budget, random_state=42)
                
                sampled_data.append(multi_sample_data)
            else:
                # é¢„ç®—ä¸è¶³ï¼Œåªèƒ½ä»å•æ ·æœ¬ç±»åˆ«ä¸­éšæœºé€‰æ‹©
                sampled_data.append(single_sample_data.sample(n=self.max_samples, random_state=42))
            
            df = pd.concat(sampled_data, ignore_index=True)
            print(f"   æ™ºèƒ½é‡‡æ ·åæ ·æœ¬: {len(df)} è¡Œ")
        
        texts = df['combined_text'].tolist()
        labels = df['isco_code'].tolist()
        
        # æœ€ç»ˆéªŒè¯ - ç§»é™¤ä»ç„¶å•æ ·æœ¬çš„ç±»åˆ«ï¼ˆå¦‚æœå¿…è¦ï¼‰
        final_class_counts = pd.Series(labels).value_counts()
        single_classes = final_class_counts[final_class_counts == 1].index.tolist()
        
        if len(single_classes) > len(final_class_counts) * 0.3:  # å¦‚æœå•æ ·æœ¬ç±»åˆ«è¿‡å¤šï¼ˆè¶…è¿‡30%ï¼‰
            print(f"   âš ï¸ å•æ ·æœ¬ç±»åˆ«è¿‡å¤š({len(single_classes)})ï¼Œè¿›è¡Œæœ€ç»ˆè¿‡æ»¤")
            # ä¿ç•™å¤šæ ·æœ¬ç±»åˆ«
            multi_classes = final_class_counts[final_class_counts > 1].index.tolist()
            filtered_data = [(text, label) for text, label in zip(texts, labels) if label in multi_classes]
            
            if len(filtered_data) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                texts, labels = zip(*filtered_data)
                texts, labels = list(texts), list(labels)
                print(f"   è¿‡æ»¤åæ ·æœ¬: {len(texts)} è¡Œ")
        
        # æ•°æ®ç»Ÿè®¡
        original_stats = {
            'total_samples': len(texts),
            'unique_labels': len(set(labels)),
            'avg_text_length': np.mean([len(text) for text in texts]),
            'label_distribution': pd.Series(labels).value_counts().to_dict()
        }
        
        print(f"âœ… åŸå§‹æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   æ ·æœ¬æ•°: {len(texts)}")
        print(f"   ç±»åˆ«æ•°: {len(set(labels))}")
        print(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {original_stats['avg_text_length']:.1f}")
        
        # æœ€ç»ˆæ£€æŸ¥
        final_single_classes = [label for label, count in pd.Series(labels).value_counts().items() if count == 1]
        if final_single_classes:
            print(f"   âš ï¸ ä»æœ‰ {len(final_single_classes)} ä¸ªå•æ ·æœ¬ç±»åˆ«")
        
        return texts, labels, original_stats

    def load_augmented_data(self):
        """åŠ è½½å¢å¼ºæ•°æ®"""
        print(f"\nğŸ¯ åŠ è½½å¢å¼ºæ•°æ®...")
        
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        processor = EnhancedJobDataProcessor()
        
        # å¤„ç†æ•°æ®ï¼ˆå¯ç”¨å¢å¼ºå’Œå¹³è¡¡ï¼‰
        texts, labels, processing_stats = processor.process_csv_data(
            csv_path=self.csv_path,
            enable_augmentation=True,
            balance_data=True,
            target_samples_per_class=6  # é€‚ä¸­çš„å¢å¼ºé‡
        )
        
        # é™åˆ¶æ ·æœ¬æ•°ï¼ˆå¦‚æœå¢å¼ºåè¶…å‡ºé™åˆ¶ï¼‰
        if self.max_samples and len(texts) > self.max_samples * 2:  # ç»™å¢å¼ºç‰ˆæ›´å¤šç©ºé—´
            print(f"   éœ€è¦ä» {len(texts)} è¡Œä¸­é‡‡æ · {self.max_samples * 2} è¡Œ")
            
            # ä½¿ç”¨æ™ºèƒ½é‡‡æ ·è€Œä¸æ˜¯ç®€å•çš„åˆ†å±‚é‡‡æ ·
            from collections import Counter
            label_counts = Counter(labels)
            
            # åˆ†æç±»åˆ«åˆ†å¸ƒ
            single_classes = [label for label, count in label_counts.items() if count == 1]
            multi_classes = [label for label, count in label_counts.items() if count > 1]
            
            if len(single_classes) > 0:
                print(f"   æ£€æµ‹åˆ° {len(single_classes)} ä¸ªå•æ ·æœ¬ç±»åˆ«ï¼Œä½¿ç”¨æ™ºèƒ½é‡‡æ ·")
                
                # ä¿ç•™æ‰€æœ‰å•æ ·æœ¬ç±»åˆ«
                single_indices = [i for i, label in enumerate(labels) if label in single_classes]
                
                # ä»å¤šæ ·æœ¬ç±»åˆ«ä¸­é‡‡æ ·
                multi_indices = [i for i, label in enumerate(labels) if label in multi_classes]
                remaining_budget = self.max_samples * 2 - len(single_indices)
                
                if remaining_budget > 0 and len(multi_indices) > remaining_budget:
                    # åˆ†å±‚é‡‡æ ·å¤šæ ·æœ¬ç±»åˆ«
                    multi_texts_labels = [(texts[i], labels[i]) for i in multi_indices]
                    multi_texts_only = [texts[i] for i in multi_indices]
                    multi_labels_only = [labels[i] for i in multi_indices]
                    
                    try:
                        sampled_multi_texts, _, sampled_multi_labels, _ = train_test_split(
                            multi_texts_only, multi_labels_only,
                            train_size=remaining_budget,
                            stratify=multi_labels_only,
                            random_state=42
                        )
                        
                        # ç»„åˆç»“æœ
                        final_texts = [texts[i] for i in single_indices] + sampled_multi_texts
                        final_labels = [labels[i] for i in single_indices] + sampled_multi_labels
                        
                    except ValueError:
                        # å¦‚æœåˆ†å±‚é‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨éšæœºé‡‡æ ·
                        print("   åˆ†å±‚é‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨éšæœºé‡‡æ ·")
                        selected_multi_indices = np.random.choice(multi_indices, 
                                                                 size=min(remaining_budget, len(multi_indices)), 
                                                                 replace=False)
                        
                        final_texts = [texts[i] for i in single_indices] + [texts[i] for i in selected_multi_indices]
                        final_labels = [labels[i] for i in single_indices] + [labels[i] for i in selected_multi_indices]
                
                else:
                    # é¢„ç®—è¶³å¤Ÿæˆ–æ²¡æœ‰å¤šæ ·æœ¬ç±»åˆ«
                    final_texts = texts
                    final_labels = labels
                
                texts = final_texts
                labels = final_labels
                
            else:
                # æ²¡æœ‰å•æ ·æœ¬ç±»åˆ«ï¼Œæ­£å¸¸åˆ†å±‚é‡‡æ ·
                try:
                    texts, _, labels, _ = train_test_split(
                        texts, labels, 
                        train_size=self.max_samples * 2,
                        stratify=labels,
                        random_state=42
                    )
                except ValueError:
                    # åˆ†å±‚é‡‡æ ·å¤±è´¥ï¼Œéšæœºé‡‡æ ·
                    print("   åˆ†å±‚é‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨éšæœºé‡‡æ ·")
                    indices = np.random.choice(len(texts), size=self.max_samples * 2, replace=False)
                    texts = [texts[i] for i in indices]
                    labels = [labels[i] for i in indices]
            
            print(f"   æ™ºèƒ½é‡‡æ ·åæ ·æœ¬: {len(texts)}")
        
        augmented_stats = processing_stats['final_stats']
        
        print(f"âœ… å¢å¼ºæ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   åŸå§‹æ ·æœ¬: {processing_stats['original_stats']['total_samples']}")
        print(f"   å¢å¼ºåæ ·æœ¬: {len(texts)}")
        print(f"   ç±»åˆ«æ•°: {augmented_stats['unique_labels']}")
        print(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {augmented_stats['avg_text_length']:.1f}")
        
        # æœ€ç»ˆæ£€æŸ¥å•æ ·æœ¬ç±»åˆ«
        final_class_counts = Counter(labels)
        final_single_classes = [label for label, count in final_class_counts.items() if count == 1]
        if final_single_classes:
            print(f"   âš ï¸ ä»æœ‰ {len(final_single_classes)} ä¸ªå•æ ·æœ¬ç±»åˆ«")
        
        return texts, labels, augmented_stats

    def train_single_experiment(self, texts, labels, experiment_name, results_dir):
        """è®­ç»ƒå•ä¸ªå®éªŒ"""
        print(f"\nğŸ¤– å¼€å§‹è®­ç»ƒ: {experiment_name}")
        
        # æ™ºèƒ½æ•°æ®åˆ’åˆ† - å¤„ç†å•æ ·æœ¬ç±»åˆ«
        print("   ğŸ“Š åˆ†ææ•°æ®åˆ†å¸ƒ...")
        from collections import Counter
        label_counts = Counter(labels)
        
        # ç»Ÿè®¡å•æ ·æœ¬ç±»åˆ«
        single_sample_classes = [label for label, count in label_counts.items() if count == 1]
        multi_sample_classes = [label for label, count in label_counts.items() if count > 1]
        
        print(f"   å•æ ·æœ¬ç±»åˆ«: {len(single_sample_classes)}")
        print(f"   å¤šæ ·æœ¬ç±»åˆ«: {len(multi_sample_classes)}")
        
        if len(single_sample_classes) > 0:
            print(f"   âš ï¸ æ£€æµ‹åˆ° {len(single_sample_classes)} ä¸ªå•æ ·æœ¬ç±»åˆ«ï¼Œä½¿ç”¨æ™ºèƒ½åˆ’åˆ†ç­–ç•¥")
            
            # åˆ†åˆ«å¤„ç†å•æ ·æœ¬å’Œå¤šæ ·æœ¬ç±»åˆ«
            train_indices = []
            test_indices = []
            
            # å•æ ·æœ¬ç±»åˆ«å…¨éƒ¨æ”¾å…¥è®­ç»ƒé›†
            for i, (text, label) in enumerate(zip(texts, labels)):
                if label in single_sample_classes:
                    train_indices.append(i)
            
            # å¤šæ ·æœ¬ç±»åˆ«æ­£å¸¸åˆ†å±‚åˆ’åˆ†
            if multi_sample_classes:
                multi_texts = []
                multi_labels = []
                multi_indices = []
                
                for i, (text, label) in enumerate(zip(texts, labels)):
                    if label in multi_sample_classes:
                        multi_texts.append(text)
                        multi_labels.append(label)
                        multi_indices.append(i)
                
                if len(multi_texts) > 0:
                    # å¯¹å¤šæ ·æœ¬ç±»åˆ«è¿›è¡Œåˆ†å±‚åˆ’åˆ†
                    multi_train_idx, multi_test_idx = train_test_split(
                        range(len(multi_texts)), 
                        test_size=0.2, 
                        random_state=42, 
                        stratify=multi_labels
                    )
                    
                    # è½¬æ¢å›åŸå§‹ç´¢å¼•
                    train_indices.extend([multi_indices[i] for i in multi_train_idx])
                    test_indices.extend([multi_indices[i] for i in multi_test_idx])
            
            # æ„å»ºæœ€ç»ˆçš„è®­ç»ƒæµ‹è¯•é›†
            train_texts = [texts[i] for i in train_indices]
            train_labels = [labels[i] for i in train_indices]
            test_texts = [texts[i] for i in test_indices]
            test_labels = [labels[i] for i in test_indices]
            
        else:
            # æ­£å¸¸åˆ†å±‚åˆ’åˆ†
            train_texts, test_texts, train_labels, test_labels = train_test_split(
                texts, labels, test_size=0.2, random_state=42, stratify=labels
            )
        
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_texts)}")
        print(f"   æµ‹è¯•æ ·æœ¬: {len(test_texts)}")
        
        # éªŒè¯æ•°æ®åˆ’åˆ†ç»“æœ
        if len(test_texts) == 0:
            print("   âš ï¸ æµ‹è¯•é›†ä¸ºç©ºï¼Œè°ƒæ•´åˆ’åˆ†ç­–ç•¥")
            # å¦‚æœæµ‹è¯•é›†ä¸ºç©ºï¼Œå¼ºåˆ¶ä»è®­ç»ƒé›†ä¸­åˆ†å‡ºä¸€éƒ¨åˆ†
            if len(train_texts) >= 10:
                split_point = max(1, len(train_texts) // 5)  # å–20%ä½œä¸ºæµ‹è¯•é›†
                test_texts = train_texts[-split_point:]
                test_labels = train_labels[-split_point:]
                train_texts = train_texts[:-split_point]
                train_labels = train_labels[:-split_point]
                print(f"   è°ƒæ•´å - è®­ç»ƒæ ·æœ¬: {len(train_texts)}, æµ‹è¯•æ ·æœ¬: {len(test_texts)}")
            else:
                print("   âŒ æ•°æ®é‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆè®­ç»ƒ")
                return {
                    'experiment_name': experiment_name,
                    'model_name': self.test_model,
                    'accuracy': 0.0,
                    'top_3_accuracy': 0.0,
                    'top_5_accuracy': 0.0,
                    'training_time_minutes': 0.0,
                    'status': 'failed',
                    'error': 'Insufficient data for training'
                }
        
        # åˆ›å»ºå±‚æ¬¡ç»“æ„
        hierarchy = self.create_isco_hierarchy_from_codes(set(labels))
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        model_dir = results_dir / f"model_{experiment_name.lower().replace(' ', '_')}"
        
        start_time = time.time()
        
        try:
            # åˆ›å»ºåˆ†ç±»å™¨
            classifier = ChineseTransformerJobOffersClassifier(
                model_dir=str(model_dir),
                hierarchy=hierarchy,
                transformer_model=self.test_model,
                learning_rate=self.training_config['learning_rate'],
                batch_size=self.training_config['batch_size'],
                max_epochs=self.training_config['max_epochs'],
                early_stopping=True,
                early_stopping_patience=self.training_config['patience'],
                max_sequence_length=self.training_config['max_seq_length'],
                devices=1,
                accelerator="gpu" if torch.cuda.is_available() else "cpu",
                precision="16-mixed" if torch.cuda.is_available() else 32,
                threads=0,
                verbose=True
            )
            
            # å‡†å¤‡éªŒè¯é›†
            val_size = min(300, len(test_texts) // 3)
            val_texts = test_texts[:val_size]
            val_labels = test_labels[:val_size]
            final_test_texts = test_texts[val_size:]
            final_test_labels = test_labels[val_size:]
            
            print(f"   éªŒè¯æ ·æœ¬: {val_size}")
            print(f"   æœ€ç»ˆæµ‹è¯•æ ·æœ¬: {len(final_test_texts)}")
            
            # è®­ç»ƒ
            print(f"   ğŸ¯ å¼€å§‹è®­ç»ƒ...")
            classifier.fit(train_labels, train_texts, y_val=val_labels, X_val=val_texts)
            
            # é¢„æµ‹
            print(f"   ğŸ”® é¢„æµ‹ä¸­...")
            predictions_df = classifier.predict(final_test_texts, format='dataframe', top_k=5)
            
            # è®¡ç®—æŒ‡æ ‡
            y_true = final_test_labels
            y_pred = predictions_df['class_1'].tolist()
            
            accuracy = accuracy_score(y_true, y_pred)
            top_3_acc = sum(
                true_label in [predictions_df.iloc[i][f'class_{j}'] for j in range(1, 4)]
                for i, true_label in enumerate(y_true)
            ) / len(y_true)
            top_5_acc = sum(
                true_label in [predictions_df.iloc[i][f'class_{j}'] for j in range(1, 6)]
                for i, true_label in enumerate(y_true)
            ) / len(y_true)
            
            training_time = time.time() - start_time
            
            # è¯¦ç»†åˆ†æ
            detailed_analysis = self.analyze_predictions(y_true, y_pred, predictions_df)
            
            result = {
                'experiment_name': experiment_name,
                'model_name': self.test_model,
                'train_samples': len(train_texts),
                'test_samples': len(final_test_texts),
                'accuracy': accuracy,
                'top_3_accuracy': top_3_acc,
                'top_5_accuracy': top_5_acc,
                'training_time_minutes': training_time / 60,
                'status': 'success',
                'detailed_analysis': detailed_analysis
            }
            
            print(f"âœ… {experiment_name} è®­ç»ƒå®Œæˆ!")
            print(f"   å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
            print(f"   Top-3å‡†ç¡®ç‡: {top_3_acc:.4f} ({top_3_acc*100:.2f}%)")
            print(f"   Top-5å‡†ç¡®ç‡: {top_5_acc:.4f} ({top_5_acc*100:.2f}%)")
            print(f"   è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            detailed_results = pd.DataFrame({
                'true_label': y_true,
                'predicted_label': y_pred,
                'confidence': predictions_df['prob_1'].tolist(),
                'correct': [t == p for t, p in zip(y_true, y_pred)]
            })
            detailed_results.to_csv(
                results_dir / f"{experiment_name.lower().replace(' ', '_')}_detailed_results.csv", 
                index=False, encoding='utf-8'
            )
            
            return result
            
        except Exception as e:
            print(f"âŒ {experiment_name} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'experiment_name': experiment_name,
                'model_name': self.test_model,
                'accuracy': 0.0,
                'top_3_accuracy': 0.0,
                'top_5_accuracy': 0.0,
                'training_time_minutes': 0.0,
                'status': 'failed',
                'error': str(e)
            }

    def analyze_predictions(self, y_true, y_pred, predictions_df):
        """åˆ†æé¢„æµ‹ç»“æœ"""
        from collections import defaultdict
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡å‡†ç¡®ç‡
        class_accuracy = defaultdict(list)
        for true_label, pred_label in zip(y_true, y_pred):
            class_accuracy[true_label].append(true_label == pred_label)
        
        class_acc_summary = {}
        for class_label, correct_list in class_accuracy.items():
            class_acc_summary[class_label] = {
                'accuracy': np.mean(correct_list),
                'samples': len(correct_list)
            }
        
        # ç½®ä¿¡åº¦åˆ†æ
        confidences = predictions_df['prob_1'].tolist()
        correct_mask = [t == p for t, p in zip(y_true, y_pred)]
        
        correct_confidences = [conf for conf, correct in zip(confidences, correct_mask) if correct]
        wrong_confidences = [conf for conf, correct in zip(confidences, correct_mask) if not correct]
        
        return {
            'class_accuracy': class_acc_summary,
            'avg_confidence_correct': np.mean(correct_confidences) if correct_confidences else 0,
            'avg_confidence_wrong': np.mean(wrong_confidences) if wrong_confidences else 0,
            'confidence_stats': {
                'mean': np.mean(confidences),
                'std': np.std(confidences),
                'min': np.min(confidences),
                'max': np.max(confidences)
            }
        }

    def run_comparison(self):
        """è¿è¡Œå¯¹æ¯”å®éªŒ"""
        print("ğŸ”¬ å¼€å§‹æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®éªŒ")
        print("=" * 60)
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = Path("augmentation_comparison_results")
        results_dir.mkdir(exist_ok=True)
        
        experiment_start_time = time.time()
        results = []
        data_stats = {}
        
        # å®éªŒ1: åŸå§‹æ•°æ®ï¼ˆä¸å¢å¼ºï¼‰
        print(f"\n{'='*60}")
        print(f"ğŸ”µ å®éªŒ1: åŸå§‹æ•°æ®è®­ç»ƒ")
        print(f"{'='*60}")
        
        original_texts, original_labels, original_stats = self.load_original_data()
        data_stats['original'] = original_stats
        
        original_result = self.train_single_experiment(
            original_texts, original_labels, "Original Data", results_dir
        )
        results.append(original_result)
        
        # å®éªŒ2: å¢å¼ºæ•°æ®
        print(f"\n{'='*60}")
        print(f"ğŸŸ¢ å®éªŒ2: æ•°æ®å¢å¼ºè®­ç»ƒ")
        print(f"{'='*60}")
        
        augmented_texts, augmented_labels, augmented_stats = self.load_augmented_data()
        data_stats['augmented'] = augmented_stats
        
        augmented_result = self.train_single_experiment(
            augmented_texts, augmented_labels, "Augmented Data", results_dir
        )
        results.append(augmented_result)
        
        total_time = time.time() - experiment_start_time
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report(results, data_stats, total_time, results_dir)
        
        return results, results_dir

    def generate_comparison_report(self, results, data_stats, total_time, results_dir):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\nğŸ“Š ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š...")
        
        # åˆ›å»ºç»“æœDataFrame
        results_df = pd.DataFrame(results)
        results_df.to_csv(results_dir / "comparison_results.csv", index=False, encoding='utf-8')
        
        # è®¡ç®—æ”¹è¿›æƒ…å†µ
        if len(results) >= 2 and results[0]['status'] == 'success' and results[1]['status'] == 'success':
            original_acc = results[0]['accuracy']
            augmented_acc = results[1]['accuracy']
            improvement = augmented_acc - original_acc
            improvement_pct = (improvement / original_acc * 100) if original_acc > 0 else 0
            
            original_top3 = results[0]['top_3_accuracy']
            augmented_top3 = results[1]['top_3_accuracy']
            top3_improvement = augmented_top3 - original_top3
            top3_improvement_pct = (top3_improvement / original_top3 * 100) if original_top3 > 0 else 0
        else:
            improvement = improvement_pct = 0
            top3_improvement = top3_improvement_pct = 0
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = {
            'experiment_info': {
                'experiment_type': 'Data Augmentation Effect Comparison',
                'timestamp': datetime.now().isoformat(),
                'total_time_hours': total_time / 3600,
                'model_used': self.test_model,
                'max_samples': self.max_samples
            },
            'data_statistics': data_stats,
            'results': results,
            'comparison_analysis': {
                'accuracy_improvement': improvement,
                'accuracy_improvement_percentage': improvement_pct,
                'top3_improvement': top3_improvement,
                'top3_improvement_percentage': top3_improvement_pct,
                'training_time_difference': results[1]['training_time_minutes'] - results[0]['training_time_minutes'] if len(results) >= 2 else 0
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(results_dir / "comparison_report.json", 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        print(f"\nğŸ‰ æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®Œæˆ!")
        print(f"ğŸ“ˆ å®éªŒæ€»ç»“:")
        print(f"   æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
        print(f"   æµ‹è¯•æ¨¡å‹: {self.test_model}")
        
        if len(results) >= 2:
            print(f"\nğŸ“Š è¯¦ç»†å¯¹æ¯”ç»“æœ:")
            print(f"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
            print(f"â”‚ å®éªŒç±»å‹        â”‚ å‡†ç¡®ç‡      â”‚ Top-3å‡†ç¡®ç‡ â”‚ è®­ç»ƒæ—¶é—´    â”‚")
            print(f"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
            
            for result in results:
                if result['status'] == 'success':
                    name = result['experiment_name'][:15]
                    acc = f"{result['accuracy']:.4f}"
                    top3 = f"{result['top_3_accuracy']:.4f}"
                    time_str = f"{result['training_time_minutes']:.1f}åˆ†"
                    print(f"â”‚ {name:<15} â”‚ {acc:<11} â”‚ {top3:<11} â”‚ {time_str:<11} â”‚")
            
            print(f"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            
            if results[0]['status'] == 'success' and results[1]['status'] == 'success':
                print(f"\nğŸ¯ å¢å¼ºæ•ˆæœåˆ†æ:")
                print(f"   å‡†ç¡®ç‡æå‡: {improvement:+.4f} ({improvement_pct:+.2f}%)")
                print(f"   Top-3å‡†ç¡®ç‡æå‡: {top3_improvement:+.4f} ({top3_improvement_pct:+.2f}%)")
                print(f"   è®­ç»ƒæ—¶é—´å¢åŠ : {report['comparison_analysis']['training_time_difference']:+.1f} åˆ†é’Ÿ")
                
                if improvement > 0:
                    print(f"   ğŸ‰ æ•°æ®å¢å¼ºæ•ˆæœæ˜¾è‘—ï¼")
                elif improvement > -0.01:
                    print(f"   âš–ï¸ æ•°æ®å¢å¼ºæ•ˆæœä¸­æ€§")
                else:
                    print(f"   âš ï¸ æ•°æ®å¢å¼ºå¯èƒ½éœ€è¦è°ƒä¼˜")
                
                # æ•°æ®é‡å¯¹æ¯”
                print(f"\nğŸ“Š æ•°æ®é‡å¯¹æ¯”:")
                original_samples = data_stats['original']['total_samples']
                augmented_samples = data_stats['augmented']['total_samples']
                increase_ratio = augmented_samples / original_samples
                print(f"   åŸå§‹æ•°æ®: {original_samples:,} æ ·æœ¬")
                print(f"   å¢å¼ºæ•°æ®: {augmented_samples:,} æ ·æœ¬")
                print(f"   å¢é•¿å€æ•°: {increase_ratio:.1f}x")
                
        print(f"\nğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {results_dir}")
        
        # ç”Ÿæˆå¯è§†åŒ–å»ºè®®
        print(f"\nğŸ’¡ åç»­åˆ†æå»ºè®®:")
        print(f"   1. æŸ¥çœ‹ comparison_results.csv äº†è§£è¯¦ç»†æŒ‡æ ‡")
        print(f"   2. æŸ¥çœ‹ *_detailed_results.csv åˆ†æå…·ä½“é¢„æµ‹")
        print(f"   3. å¦‚æœæ•ˆæœä¸ä½³ï¼Œå¯ä»¥è°ƒæ•´å¢å¼ºç­–ç•¥å‚æ•°")
        print(f"   4. è€ƒè™‘åœ¨æ›´å¤§æ•°æ®é›†ä¸ŠéªŒè¯æ•ˆæœ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”å®éªŒ")
    print("æ¯”è¾ƒåŸå§‹æ•°æ® vs å¢å¼ºæ•°æ®çš„æ¨¡å‹æ€§èƒ½")
    print("=" * 60)
    
    # é…ç½®å®éªŒ
    csv_path = "newjob1_sortall.csv"
    
    if not os.path.exists(csv_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {csv_path}")
        print("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹")
        return
    
    print("é€‰æ‹©å®éªŒè§„æ¨¡:")
    print("1. å¿«é€Ÿå¯¹æ¯” (8Kæ ·æœ¬é™åˆ¶)")
    print("2. æ ‡å‡†å¯¹æ¯” (15Kæ ·æœ¬é™åˆ¶)")
    print("3. å®Œæ•´å¯¹æ¯” (æ— æ ·æœ¬é™åˆ¶)")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1-3): ")
    
    if choice == "1":
        max_samples = 8000
    elif choice == "2":
        max_samples = 15000
    elif choice == "3":
        max_samples = None
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨å¿«é€Ÿå¯¹æ¯”")
        max_samples = 8000
    
    print(f"\nğŸ¯ å®éªŒé…ç½®:")
    print(f"   æ•°æ®æ–‡ä»¶: {csv_path}")
    print(f"   æ ·æœ¬é™åˆ¶: {max_samples if max_samples else 'æ— é™åˆ¶'}")
    print(f"   æµ‹è¯•æ¨¡å‹: hfl/chinese-roberta-wwm-ext")
    print(f"   å¯¹æ¯”å†…å®¹: åŸå§‹æ•°æ® vs æ•°æ®å¢å¼º")
    
    confirm = input(f"\nç¡®è®¤å¼€å§‹å¯¹æ¯”å®éªŒ? (y/N): ")
    if confirm.lower() != 'y':
        print("å®éªŒå·²å–æ¶ˆ")
        return
    
    try:
        # åˆ›å»ºå®éªŒå¯¹è±¡
        experiment = AugmentationComparisonExperiment(csv_path, max_samples)
        
        # è¿è¡Œå¯¹æ¯”å®éªŒ
        results, results_dir = experiment.run_comparison()
        
        print(f"\nğŸ¯ å¯¹æ¯”å®éªŒå®Œæˆ! æŸ¥çœ‹è¯¦ç»†ç»“æœ: {results_dir}")
        
    except Exception as e:
        print(f"âŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()