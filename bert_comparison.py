#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›´æ–°çš„ä¸­æ–‡BERTæ¨¡å‹å¯¹æ¯”è„šæœ¬
ç§»é™¤TensorFlowä¾èµ–ï¼Œæ·»åŠ æ›´å¤šPyTorchåŸç”Ÿæ¨¡å‹
"""

import os
import time
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

# Windowså¤šè¿›ç¨‹ä¿®å¤
import torch
torch.set_float32_matmul_precision('medium')

if __name__ == '__main__':
    from job_offers_classifier.job_offers_classfier_old import (
        ChineseTransformerJobOffersClassifier,
        get_recommended_chinese_models
    )
    from job_offers_classifier.job_offers_utils_old import create_hierarchy_node
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, accuracy_score

    # æ›´æ–°çš„æ¨¡å‹é…ç½® - åªåŒ…å«PyTorchåŸç”Ÿæ”¯æŒçš„æ¨¡å‹
    UPDATED_MODELS = {
        'google_bert': {
            'name': 'bert-base-chinese',
            'description': 'Google Chinese BERT-base',
            'learning_rate': 2e-5,
            'batch_size': 16,
            'notes': 'Googleå®˜æ–¹ä¸­æ–‡BERT'
        },
        'ckiplab_bert': {
            'name': 'ckiplab/bert-base-chinese',
            'description': 'CKIP Chinese BERT-base',
            'learning_rate': 2e-5,
            'batch_size': 16,
            'notes': 'å°æ¹¾ä¸­ç ”é™¢ä¸­æ–‡BERT'
        },
        'hfl_roberta': {
            'name': 'hfl/chinese-roberta-wwm-ext',
            'description': 'HFL Chinese RoBERTa-wwm-ext (æ¨è)',
            'learning_rate': 2e-5,
            'batch_size': 16,
            'notes': 'å“ˆå·¥å¤§RoBERTaï¼Œæ€§èƒ½æœ€ä½³'
        },
        'hfl_bert': {
            'name': 'hfl/chinese-bert-wwm-ext',
            'description': 'HFL Chinese BERT-wwm-ext',
            'learning_rate': 2e-5,
            'batch_size': 16,
            'notes': 'å“ˆå·¥å¤§BERTï¼Œç»å…¸ç¨³å®š'
        },
        'hfl_electra': {
            'name': 'hfl/chinese-electra-180g-base-discriminator',
            'description': 'HFL Chinese ELECTRA-base',
            'learning_rate': 2e-5,
            'batch_size': 16,
            'notes': 'å“ˆå·¥å¤§ELECTRAï¼Œé«˜æ•ˆè®­ç»ƒ'
        },
        'bert_base_multilingual': {
            'name': 'bert-base-multilingual-cased',
            'description': 'BERT Multilingual (åŒ…å«ä¸­æ–‡)',
            'learning_rate': 2e-5,
            'batch_size': 16,
            'notes': 'å¤šè¯­è¨€BERTï¼Œä½œä¸ºåŸºå‡†å¯¹æ¯”'
        }
    }

    def setup_comparison_environment():
        """è®¾ç½®å¯¹æ¯”ç¯å¢ƒ"""
        print("ğŸ”§ è®¾ç½®æ¨¡å‹å¯¹æ¯”ç¯å¢ƒ...")
        
        results_dir = Path("chinese_bert_comparison_updated")
        results_dir.mkdir(exist_ok=True)
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            print(f"âœ… GPU: {gpu_name}")
            print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
        else:
            print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
        
        return results_dir

    def load_and_prepare_data(csv_path, test_size=0.2, max_samples=None):
        """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½æ•°æ®: {csv_path}")
        
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
        except UnicodeDecodeError:
            df = pd.read_csv(csv_path, encoding='gbk')
        
        print(f"   åŸå§‹æ•°æ®: {len(df)} è¡Œ")
        
        if max_samples and len(df) > max_samples:
            df = df.sample(n=max_samples, random_state=42)
            print(f"   éšæœºé‡‡æ ·: {len(df)} è¡Œ")
        
        # ç»„åˆæ–‡æœ¬ç‰¹å¾
        def combine_features(row):
            parts = []
            for col in ['å²—ä½', 'å²—ä½æè¿°', 'å²—ä½èŒèƒ½']:
                if pd.notna(row[col]):
                    parts.append(str(row[col]))
            return ' '.join(parts)
        
        df['combined_text'] = df.apply(combine_features, axis=1)
        df['isco_code'] = df['ISCO_4_Digit_Code_Gemini'].astype(str).str.zfill(4)
        
        # æ•°æ®ç»Ÿè®¡
        isco_counts = df['isco_code'].value_counts()
        print(f"   å”¯ä¸€ISCOç¼–ç : {len(isco_counts)}")
        print(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {df['combined_text'].str.len().mean():.0f} å­—ç¬¦")
        
        # æ™ºèƒ½æ•°æ®åˆ’åˆ†
        rare_threshold = 3
        rare_classes = isco_counts[isco_counts <= rare_threshold].index.tolist()
        common_classes = isco_counts[isco_counts > rare_threshold].index.tolist()
        
        print(f"   ç¨€æœ‰ç±»åˆ« (â‰¤{rare_threshold}æ ·æœ¬): {len(rare_classes)}")
        print(f"   å¸¸è§ç±»åˆ« (>{rare_threshold}æ ·æœ¬): {len(common_classes)}")
        
        # åˆ†åˆ«å¤„ç†ç¨€æœ‰å’Œå¸¸è§ç±»åˆ«
        train_indices = []
        test_indices = []
        
        for rare_class in rare_classes:
            rare_samples = df[df['isco_code'] == rare_class].index.tolist()
            if len(rare_samples) == 1:
                train_indices.extend(rare_samples)
            elif len(rare_samples) == 2:
                train_indices.append(rare_samples[0])
                test_indices.append(rare_samples[1])
            else:
                train_indices.extend(rare_samples[:2])
                test_indices.extend(rare_samples[2:])
        
        if common_classes:
            common_df = df[df['isco_code'].isin(common_classes)]
            train_common, test_common = train_test_split(
                common_df, test_size=test_size, random_state=42,
                stratify=common_df['isco_code']
            )
            train_indices.extend(train_common.index.tolist())
            test_indices.extend(test_common.index.tolist())
        
        train_df = df.loc[train_indices]
        test_df = df.loc[test_indices]
        
        print(f"âœ… æ•°æ®åˆ’åˆ†å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)")
        print(f"   æµ‹è¯•é›†: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)")
        
        # åˆ›å»ºå±‚æ¬¡ç»“æ„
        hierarchy = create_isco_hierarchy_from_codes(df['isco_code'].unique())
        
        return (train_df['combined_text'].tolist(),
                train_df['isco_code'].tolist(),
                test_df['combined_text'].tolist(), 
                test_df['isco_code'].tolist(),
                hierarchy,
                {
                    'total_samples': len(df),
                    'train_samples': len(train_df),
                    'test_samples': len(test_df),
                    'num_classes': len(set(df['isco_code'])),
                    'rare_classes': len(rare_classes),
                    'common_classes': len(common_classes)
                })

    def create_isco_hierarchy_from_codes(isco_codes):
        """ä»ISCOç¼–ç åˆ›å»ºå±‚æ¬¡ç»“æ„"""
        hierarchy = {}
        
        for code in isco_codes:
            code_str = str(code).zfill(4)
            
            for level in [1, 2, 3, 4]:
                level_code = code_str[:level]
                if level_code not in hierarchy:
                    hierarchy[level_code] = create_hierarchy_node(
                        level_code, 
                        f"ISCO-{level}ä½-{level_code}"
                    )
        
        return hierarchy

    def test_model_availability(model_config):
        """æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        model_name = model_config['name']
        print(f"   ğŸ” æµ‹è¯•æ¨¡å‹: {model_name}")
        
        try:
            from transformers import AutoConfig, AutoModel, AutoTokenizer
            
            config = AutoConfig.from_pretrained(model_name)
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # å°è¯•åŠ è½½æ¨¡å‹æ¥æµ‹è¯•
            model = AutoModel.from_pretrained(model_name)
            
            # æ¸…ç†å†…å­˜
            del model
            del tokenizer
            
            print(f"      âœ… æ¨¡å‹å¯ç”¨")
            return True
            
        except Exception as e:
            error_msg = str(e)
            if "TensorFlow" in error_msg or "from_tf=True" in error_msg:
                print(f"      âŒ æ¨¡å‹éœ€è¦TensorFlowæƒé‡è½¬æ¢")
            else:
                print(f"      âŒ æ¨¡å‹ä¸å¯ç”¨: {e}")
            return False

    def train_single_model(model_key, model_config, train_texts, train_labels, 
                          test_texts, test_labels, hierarchy, results_dir, 
                          training_config):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"\nğŸ¤– å¼€å§‹è®­ç»ƒ: {model_config['description']}")
        print(f"   æ¨¡å‹: {model_config['name']}")
        print(f"   è¯´æ˜: {model_config['notes']}")
        
        # æµ‹è¯•æ¨¡å‹å¯ç”¨æ€§
        if not test_model_availability(model_config):
            return {
                'model_key': model_key,
                'model_name': model_config['name'],
                'description': model_config['description'],
                'accuracy': 0.0,
                'top_3_accuracy': 0.0,
                'top_5_accuracy': 0.0,
                'training_time_minutes': 0.0,
                'test_samples': 0,
                'status': 'failed',
                'error': 'Model not available (possibly requires TensorFlow)'
            }
        
        start_time = time.time()
        model_dir = results_dir / f"model_{model_key}"
        
        try:
            # åˆ›å»ºåˆ†ç±»å™¨
            classifier = ChineseTransformerJobOffersClassifier(
                model_dir=str(model_dir),
                hierarchy=hierarchy,
                transformer_model=model_config['name'],
                learning_rate=model_config['learning_rate'],
                batch_size=model_config['batch_size'],
                max_epochs=training_config['max_epochs'],
                early_stopping=True,
                early_stopping_patience=training_config['patience'],
                max_sequence_length=training_config['max_seq_length'],
                devices=1,
                accelerator="gpu" if torch.cuda.is_available() else "cpu",
                precision="16-mixed" if torch.cuda.is_available() else 32,
                threads=0,
                verbose=False
            )
            
            # å‡†å¤‡éªŒè¯é›†
            val_size = min(1000, len(test_texts) // 3)
            val_texts = test_texts[:val_size]
            val_labels = test_labels[:val_size]
            final_test_texts = test_texts[val_size:]
            final_test_labels = test_labels[val_size:]
            
            print(f"   è®­ç»ƒæ ·æœ¬: {len(train_texts):,}")
            print(f"   éªŒè¯æ ·æœ¬: {val_size:,}")
            print(f"   æµ‹è¯•æ ·æœ¬: {len(final_test_texts):,}")
            
            # è®­ç»ƒæ¨¡å‹
            print(f"   ğŸ¯ å¼€å§‹è®­ç»ƒ...")
            classifier.fit(train_labels, train_texts, y_val=val_labels, X_val=val_texts)
            
            # é¢„æµ‹
            print(f"   ğŸ”® é¢„æµ‹ä¸­...")
            predictions_df = classifier.predict(final_test_texts, format='dataframe', top_k=5)
            
            # è®¡ç®—æŒ‡æ ‡
            y_true = final_test_labels
            y_pred = predictions_df['class_1'].tolist()
            
            accuracy = accuracy_score(y_true, y_pred)
            top_3_acc = sum(
                true_label in [predictions_df.iloc[i][f'class_{j}'] for j in range(1, 4)]
                for i, true_label in enumerate(y_true)
            ) / len(y_true)
            top_5_acc = sum(
                true_label in [predictions_df.iloc[i][f'class_{j}'] for j in range(1, 6)]
                for i, true_label in enumerate(y_true)
            ) / len(y_true)
            
            training_time = time.time() - start_time
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            detailed_results = pd.DataFrame({
                'true_label': y_true,
                'predicted_label': y_pred,
                'confidence': predictions_df['prob_1'].tolist(),
                'correct': [t == p for t, p in zip(y_true, y_pred)]
            })
            detailed_results.to_csv(results_dir / f"{model_key}_detailed_results.csv", 
                                  index=False, encoding='utf-8')
            
            result = {
                'model_key': model_key,
                'model_name': model_config['name'],
                'description': model_config['description'],
                'notes': model_config['notes'],
                'accuracy': accuracy,
                'top_3_accuracy': top_3_acc,
                'top_5_accuracy': top_5_acc,
                'training_time_minutes': training_time / 60,
                'test_samples': len(final_test_labels),
                'status': 'success'
            }
            
            print(f"âœ… è®­ç»ƒå®Œæˆ!")
            print(f"   å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
            print(f"   Top-3å‡†ç¡®ç‡: {top_3_acc:.4f} ({top_3_acc*100:.2f}%)")
            print(f"   Top-5å‡†ç¡®ç‡: {top_5_acc:.4f} ({top_5_acc*100:.2f}%)")
            print(f"   è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
            
            return result
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'model_key': model_key,
                'model_name': model_config['name'],
                'description': model_config['description'],
                'notes': model_config['notes'],
                'accuracy': 0.0,
                'top_3_accuracy': 0.0,
                'top_5_accuracy': 0.0,
                'training_time_minutes': 0.0,
                'test_samples': 0,
                'status': 'failed',
                'error': str(e)
            }

    def run_model_comparison(csv_path, models_to_test=None, max_samples=None):
        """è¿è¡Œæ¨¡å‹å¯¹æ¯”"""
        print("ğŸš€ å¼€å§‹æ›´æ–°ç‰ˆä¸­æ–‡BERTæ¨¡å‹å¯¹æ¯”è¯„ä¼°")
        print("ğŸ”§ å·²ç§»é™¤TensorFlowä¾èµ–æ¨¡å‹ï¼Œæ·»åŠ æ–°çš„PyTorchæ¨¡å‹")
        print("=" * 60)
        
        results_dir = setup_comparison_environment()
        
        train_texts, train_labels, test_texts, test_labels, hierarchy, data_info = load_and_prepare_data(
            csv_path, max_samples=max_samples)
        
        # è®­ç»ƒé…ç½®
        training_config = {
            'max_epochs': 3,
            'patience': 2,
            'max_seq_length': 256
        }
        
        if models_to_test is None:
            models_to_test = list(UPDATED_MODELS.keys())
        
        print(f"\nğŸ“‹ å°†æµ‹è¯•ä»¥ä¸‹æ¨¡å‹:")
        for model_key in models_to_test:
            model_config = UPDATED_MODELS[model_key]
            print(f"   {model_key}: {model_config['description']}")
            print(f"      è¯´æ˜: {model_config['notes']}")
        
        total_start_time = time.time()
        results = []
        
        for i, model_key in enumerate(models_to_test, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ“Š è¿›åº¦: {i}/{len(models_to_test)} - {model_key}")
            print(f"{'='*60}")
            
            model_config = UPDATED_MODELS[model_key]
            result = train_single_model(
                model_key, model_config, train_texts, train_labels,
                test_texts, test_labels, hierarchy, results_dir, training_config
            )
            results.append(result)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            interim_df = pd.DataFrame(results)
            interim_df.to_csv(results_dir / "interim_results.csv", index=False, encoding='utf-8')
        
        total_time = time.time() - total_start_time
        generate_comparison_report(results, data_info, total_time, results_dir)
        
        return results, results_dir

    def generate_comparison_report(results, data_info, total_time, results_dir):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\nğŸ“Š ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
        
        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values('accuracy', ascending=False)
        
        results_df.to_csv(results_dir / "updated_comparison_results.csv", index=False, encoding='utf-8')
        
        report = {
            'experiment_info': {
                'experiment_type': 'Updated Chinese BERT Model Comparison (No TensorFlow)',
                'timestamp': datetime.now().isoformat(),
                'total_time_hours': total_time / 3600,
                'data_info': data_info,
                'python_version': f"Python {'.'.join(map(str, __import__('sys').version_info[:3]))}",
                'pytorch_version': torch.__version__
            },
            'results': results_df.to_dict('records'),
            'summary': {
                'best_model': results_df.iloc[0]['model_name'] if len(results_df) > 0 else None,
                'best_accuracy': results_df.iloc[0]['accuracy'] if len(results_df) > 0 else 0,
                'models_tested': len(results_df),
                'successful_models': len(results_df[results_df['status'] == 'success']),
                'failed_models': len(results_df[results_df['status'] == 'failed'])
            }
        }
        
        with open(results_dir / "updated_comparison_report.json", 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ æ›´æ–°ç‰ˆæ¨¡å‹å¯¹æ¯”å®Œæˆ!")
        print(f"ğŸ“ˆ ç»“æœæ€»ç»“:")
        print(f"   æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
        print(f"   æˆåŠŸæ¨¡å‹: {report['summary']['successful_models']}/{report['summary']['models_tested']}")
        print(f"   å¤±è´¥æ¨¡å‹: {report['summary']['failed_models']}")
        
        if len(results_df) > 0:
            successful_results = results_df[results_df['status'] == 'success']
            
            if len(successful_results) > 0:
                print(f"\nğŸ† æ¨¡å‹æ’å (æŒ‰å‡†ç¡®ç‡):")
                for i, (_, row) in enumerate(successful_results.iterrows(), 1):
                    print(f"   {i}. {row['description']}")
                    print(f"      æ¨¡å‹: {row['model_name']}")
                    print(f"      å‡†ç¡®ç‡: {row['accuracy']:.4f} ({row['accuracy']*100:.2f}%)")
                    print(f"      Top-3: {row['top_3_accuracy']:.4f} ({row['top_3_accuracy']*100:.2f}%)")
                    print(f"      Top-5: {row['top_5_accuracy']:.4f} ({row['top_5_accuracy']*100:.2f}%)")
                    print(f"      è®­ç»ƒæ—¶é—´: {row['training_time_minutes']:.1f} åˆ†é’Ÿ")
                    print(f"      è¯´æ˜: {row['notes']}")
                    print()
            
            failed_results = results_df[results_df['status'] == 'failed']
            if len(failed_results) > 0:
                print(f"âŒ å¤±è´¥æ¨¡å‹:")
                for _, row in failed_results.iterrows():
                    print(f"   - {row['description']}: {row.get('error', 'Unknown error')}")
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {results_dir}")

    def main():
        """ä¸»å‡½æ•°"""
        print("ğŸš€ æ›´æ–°ç‰ˆä¸­æ–‡BERTæ¨¡å‹å¯¹æ¯”è¯„ä¼°")
        print("ğŸ”§ ç§»é™¤TensorFlowä¾èµ–ï¼Œæ”¯æŒPython 3.13")
        print("=" * 60)
        
        print("é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
        print("1. å®Œæ•´æµ‹è¯• (æ‰€æœ‰6ä¸ªæ¨¡å‹)")
        print("2. å¿«é€Ÿæµ‹è¯• (é™åˆ¶æ ·æœ¬æ•°)")
        print("3. ä¼˜è´¨æ¨¡å‹æµ‹è¯• (åªæµ‹è¯•å“ˆå·¥å¤§3ä¸ªæ¨¡å‹)")
        print("4. åŸºå‡†å¯¹æ¯”æµ‹è¯• (Google + HFL RoBERTa)")
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1-4): ")
        
        if choice == "1":
            models_to_test = list(UPDATED_MODELS.keys())
            max_samples = None
        elif choice == "2":
            models_to_test = list(UPDATED_MODELS.keys())
            max_samples = 15000
        elif choice == "3":
            models_to_test = ['hfl_roberta', 'hfl_bert', 'hfl_electra']
            max_samples = 15000
        elif choice == "4":
            models_to_test = ['google_bert', 'hfl_roberta']
            max_samples = 10000
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨å¿«é€Ÿæµ‹è¯•")
            models_to_test = list(UPDATED_MODELS.keys())
            max_samples = 15000
        
        csv_path = "newjob1_sortall.csv"
        if not os.path.exists(csv_path):
            print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {csv_path}")
            return
        
        print(f"\nğŸ¯ å³å°†æµ‹è¯•çš„æ¨¡å‹:")
        for model_key in models_to_test:
            config = UPDATED_MODELS[model_key]
            print(f"   - {config['description']}")
            print(f"     {config['notes']}")
        
        if max_samples:
            print(f"\nâš¡ å¿«é€Ÿæ¨¡å¼: é™åˆ¶æ ·æœ¬æ•°ä¸º {max_samples:,}")
        
        confirm = input(f"\nç¡®è®¤å¼€å§‹æµ‹è¯•? (y/N): ")
        if confirm.lower() != 'y':
            print("æµ‹è¯•å·²å–æ¶ˆ")
            return
        
        results, results_dir = run_model_comparison(csv_path, models_to_test, max_samples)
        
        print(f"\nğŸ¯ æ›´æ–°ç‰ˆå¯¹æ¯”å®Œæˆ! æŸ¥çœ‹è¯¦ç»†ç»“æœ: {results_dir}")

    main()