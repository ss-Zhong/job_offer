#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­æ–‡èŒä¸šåˆ†ç±»æ•°æ®å¢å¼ºä¸é¢„å¤„ç†æ¨¡å—
ä¸“ä¸ºISCOç¼–å·è¯†åˆ«ä»»åŠ¡è®¾è®¡çš„å¢å¼ºç‰ˆé¢„å¤„ç†å™¨
"""

import re
import jieba
import jieba.posseg as pseg
import pandas as pd
import numpy as np
import random
import ast
from typing import List, Dict, Tuple, Optional
import json
from pathlib import Path
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

class ChineseJobTextProcessor:
    """ä¸­æ–‡èŒä¸šæ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def __init__(self, 
                 stopwords_path: Optional[str] = None,
                 job_synonyms_path: Optional[str] = None,
                 industry_terms_path: Optional[str] = None):
        
        # åŠ è½½åœç”¨è¯
        self.stopwords = self._load_stopwords(stopwords_path)
        
        # åŠ è½½èŒä¸šåŒä¹‰è¯å­—å…¸
        self.job_synonyms = self._load_job_synonyms(job_synonyms_path)
        
        # åŠ è½½è¡Œä¸šæœ¯è¯­æ ‡å‡†åŒ–å­—å…¸
        self.industry_terms = self._load_industry_terms(industry_terms_path)
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self._compile_patterns()
        
        # åˆå§‹åŒ–jieba
        self._init_jieba()
        
        print("âœ… ä¸­æ–‡èŒä¸šæ–‡æœ¬å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_stopwords(self, path: Optional[str]) -> set:
        """åŠ è½½åœç”¨è¯"""
        if path and Path(path).exists():
            with open(path, 'r', encoding='utf-8') as f:
                return set(line.strip() for line in f if line.strip())
        
        # é»˜è®¤åœç”¨è¯ï¼ˆé’ˆå¯¹èŒä¸šæè¿°ä¼˜åŒ–ï¼‰
        return {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'éƒ½', 'ä¸€', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¦', 'ä¼š', 'ç€', 'çœ‹', 'å¥½', 'è‡ªå·±',
            'è¿™', 'é‚£', 'é‡Œ', 'ä¸º', 'ä»–', 'å¥¹', 'å®ƒ', 'ä½†', 'è€Œ', 'æˆ–', 'ä¸', 'åŠ', 'ä»¥', 'å¯ä»¥', 'èƒ½å¤Ÿ', 'åº”è¯¥', 'éœ€è¦', 'é€šè¿‡',
            'ç”±äº', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è™½ç„¶', 'ç„¶è€Œ', 'å› æ­¤', 'äºæ˜¯', 'å¹¶ä¸”', 'è€Œä¸”', 'ä½†æ˜¯', 'ç›¸å…³', 'ç­‰ç­‰', 'åŒ…æ‹¬',
            'å…·æœ‰', 'å…·å¤‡', 'æ‹¥æœ‰', 'ç†Ÿæ‚‰', 'äº†è§£', 'æŒæ¡', 'ä¼˜å…ˆ', 'ä¼˜ç§€', 'è‰¯å¥½', 'è¾ƒå¼º', 'èƒ½åŠ›', 'å·¥ä½œ', 'ç»éªŒ', 'è¦æ±‚',
            'å²—ä½', 'èŒä½', 'ä»»èŒ', 'ä»äº‹', 'è´Ÿè´£', 'å®Œæˆ', 'è¿›è¡Œ', 'ç»„ç»‡', 'åè°ƒ', 'ç®¡ç†', 'å»ºç«‹', 'åˆ¶å®š', 'å®æ–½', 'æ‰§è¡Œ'
        }

    def _load_job_synonyms(self, path: Optional[str]) -> Dict[str, List[str]]:
        """åŠ è½½èŒä¸šåŒä¹‰è¯å­—å…¸"""
        if path and Path(path).exists():
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        
        # é»˜è®¤èŒä¸šåŒä¹‰è¯å­—å…¸
        return {
            'CEO': ['é¦–å¸­æ‰§è¡Œå®˜', 'æ€»è£', 'æ€»ç»ç†', 'æ‰§è¡Œæ€»è£', 'è‘£äº‹é•¿'],
            'äººäº‹': ['äººåŠ›èµ„æº', 'äººäº‹ç®¡ç†', 'HR', 'äººå‘˜ç®¡ç†', 'äººäº‹è¡Œæ”¿'],
            'é‡‡è´­': ['é‡‡è´­ç®¡ç†', 'ä¾›åº”é“¾', 'ç‰©æ–™é‡‡è´­', 'å•†å“é‡‡è´­', 'æˆ˜ç•¥é‡‡è´­'],
            'é”€å”®': ['è¥é”€', 'å¸‚åœºé”€å”®', 'ä¸šåŠ¡', 'å•†åŠ¡', 'å®¢æˆ·ç»ç†'],
            'è´¢åŠ¡': ['ä¼šè®¡', 'è´¢åŠ¡ç®¡ç†', 'èµ„é‡‘ç®¡ç†', 'æˆæœ¬æ§åˆ¶', 'å®¡è®¡'],
            'æŠ€æœ¯': ['ç ”å‘', 'å¼€å‘', 'å·¥ç¨‹å¸ˆ', 'æŠ€æœ¯å¼€å‘', 'è½¯ä»¶å¼€å‘'],
            'è¿è¥': ['è¿è¥ç®¡ç†', 'ä¸šåŠ¡è¿è¥', 'äº§å“è¿è¥', 'æ•°æ®è¿è¥'],
            'è¡Œæ”¿': ['è¡Œæ”¿ç®¡ç†', 'åŠå…¬å®¤ç®¡ç†', 'åå‹¤', 'ç»¼åˆç®¡ç†'],
            'åŸ¹è®­': ['åŸ¹è®­ç®¡ç†', 'äººæ‰å‘å±•', 'å­¦ä¹ å‘å±•', 'æ•™è‚²åŸ¹è®­'],
            'è´¨é‡': ['è´¨é‡ç®¡ç†', 'å“è´¨æ§åˆ¶', 'QA', 'QC', 'è´¨é‡ä¿è¯'],
            'é¡¹ç›®': ['é¡¹ç›®ç®¡ç†', 'é¡¹ç›®ç»ç†', 'é¡¹ç›®ä¸»ç®¡', 'é¡¹ç›®åè°ƒ'],
            'å®¢æœ': ['å®¢æˆ·æœåŠ¡', 'å”®åæœåŠ¡', 'å®¢æˆ·æ”¯æŒ', 'æœåŠ¡ç®¡ç†']
        }

    def _load_industry_terms(self, path: Optional[str]) -> Dict[str, str]:
        """åŠ è½½è¡Œä¸šæœ¯è¯­æ ‡å‡†åŒ–å­—å…¸"""
        if path and Path(path).exists():
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        
        # é»˜è®¤è¡Œä¸šæœ¯è¯­æ ‡å‡†åŒ–
        return {
            'IT': 'ä¿¡æ¯æŠ€æœ¯', 'AI': 'äººå·¥æ™ºèƒ½', 'ERP': 'ä¼ä¸šèµ„æºè§„åˆ’',
            'CRM': 'å®¢æˆ·å…³ç³»ç®¡ç†', 'SCM': 'ä¾›åº”é“¾ç®¡ç†', 'OA': 'åŠå…¬è‡ªåŠ¨åŒ–',
            'KPI': 'å…³é”®ç»©æ•ˆæŒ‡æ ‡', 'ROI': 'æŠ•èµ„å›æŠ¥ç‡', 'SOP': 'æ ‡å‡†ä½œä¸šç¨‹åº',
            'B2B': 'ä¼ä¸šå¯¹ä¼ä¸š', 'B2C': 'ä¼ä¸šå¯¹æ¶ˆè´¹è€…', 'O2O': 'çº¿ä¸Šåˆ°çº¿ä¸‹',
            'UI': 'ç”¨æˆ·ç•Œé¢', 'UE': 'ç”¨æˆ·ä½“éªŒ', 'UX': 'ç”¨æˆ·ä½“éªŒ',
            'API': 'åº”ç”¨ç¨‹åºæ¥å£', 'SDK': 'è½¯ä»¶å¼€å‘å·¥å…·åŒ…',
            'SQL': 'ç»“æ„åŒ–æŸ¥è¯¢è¯­è¨€', 'HTML': 'è¶…æ–‡æœ¬æ ‡è®°è¯­è¨€',
            'CEO': 'é¦–å¸­æ‰§è¡Œå®˜', 'CFO': 'é¦–å¸­è´¢åŠ¡å®˜', 'CTO': 'é¦–å¸­æŠ€æœ¯å®˜',
            'COO': 'é¦–å¸­è¿è¥å®˜', 'CHO': 'é¦–å¸­äººåŠ›èµ„æºå®˜', 'CMO': 'é¦–å¸­è¥é”€å®˜'
        }

    def _compile_patterns(self):
        """é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        # æ¸…ç†æ¨¡å¼
        self.email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        self.phone_pattern = re.compile(r'1[3-9]\d{9}|0\d{2,3}-?\d{7,8}')
        self.url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        self.number_pattern = re.compile(r'\d+')
        
        # èŒä¸šå±‚çº§å…³é”®è¯
        self.level_patterns = {
            'é«˜çº§': re.compile(r'(é«˜çº§|èµ„æ·±|é¦–å¸­|æ€»|å‰¯æ€»|ä¸»ä»»|ç»ç†|æ€»ç›‘|è‘£äº‹)'),
            'ä¸­çº§': re.compile(r'(ä¸»ç®¡|å‰¯ä¸»ç®¡|ç»„é•¿|leader|è´Ÿè´£äºº)'),
            'åˆçº§': re.compile(r'(ä¸“å‘˜|åŠ©ç†|æ–‡å‘˜|å®ä¹ |è§ä¹ |åˆçº§)')
        }
        
        # æŠ€èƒ½å…³é”®è¯æ¨¡å¼
        self.skill_pattern = re.compile(r'(ç†Ÿç»ƒ|ç†Ÿæ‚‰|æŒæ¡|ç²¾é€š|äº†è§£|ä¼š|èƒ½å¤Ÿ|å…·å¤‡).{0,20}(è½¯ä»¶|ç³»ç»Ÿ|å·¥å…·|æŠ€èƒ½|æŠ€æœ¯|è¯­è¨€|å¹³å°)')

    def _init_jieba(self):
        """åˆå§‹åŒ–jiebaåˆ†è¯å™¨å¹¶æ·»åŠ èŒä¸šç›¸å…³è¯æ±‡"""
        # æ·»åŠ èŒä¸šç›¸å…³è¯æ±‡åˆ°jiebaè¯å…¸
        job_terms = [
            'é¦–å¸­æ‰§è¡Œå®˜', 'é¦–å¸­è´¢åŠ¡å®˜', 'é¦–å¸­æŠ€æœ¯å®˜', 'äººåŠ›èµ„æº',
            'å¸‚åœºè¥é”€', 'å®¢æˆ·æœåŠ¡', 'ä¾›åº”é“¾ç®¡ç†', 'é¡¹ç›®ç®¡ç†',
            'æ•°æ®åˆ†æ', 'è½¯ä»¶å¼€å‘', 'äº§å“ç»ç†', 'è¿è¥ç®¡ç†'
        ]
        
        for term in job_terms:
            jieba.add_word(term)

    def clean_text(self, text: str) -> str:
        """æ–‡æœ¬æ¸…ç†"""
        if pd.isna(text) or not text:
            return ""
        
        text = str(text)
        
        # ç§»é™¤é‚®ç®±ã€ç”µè¯ã€ç½‘å€
        text = self.email_pattern.sub('', text)
        text = self.phone_pattern.sub('', text)
        text = self.url_pattern.sub('', text)
        
        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹
        text = re.sub(r'[^\u4e00-\u9fff\u0041-\u005a\u0061-\u007a0-9\sï¼Œã€‚ã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘]', '', text)
        
        return text

    def extract_structure_info(self, text: str) -> Dict[str, str]:
        """æå–èŒä½æè¿°çš„ç»“æ„åŒ–ä¿¡æ¯"""
        structure = {
            'responsibilities': '',  # å²—ä½èŒè´£
            'requirements': '',      # ä»»èŒè¦æ±‚
            'skills': '',           # æŠ€èƒ½è¦æ±‚
            'experience': '',       # ç»éªŒè¦æ±‚
            'education': ''         # å­¦å†è¦æ±‚
        }
        
        # èŒè´£å…³é”®è¯
        resp_keywords = ['èŒè´£', 'å·¥ä½œå†…å®¹', 'ä¸»è¦èŒè´£', 'å…·ä½“å·¥ä½œ', 'å·¥ä½œèŒè´£']
        req_keywords = ['è¦æ±‚', 'ä»»èŒè¦æ±‚', 'å²—ä½è¦æ±‚', 'æ‹›è˜è¦æ±‚', 'åº”è˜è¦æ±‚']
        skill_keywords = ['æŠ€èƒ½', 'èƒ½åŠ›', 'ä¸“ä¸šæŠ€èƒ½', 'æŠ€æœ¯è¦æ±‚']
        exp_keywords = ['ç»éªŒ', 'å·¥ä½œç»éªŒ', 'ä»ä¸šç»éªŒ', 'ç›¸å…³ç»éªŒ']
        edu_keywords = ['å­¦å†', 'æ•™è‚²', 'ä¸“ä¸š', 'æ¯•ä¸š']
        
        # åˆ†æ®µæå–
        sections = re.split(r'[1-9]ã€|[1-9]\.|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€', text)
        
        for section in sections:
            section_lower = section.lower()
            
            # åˆ¤æ–­æ®µè½ç±»å‹å¹¶æå–ç›¸å…³å†…å®¹
            if any(kw in section for kw in resp_keywords):
                structure['responsibilities'] += section + ' '
            elif any(kw in section for kw in req_keywords):
                structure['requirements'] += section + ' '
            elif any(kw in section for kw in skill_keywords):
                structure['skills'] += section + ' '
            elif any(kw in section for kw in exp_keywords):
                structure['experience'] += section + ' '
            elif any(kw in section for kw in edu_keywords):
                structure['education'] += section + ' '
        
        # æ¸…ç†å’Œæˆªæ–­
        for key in structure:
            structure[key] = structure[key].strip()[:200]  # é™åˆ¶é•¿åº¦
        
        return structure

    def normalize_industry_terms(self, text: str) -> str:
        """æ ‡å‡†åŒ–è¡Œä¸šæœ¯è¯­"""
        for abbr, full in self.industry_terms.items():
            # æ›¿æ¢ç‹¬ç«‹çš„ç¼©å†™è¯
            text = re.sub(r'\b' + re.escape(abbr) + r'\b', full, text, flags=re.IGNORECASE)
        
        return text

    def segment_text(self, text: str, extract_pos: bool = True) -> List[str]:
        """ä¸­æ–‡åˆ†è¯"""
        if extract_pos:
            # ä½¿ç”¨è¯æ€§æ ‡æ³¨ï¼Œåªä¿ç•™æœ‰æ„ä¹‰çš„è¯
            words = []
            for word, flag in pseg.cut(text):
                # ä¿ç•™åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€ä¸“æœ‰åè¯ç­‰
                if (flag.startswith(('n', 'v', 'a', 'nr', 'nt', 'nz', 'eng')) 
                    and len(word.strip()) > 1 
                    and word not in self.stopwords):
                    words.append(word)
            return words
        else:
            # æ™®é€šåˆ†è¯
            words = jieba.lcut(text)
            return [w for w in words if len(w.strip()) > 1 and w not in self.stopwords]

    def extract_features(self, text: str) -> Dict[str, any]:
        """æå–æ–‡æœ¬ç‰¹å¾"""
        features = {}
        
        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        features['text_length'] = len(text)
        features['word_count'] = len(self.segment_text(text, extract_pos=False))
        
        # èŒä¸šå±‚çº§ç‰¹å¾
        for level, pattern in self.level_patterns.items():
            features[f'has_{level}_level'] = bool(pattern.search(text))
        
        # æŠ€èƒ½è¦æ±‚ç‰¹å¾
        skill_matches = self.skill_pattern.findall(text)
        features['skill_count'] = len(skill_matches)
        features['has_skill_requirements'] = len(skill_matches) > 0
        
        # æ•°å­—ç‰¹å¾ï¼ˆå¹´é™ã€è–ªèµ„ç­‰ï¼‰
        numbers = self.number_pattern.findall(text)
        features['number_count'] = len(numbers)
        
        # å…³é”®è¯ç‰¹å¾
        key_terms = ['ç®¡ç†', 'é”€å”®', 'æŠ€æœ¯', 'è´¢åŠ¡', 'äººäº‹', 'å¸‚åœº', 'å®¢æœ', 'é‡‡è´­', 'è¿è¥', 'è¡Œæ”¿']
        for term in key_terms:
            features[f'has_{term}'] = term in text
        
        return features


class ChineseJobDataAugmenter:
    """ä¸­æ–‡èŒä¸šæ•°æ®å¢å¼ºå™¨"""
    
    def __init__(self, processor: ChineseJobTextProcessor):
        self.processor = processor
        
        # å¢å¼ºç­–ç•¥é…ç½®
        self.augmentation_strategies = {
            'synonym_replacement': 0.3,    # åŒä¹‰è¯æ›¿æ¢æ¦‚ç‡
            'structure_reorder': 0.2,      # ç»“æ„é‡æ’æ¦‚ç‡  
            'keyword_enhancement': 0.25,   # å…³é”®è¯å¢å¼ºæ¦‚ç‡
            'paraphrase': 0.15,           # è¯­è¨€é‡ç»„æ¦‚ç‡
            'noise_injection': 0.1         # å™ªå£°æ³¨å…¥æ¦‚ç‡
        }
        
        print("âœ… ä¸­æ–‡èŒä¸šæ•°æ®å¢å¼ºå™¨åˆå§‹åŒ–å®Œæˆ")

    def synonym_replacement(self, text: str, replacement_rate: float = 0.15) -> str:
        """åŒä¹‰è¯æ›¿æ¢å¢å¼º"""
        words = self.processor.segment_text(text, extract_pos=False)
        
        # è®¡ç®—éœ€è¦æ›¿æ¢çš„è¯æ•°
        num_replace = max(1, int(len(words) * replacement_rate))
        
        # éšæœºé€‰æ‹©è¦æ›¿æ¢çš„ä½ç½®
        replace_indices = random.sample(range(len(words)), min(num_replace, len(words)))
        
        for idx in replace_indices:
            word = words[idx]
            
            # æŸ¥æ‰¾åŒä¹‰è¯
            for key, synonyms in self.processor.job_synonyms.items():
                if word in synonyms:
                    # éšæœºé€‰æ‹©ä¸€ä¸ªåŒä¹‰è¯æ›¿æ¢
                    replacement = random.choice([s for s in synonyms if s != word])
                    words[idx] = replacement
                    break
        
        return ''.join(words)

    def structure_reorder(self, text: str) -> str:
        """ç»“æ„é‡æ’å¢å¼º"""
        # æå–ç»“æ„åŒ–ä¿¡æ¯
        structure = self.processor.extract_structure_info(text)
        
        # é‡æ–°ç»„ç»‡é¡ºåº
        sections = []
        for key, content in structure.items():
            if content.strip():
                sections.append(content)
        
        if len(sections) > 1:
            # éšæœºæ‰“ä¹±éƒ¨åˆ†æ®µè½é¡ºåº
            random.shuffle(sections)
            return ' '.join(sections)
        
        return text

    def keyword_enhancement(self, text: str) -> str:
        """å…³é”®è¯å¢å¼º"""
        # æå–å…³é”®è¯
        words = self.processor.segment_text(text, extract_pos=True)
        
        # è¯†åˆ«èŒä¸šç›¸å…³å…³é”®è¯
        job_keywords = []
        for word in words:
            for category, synonyms in self.processor.job_synonyms.items():
                if word in synonyms:
                    job_keywords.append(word)
                    break
        
        # éšæœºé‡å¤ä¸€äº›å…³é”®è¯ï¼ˆæ¨¡æ‹Ÿé‡è¦æ€§å¼ºè°ƒï¼‰
        if job_keywords:
            enhanced_keyword = random.choice(job_keywords)
            # åœ¨æ–‡æœ¬æœ«å°¾æ·»åŠ å…³é”®è¯å¼ºè°ƒ
            text += f" é‡ç‚¹å…³æ³¨{enhanced_keyword}ç›¸å…³å·¥ä½œ"
        
        return text

    def paraphrase_text(self, text: str) -> str:
        """è¯­è¨€é‡ç»„å¢å¼º"""
        # ç®€å•çš„è¯­è¨€é‡ç»„ç­–ç•¥
        sentences = re.split(r'[ã€‚ï¼›ï¼ï¼Ÿ]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) > 1:
            # éšæœºè°ƒæ•´å¥å­é¡ºåº
            random.shuffle(sentences)
            return 'ã€‚'.join(sentences) + 'ã€‚'
        
        return text

    def inject_noise(self, text: str, noise_rate: float = 0.05) -> str:
        """å™ªå£°æ³¨å…¥å¢å¼º"""
        chars = list(text)
        num_noise = max(1, int(len(chars) * noise_rate))
        
        # éšæœºæ’å…¥æˆ–åˆ é™¤å°‘é‡å­—ç¬¦
        for _ in range(num_noise):
            if random.random() < 0.5 and len(chars) > 10:
                # åˆ é™¤å­—ç¬¦
                idx = random.randint(0, len(chars) - 1)
                if chars[idx] not in 'ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ':  # ä¿ç•™é‡è¦æ ‡ç‚¹
                    chars.pop(idx)
            else:
                # æ’å…¥ç©ºæ ¼ï¼ˆæ¨¡æ‹ŸOCRé”™è¯¯ï¼‰
                idx = random.randint(0, len(chars))
                chars.insert(idx, ' ')
        
        return ''.join(chars)

    def augment_single_text(self, text: str, num_augmented: int = 3) -> List[str]:
        """å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œå¢å¼º"""
        augmented_texts = [text]  # åŒ…å«åŸæ–‡
        
        strategies = list(self.augmentation_strategies.keys())
        
        for _ in range(num_augmented):
            current_text = text
            
            # éšæœºé€‰æ‹©1-2ä¸ªå¢å¼ºç­–ç•¥ç»„åˆä½¿ç”¨
            selected_strategies = random.sample(strategies, random.randint(1, 2))
            
            for strategy in selected_strategies:
                if random.random() < self.augmentation_strategies[strategy]:
                    if strategy == 'synonym_replacement':
                        current_text = self.synonym_replacement(current_text)
                    elif strategy == 'structure_reorder':
                        current_text = self.structure_reorder(current_text)
                    elif strategy == 'keyword_enhancement':
                        current_text = self.keyword_enhancement(current_text)
                    elif strategy == 'paraphrase':
                        current_text = self.paraphrase_text(current_text)
                    elif strategy == 'noise_injection':
                        current_text = self.inject_noise(current_text)
            
            # æ¸…ç†å¢å¼ºåçš„æ–‡æœ¬
            current_text = self.processor.clean_text(current_text)
            
            if current_text and current_text != text:  # ç¡®ä¿å¢å¼ºåæ–‡æœ¬æœ‰æ•ˆä¸”ä¸åŒ
                augmented_texts.append(current_text)
        
        return augmented_texts


class ImbalancedDataHandler:
    """ä¸å¹³è¡¡æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, min_samples_per_class: int = 5):
        self.min_samples_per_class = min_samples_per_class
        
    def analyze_class_distribution(self, labels: List[str]) -> Dict[str, int]:
        """åˆ†æç±»åˆ«åˆ†å¸ƒ"""
        distribution = Counter(labels)
        
        print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒåˆ†æ:")
        print(f"   æ€»ç±»åˆ«æ•°: {len(distribution)}")
        print(f"   æ€»æ ·æœ¬æ•°: {sum(distribution.values())}")
        print(f"   å¹³å‡æ¯ç±»æ ·æœ¬æ•°: {sum(distribution.values()) / len(distribution):.1f}")
        
        # åˆ†ç±»ç»Ÿè®¡
        rare_classes = {k: v for k, v in distribution.items() if v < self.min_samples_per_class}
        medium_classes = {k: v for k, v in distribution.items() if self.min_samples_per_class <= v < 20}
        common_classes = {k: v for k, v in distribution.items() if v >= 20}
        
        print(f"   ç¨€æœ‰ç±»åˆ« (<{self.min_samples_per_class}): {len(rare_classes)}")
        print(f"   ä¸­ç­‰ç±»åˆ« ({self.min_samples_per_class}-19): {len(medium_classes)}")
        print(f"   å¸¸è§ç±»åˆ« (>=20): {len(common_classes)}")
        
        return {
            'all': distribution,
            'rare': rare_classes,
            'medium': medium_classes,
            'common': common_classes
        }
    
    def balance_dataset(self, 
                       texts: List[str], 
                       labels: List[str], 
                       augmenter: ChineseJobDataAugmenter,
                       target_samples: int = 10) -> Tuple[List[str], List[str]]:
        """å¹³è¡¡æ•°æ®é›†"""
        
        distribution = self.analyze_class_distribution(labels)
        
        balanced_texts = []
        balanced_labels = []
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        class_data = defaultdict(list)
        for text, label in zip(texts, labels):
            class_data[label].append(text)
        
        print(f"\nğŸ”„ å¼€å§‹æ•°æ®å¹³è¡¡å¤„ç†...")
        
        for label, class_texts in class_data.items():
            current_count = len(class_texts)
            
            # æ·»åŠ åŸå§‹æ•°æ®
            balanced_texts.extend(class_texts)
            balanced_labels.extend([label] * current_count)
            
            # å¦‚æœéœ€è¦å¢å¼º
            if current_count < target_samples:
                needed = target_samples - current_count
                print(f"   {label}: åŸæœ‰{current_count}ä¸ªæ ·æœ¬ï¼Œéœ€å¢å¼º{needed}ä¸ª")
                
                # å¯¹ç¨€æœ‰ç±»åˆ«è¿›è¡Œæ›´å¼ºçš„å¢å¼º
                augment_per_sample = max(1, needed // current_count + 1)
                
                augmented_count = 0
                for text in class_texts:
                    if augmented_count >= needed:
                        break
                    
                    # ç”Ÿæˆå¢å¼ºæ ·æœ¬
                    augmented = augmenter.augment_single_text(text, augment_per_sample)
                    
                    # è·³è¿‡åŸæ–‡ï¼Œåªè¦å¢å¼ºæ ·æœ¬
                    for aug_text in augmented[1:]:
                        if augmented_count < needed:
                            balanced_texts.append(aug_text)
                            balanced_labels.append(label)
                            augmented_count += 1
        
        print(f"âœ… æ•°æ®å¹³è¡¡å®Œæˆ:")
        print(f"   åŸå§‹æ ·æœ¬: {len(texts)}")
        print(f"   å¹³è¡¡åæ ·æœ¬: {len(balanced_texts)}")
        
        return balanced_texts, balanced_labels


class EnhancedJobDataProcessor:
    """å¢å¼ºç‰ˆèŒä¸šæ•°æ®å¤„ç†å™¨ - æ•´åˆæ‰€æœ‰åŠŸèƒ½"""
    
    def __init__(self, 
                 stopwords_path: Optional[str] = None,
                 job_synonyms_path: Optional[str] = None,
                 industry_terms_path: Optional[str] = None):
        
        # åˆå§‹åŒ–å„ç»„ä»¶
        self.text_processor = ChineseJobTextProcessor(
            stopwords_path, job_synonyms_path, industry_terms_path
        )
        self.augmenter = ChineseJobDataAugmenter(self.text_processor)
        self.balance_handler = ImbalancedDataHandler()
        
        print("ğŸš€ å¢å¼ºç‰ˆèŒä¸šæ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def safe_train_test_split(self, df, test_size=0.2, random_state=42):
        """å®‰å…¨çš„è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†ï¼Œå¤„ç†å•æ ·æœ¬ç±»åˆ«"""
        from sklearn.model_selection import train_test_split
        
        # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
        label_counts = Counter(df['isco_code'])
        single_sample_classes = [label for label, count in label_counts.items() if count == 1]
        multi_sample_classes = [label for label, count in label_counts.items() if count > 1]
        
        print(f"   æ£€æµ‹åˆ° {len(single_sample_classes)} ä¸ªå•æ ·æœ¬ç±»åˆ«")
        print(f"   æ£€æµ‹åˆ° {len(multi_sample_classes)} ä¸ªå¤šæ ·æœ¬ç±»åˆ«")
        
        train_dfs = []
        test_dfs = []
        
        # å¤„ç†å•æ ·æœ¬ç±»åˆ« - å…¨éƒ¨æ”¾å…¥è®­ç»ƒé›†
        if single_sample_classes:
            single_df = df[df['isco_code'].isin(single_sample_classes)]
            train_dfs.append(single_df)
            print(f"   å•æ ·æœ¬ç±»åˆ« {len(single_sample_classes)} ä¸ªå·²åŠ å…¥è®­ç»ƒé›†")
        
        # å¤„ç†å¤šæ ·æœ¬ç±»åˆ« - æ­£å¸¸åˆ†å±‚åˆ’åˆ†
        if multi_sample_classes:
            multi_df = df[df['isco_code'].isin(multi_sample_classes)]
            
            try:
                train_multi, test_multi = train_test_split(
                    multi_df, test_size=test_size, random_state=random_state,
                    stratify=multi_df['isco_code']
                )
                train_dfs.append(train_multi)
                test_dfs.append(test_multi)
                print(f"   å¤šæ ·æœ¬ç±»åˆ«æˆåŠŸåˆ†å±‚åˆ’åˆ†")
            except ValueError as e:
                print(f"   åˆ†å±‚åˆ’åˆ†å¤±è´¥: {e}")
                print(f"   ä½¿ç”¨éšæœºåˆ’åˆ†")
                train_multi, test_multi = train_test_split(
                    multi_df, test_size=test_size, random_state=random_state
                )
                train_dfs.append(train_multi)
                test_dfs.append(test_multi)
        
        # åˆå¹¶ç»“æœ
        if train_dfs:
            train_df = pd.concat(train_dfs, ignore_index=True)
        else:
            train_df = pd.DataFrame()
        
        if test_dfs:
            test_df = pd.concat(test_dfs, ignore_index=True)
        else:
            # å¦‚æœæ²¡æœ‰æµ‹è¯•é›†ï¼Œä»è®­ç»ƒé›†ä¸­åˆ†å‡ºä¸€éƒ¨åˆ†
            if len(train_df) >= 10:
                split_point = max(1, len(train_df) // 5)
                test_df = train_df.tail(split_point).copy()
                train_df = train_df.head(len(train_df) - split_point).copy()
                print(f"   ä»è®­ç»ƒé›†ä¸­åˆ†å‡º {len(test_df)} ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•é›†")
            else:
                test_df = pd.DataFrame()
        
        return train_df, test_df

    def process_csv_data(self, 
                        csv_path: str,
                        text_columns: List[str] = ['å²—ä½', 'å²—ä½æè¿°', 'å²—ä½èŒèƒ½'],
                        label_column: str = 'ISCO_4_Digit_Code_Gemini',
                        enable_augmentation: bool = True,
                        balance_data: bool = True,
                        target_samples_per_class: int = 10) -> Tuple[List[str], List[str], Dict]:
        """å¤„ç†CSVæ•°æ®"""
        
        print(f"ğŸ“Š å¼€å§‹å¤„ç†CSVæ•°æ®: {csv_path}")
        
        # åŠ è½½æ•°æ®
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
        except UnicodeDecodeError:
            df = pd.read_csv(csv_path, encoding='gbk')
        
        print(f"   åŸå§‹æ•°æ®: {len(df)} è¡Œ")
        
        # éªŒè¯å¿…éœ€åˆ—
        missing_cols = [col for col in text_columns + [label_column] if col not in df.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_cols}")
        
        # å¤„ç†å²—ä½èŒèƒ½åˆ—ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„åˆ—è¡¨ï¼‰
        if 'å²—ä½èŒèƒ½' in text_columns:
            df['å²—ä½èŒèƒ½'] = df['å²—ä½èŒèƒ½'].apply(self._parse_job_functions)
        
        # ç»„åˆæ–‡æœ¬ç‰¹å¾
        def combine_text_features(row):
            parts = []
            for col in text_columns:
                if col in row and pd.notna(row[col]):
                    content = str(row[col])
                    # æ¸…ç†å’Œæ ‡å‡†åŒ–
                    content = self.text_processor.clean_text(content)
                    content = self.text_processor.normalize_industry_terms(content)
                    if content:
                        parts.append(content)
            return ' '.join(parts)
        
        # å¤„ç†æ–‡æœ¬
        print("ğŸ”§ æ–‡æœ¬é¢„å¤„ç†ä¸­...")
        df['processed_text'] = df.apply(combine_text_features, axis=1)
        
        # ç§»é™¤ç©ºæ–‡æœ¬
        initial_count = len(df)
        df = df[df['processed_text'].str.strip() != ''].copy()
        if len(df) < initial_count:
            print(f"   ç§»é™¤ç©ºæ–‡æœ¬: {initial_count - len(df)} æ¡")
        
        # å¤„ç†æ ‡ç­¾
        df['isco_code'] = df[label_column].astype(str).str.zfill(4)
        
        # æå–æ–‡æœ¬å’Œæ ‡ç­¾
        texts = df['processed_text'].tolist()
        labels = df['isco_code'].tolist()
        
        # æ•°æ®è´¨é‡åˆ†æ
        stats = self._analyze_data_quality(texts, labels)
        
        # æ•°æ®å¢å¼ºå’Œå¹³è¡¡
        if enable_augmentation and balance_data:
            print("ğŸ¯ å¼€å§‹æ•°æ®å¢å¼ºå’Œå¹³è¡¡...")
            texts, labels = self.balance_handler.balance_dataset(
                texts, labels, self.augmenter, target_samples_per_class
            )
            
        elif enable_augmentation:
            print("ğŸ¯ å¼€å§‹æ•°æ®å¢å¼º...")
            augmented_texts = []
            augmented_labels = []
            
            for text, label in zip(texts, labels):
                aug_texts = self.augmenter.augment_single_text(text, num_augmented=2)
                augmented_texts.extend(aug_texts)
                augmented_labels.extend([label] * len(aug_texts))
            
            texts, labels = augmented_texts, augmented_labels
        
        # æœ€ç»ˆç»Ÿè®¡
        final_stats = self._analyze_data_quality(texts, labels)
        
        return texts, labels, {
            'original_stats': stats,
            'final_stats': final_stats,
            'processing_info': {
                'augmentation_enabled': enable_augmentation,
                'balance_enabled': balance_data,
                'target_samples_per_class': target_samples_per_class
            }
        }

    def _parse_job_functions(self, job_func_str):
        """è§£æå²—ä½èŒèƒ½å­—ç¬¦ä¸²"""
        if pd.isna(job_func_str):
            return ""
        
        job_func_str = str(job_func_str)
        
        try:
            # å°è¯•è§£æä¸ºPythonåˆ—è¡¨
            if job_func_str.startswith('[') and job_func_str.endswith(']'):
                job_funcs = ast.literal_eval(job_func_str)
                return ' '.join(job_funcs) if isinstance(job_funcs, list) else str(job_func_str)
        except:
            pass
        
        return job_func_str

    def _analyze_data_quality(self, texts: List[str], labels: List[str]) -> Dict:
        """åˆ†ææ•°æ®è´¨é‡"""
        text_lengths = [len(text) for text in texts]
        word_counts = [len(self.text_processor.segment_text(text, extract_pos=False)) for text in texts]
        
        return {
            'total_samples': len(texts),
            'unique_labels': len(set(labels)),
            'avg_text_length': np.mean(text_lengths),
            'avg_word_count': np.mean(word_counts),
            'min_text_length': min(text_lengths) if text_lengths else 0,
            'max_text_length': max(text_lengths) if text_lengths else 0,
            'label_distribution': Counter(labels)
        }

    def save_processed_data(self, 
                           texts: List[str], 
                           labels: List[str], 
                           output_dir: str):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ä¿å­˜æ–‡æœ¬å’Œæ ‡ç­¾
        with open(output_path / 'processed_texts.txt', 'w', encoding='utf-8') as f:
            for text in texts:
                f.write(text + '\n')
        
        with open(output_path / 'processed_labels.txt', 'w', encoding='utf-8') as f:
            for label in labels:
                f.write(label + '\n')
        
        print(f"âœ… å¤„ç†åæ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = EnhancedJobDataProcessor()
    
    # ç¤ºä¾‹ï¼šå¤„ç†ä½ çš„æ•°æ®
    csv_path = "newjob1_sortall.csv"  # ä½ çš„æ•°æ®æ–‡ä»¶è·¯å¾„
    
    try:
        # å¤„ç†æ•°æ®
        texts, labels, stats = processor.process_csv_data(
            csv_path=csv_path,
            enable_augmentation=True,
            balance_data=True,
            target_samples_per_class=8
        )
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœ
        print(f"\nğŸ“ˆ å¤„ç†ç»“æœç»Ÿè®¡:")
        print(f"åŸå§‹æ•°æ®: {stats['original_stats']['total_samples']} æ ·æœ¬")
        print(f"å¤„ç†åæ•°æ®: {stats['final_stats']['total_samples']} æ ·æœ¬")
        print(f"ç±»åˆ«æ•°é‡: {stats['final_stats']['unique_labels']}")
        print(f"å¹³å‡æ–‡æœ¬é•¿åº¦: {stats['final_stats']['avg_text_length']:.1f} å­—ç¬¦")
        print(f"å¹³å‡è¯æ•°: {stats['final_stats']['avg_word_count']:.1f} è¯")
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        processor.save_processed_data(texts, labels, "processed_job_data")
        
        print("\nğŸ‰ æ•°æ®å¢å¼ºä¸é¢„å¤„ç†å®Œæˆï¼")
        
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {csv_path}")
        print("è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨ç¤ºä¾‹æ•°æ®æµ‹è¯•åŠŸèƒ½")
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•
        sample_texts = [
            "è´Ÿè´£å…¬å¸äººåŠ›èµ„æºç®¡ç†å·¥ä½œï¼ŒåŒ…æ‹¬æ‹›è˜ã€åŸ¹è®­ã€ç»©æ•ˆè€ƒæ ¸ç­‰",
            "ååŠ©CEOåˆ¶å®šå…¬å¸å‘å±•æˆ˜ç•¥ï¼Œç®¡ç†é«˜çº§å›¢é˜Ÿ",
            "è´Ÿè´£é‡‡è´­ç®¡ç†ï¼Œä¾›åº”å•†å…³ç³»ç»´æŠ¤ï¼Œæˆæœ¬æ§åˆ¶"
        ]
        sample_labels = ["1212", "1111", "1212"]
        
        print("\nğŸ§ª ä½¿ç”¨ç¤ºä¾‹æ•°æ®æµ‹è¯•å¢å¼ºåŠŸèƒ½...")
        
        # æµ‹è¯•å•ä¸ªæ–‡æœ¬å¢å¼º
        augmented = processor.augmenter.augment_single_text(sample_texts[0], num_augmented=3)
        
        print(f"åŸæ–‡: {sample_texts[0]}")
        print("å¢å¼ºç»“æœ:")
        for i, aug_text in enumerate(augmented[1:], 1):
            print(f"  {i}. {aug_text}")