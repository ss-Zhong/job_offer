#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­æ–‡èŒä¸šåˆ†ç±»å™¨æ¨¡å—
æ”¯æŒå±‚æ¬¡åŒ–æŸå¤±å’Œå¤šä»»åŠ¡å­¦ä¹ 
"""

import os
import sys
import numpy as np
import pandas as pd
from tqdm import tqdm
from scipy.sparse import csr_matrix, vstack, hstack
from sklearn.datasets import dump_svmlight_file
from sklearn.feature_extraction.text import TfidfVectorizer
import string
import jieba
import jieba.posseg as pseg
from zhon.hanzi import punctuation as chinese_punctuation
from napkinxc.models import HSM, OVR

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader

# åŸºç¡€å¯¼å…¥
try:
    from job_offers_classifier.load_save import save_obj, load_obj
except ImportError:
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç®€å•çš„pickleå®ç°
    import pickle
    def save_obj(path, obj):
        with open(path, 'wb') as f:
            pickle.dump(obj, f)
    
    def load_obj(path):
        with open(path, 'rb') as f:
            return pickle.load(f)

# æ£€æŸ¥å¢å¼ºç‰ˆæ¨¡å—
try:
    from enhanced_transformer_module import EnhancedTransformerClassifier
    from hierarchical_loss import HierarchicalISCOLoss, HierarchicalMultitaskLoss
    ENHANCED_AVAILABLE = True
    print("âœ… å¢å¼ºç‰ˆTransformerClassifierå¯ç”¨")
except ImportError as e:
    ENHANCED_AVAILABLE = False
    print(f"âš ï¸ å¢å¼ºç‰ˆTransformerClassifierä¸å¯ç”¨: {e}")


# ç®€å•çš„æ•°æ®é›†ç±»
class TextDataset(Dataset):
    """æ–‡æœ¬æ•°æ®é›†"""
    def __init__(self, texts, labels=None, num_labels=None, 
                 lazy_encode=True, labels_dense_vec=False, labels_groups=None):
        self.texts = texts
        self.labels = labels
        self.num_labels = num_labels
        self.lazy_encode = lazy_encode
        self.labels_dense_vec = labels_dense_vec
        self.labels_groups = labels_groups
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        item = {'text': self.texts[idx]}
        if self.labels is not None:
            if self.labels_dense_vec:
                item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
            else:
                item['labels'] = self.labels[idx]
        return item


# ç®€å•çš„Transformeråˆ†ç±»å™¨
class TransformerClassifier(pl.LightningModule):
    """åŸºç¡€Transformeråˆ†ç±»å™¨"""
    
    def __init__(self,
                 model_name_or_path,
                 output_size,
                 learning_rate=2e-5,
                 adam_epsilon=1e-8,
                 weight_decay=0.01,
                 train_batch_size=16,
                 eval_batch_size=16,
                 verbose=True,
                 **kwargs):
        super().__init__()
        self.save_hyperparameters()
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.transformer = AutoModel.from_pretrained(model_name_or_path)
        self.dropout = nn.Dropout(0.1)
        
        # è¾“å‡ºå±‚
        hidden_size = self.transformer.config.hidden_size
        self.output = nn.Linear(hidden_size, output_size)
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        if verbose:
            print(f"âœ… TransformerClassifier åˆå§‹åŒ–å®Œæˆ")
            print(f"   æ¨¡å‹: {model_name_or_path}")
            print(f"   è¾“å‡ºå¤§å°: {output_size}")
    
    def forward(self, input_ids, attention_mask, token_type_ids=None):
        outputs = self.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )
        
        pooled_output = outputs.last_hidden_state[:, 0]
        pooled_output = self.dropout(pooled_output)
        logits = self.output(pooled_output)
        
        return logits
    
    def training_step(self, batch, batch_idx):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        token_type_ids = batch.get('token_type_ids', None)
        labels = batch['labels']
        
        logits = self.forward(input_ids, attention_mask, token_type_ids)
        loss = self.criterion(logits, labels)
        
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        return loss
    
    def validation_step(self, batch, batch_idx):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        token_type_ids = batch.get('token_type_ids', None)
        labels = batch['labels']
        
        logits = self.forward(input_ids, attention_mask, token_type_ids)
        loss = self.criterion(logits, labels)
        
        predictions = torch.argmax(logits, dim=1)
        accuracy = (predictions == labels).float().mean()
        
        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val_accuracy', accuracy, on_step=False, on_epoch=True, prog_bar=True)
        
        return loss
    
    def predict_step(self, batch, batch_idx):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        token_type_ids = batch.get('token_type_ids', None)
        
        logits = self.forward(input_ids, attention_mask, token_type_ids)
        probs = torch.softmax(logits, dim=-1)
        
        return probs
    
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.hparams.learning_rate,
            eps=self.hparams.adam_epsilon,
            weight_decay=self.hparams.weight_decay
        )
        return optimizer
    
    def save_transformer(self, save_path):
        """ä¿å­˜transformeræ¨¡å‹"""
        self.transformer.save_pretrained(save_path)


# ç®€åŒ–çš„DataModule
class TransformerDataModule(pl.LightningDataModule):
    """Transformeræ•°æ®æ¨¡å—"""
    
    def __init__(self, 
                 datasets,
                 tokenizer_name,
                 train_batch_size=16,
                 eval_batch_size=16,
                 max_seq_length=512,
                 num_workers=0,
                 tokenizer_config=None):
        super().__init__()
        self.datasets = datasets
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.train_batch_size = train_batch_size
        self.eval_batch_size = eval_batch_size
        self.max_seq_length = max_seq_length
        self.num_workers = num_workers
    
    def setup(self, stage=None):
        # è¿™é‡Œå¯ä»¥è¿›è¡Œæ•°æ®é¢„å¤„ç†
        pass
    
    def collate_fn(self, batch):
        """æ‰¹å¤„ç†å‡½æ•°"""
        texts = [item['text'] for item in batch]
        
        # ç¼–ç æ–‡æœ¬
        encoded = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=self.max_seq_length,
            return_tensors='pt'
        )
        
        # æ·»åŠ æ ‡ç­¾
        if 'labels' in batch[0]:
            labels = torch.tensor([item['labels'] for item in batch])
            encoded['labels'] = labels
        
        return encoded
    
    def train_dataloader(self):
        if 'train' not in self.datasets:
            return None
        return DataLoader(
            self.datasets['train'],
            batch_size=self.train_batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn
        )
    
    def val_dataloader(self):
        if 'val' not in self.datasets:
            return None
        return DataLoader(
            self.datasets['val'],
            batch_size=self.eval_batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn
        )
    
    def test_dataloader(self):
        if 'test' not in self.datasets:
            return None
        return DataLoader(
            self.datasets['test'],
            batch_size=self.eval_batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn
        )


# ç®€åŒ–çš„TraineråŒ…è£…å™¨
class TrainerWrapper:
    """PyTorch Lightning Trainerçš„ç®€å•åŒ…è£…å™¨"""
    
    def __init__(self, ckpt_dir=None, trainer_args=None, early_stopping=False, 
                 early_stopping_args=None, verbose=True):
        self.ckpt_dir = ckpt_dir
        self.verbose = verbose
        
        # é»˜è®¤è®­ç»ƒå™¨å‚æ•°
        default_args = {
            'max_epochs': 10,
            'devices': 1,
            'accelerator': 'auto',
            'precision': 16,
            'enable_progress_bar': verbose,
            'enable_model_summary': verbose
        }
        
        if trainer_args:
            default_args.update(trainer_args)
        
        # å›è°ƒ
        callbacks = []
        if early_stopping:
            from pytorch_lightning.callbacks import EarlyStopping
            es_args = early_stopping_args or {}
            callbacks.append(EarlyStopping(
                monitor='val_loss',
                patience=es_args.get('patience', 3),
                min_delta=es_args.get('min_delta', 0.001),
                mode='min'
            ))
        
        if ckpt_dir:
            from pytorch_lightning.callbacks import ModelCheckpoint
            callbacks.append(ModelCheckpoint(
                dirpath=ckpt_dir,
                filename='model-{epoch:02d}-{val_loss:.4f}',
                save_top_k=1,
                monitor='val_loss',
                mode='min'
            ))
        
        default_args['callbacks'] = callbacks
        
        self.trainer = pl.Trainer(**default_args)
    
    def fit(self, model, datamodule, ckpt_path=None):
        """è®­ç»ƒæ¨¡å‹"""
        self.trainer.fit(model, datamodule, ckpt_path=ckpt_path)
    
    def predict(self, model, dataloaders, ckpt_path=None):
        """é¢„æµ‹"""
        return self.trainer.predict(model, dataloaders, ckpt_path=ckpt_path)


class BaseHierarchicalJobOffersClassifier:
    """åŸºç¡€å±‚æ¬¡åŒ–èŒä¸šåˆ†ç±»å™¨"""
    
    def __init__(self,
                 model_dir=None,
                 hierarchy=None,
                 modeling_mode='bottom-up',
                 verbose=True):
        self.model_dir = model_dir
        self.hierarchy = hierarchy
        self.modeling_mode = modeling_mode
        self.base_model = None
        self.verbose = verbose

    def _init_fit(self):
        if self.model_dir is None:
            raise RuntimeError("Cannot fit with model_dir = None")

        if self.hierarchy is None:
            raise RuntimeError("Cannot fit with hierarchy = None")

        os.makedirs(self.model_dir, exist_ok=True)
        self.hierarchy_path = os.path.join(self.model_dir, "hierarchy.bin")
        save_obj(self.hierarchy_path, self.hierarchy)
        self._process_hierarchy()

    def _process_y(self, y):
        return [self.last_level_labels_map[y_i] for y_i in y]

    def _get_level_labels(self, level):
        level_labels = sorted([node['label'] for node in self.hierarchy.values() if node['level'] == level])
        return level_labels

    def _process_hierarchy(self):
        # Basic per level information
        self.levels = sorted({node['level'] for node in self.hierarchy.values()})
        self.levels_labels = {level: self._get_level_labels(level) for level in self.levels}
        self.levels_labels_map = {level: {label: i for i, label in enumerate(sorted(labels))} for level, labels in self.levels_labels.items()}
        self.levels_indices_map = {level: {i: label for label, i in labels_map.items()} for level, labels_map in self.levels_labels_map.items()}
        self.level_labels_count = {level: len(labels) for level, labels in self.levels_labels.items()}

        # Last level information
        self.last_level = max(self.levels)
        self.last_level_labels = self.levels_labels[self.last_level]
        self.last_level_labels_map = self.levels_labels_map[self.last_level]
        self.last_level_indices_map = self.levels_indices_map[self.last_level]
        self.last_level_labels_count = self.level_labels_count[self.last_level]

    def _init_load(self, model_dir):
        self.model_dir = model_dir
        self.hierarchy_path = os.path.join(self.model_dir, "hierarchy.bin")
        self.hierarchy = load_obj(self.hierarchy_path)
        self._process_hierarchy()

    def _get_output(self, pred, output_level="last", format="array", top_k=None, pred_type="flat"):
        if output_level == "last":
            output_level = self.last_level
        
        level_pred = pred
        level_map = self.last_level_indices_map

        # Get top_k labels
        if top_k is not None:
            if not isinstance(top_k, int) or top_k < 1:
                raise ValueError(f"top_k needs to be int > 0, is {top_k}")

            top_k_labels = np.flip(np.argsort(level_pred, axis=1), axis=1)[:, :top_k]
            top_k_prob = np.take_along_axis(level_pred, top_k_labels, axis=1)
            level_pred = (top_k_labels, top_k_prob)

        # Apply requested format
        if format == "array":
            return level_pred, level_map
        elif format == "dataframe":
            if top_k is None:
                columns = [v for i, v in sorted(list(level_map.items()))]
                return pd.DataFrame(level_pred, columns=columns)
            else:
                columns = [f"class_{i + 1}" for i in range(top_k)] + [f"prob_{i + 1}" for i in range(top_k)]
                df = pd.DataFrame(np.hstack(level_pred), columns=columns)
                for c in columns[:top_k]:
                    df[c] = df[c].apply(lambda x: level_map[int(x)])
                return df
        else:
            raise ValueError(f"Unknown format {format}")


class ChineseTransformerJobOffersClassifier(BaseHierarchicalJobOffersClassifier):
    """ä¸­æ–‡BERTèŒä¸šåˆ†ç±»å™¨ - æ”¯æŒå±‚æ¬¡åŒ–æŸå¤±"""
    
    def __init__(self,
                 model_dir=None,
                 hierarchy=None,
                 ckpt_path=None,
                 transformer_model="hfl/chinese-roberta-wwm-ext",
                 transformer_ckpt_path="",
                 modeling_mode='bottom-up',
                 adam_epsilon=1e-8,
                 learning_rate=2e-5,
                 weight_decay=0.01,
                 max_epochs=20,
                 batch_size=16,
                 max_sequence_length=256,
                 early_stopping=True,
                 early_stopping_delta=0.001,
                 early_stopping_patience=3,
                 devices=1,
                 accelerator="auto",
                 num_nodes=1,
                 threads=-1,
                 precision=16,
                 verbose=True,
                 # å±‚æ¬¡åŒ–å‚æ•°
                 use_hierarchical_loss=False,
                 use_multitask_learning=False,
                 hierarchical_loss_weights=None,
                 task_weights=None):
        
        super().__init__(model_dir, hierarchy, modeling_mode, verbose)
        
        # ä¿å­˜æ‰€æœ‰å‚æ•°
        self.ckpt_path = ckpt_path
        self.transformer_model = transformer_model
        self.transformer_ckpt_path = transformer_ckpt_path
        self.adam_epsilon = adam_epsilon
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.max_epochs = max_epochs
        self.batch_size = batch_size
        self.max_sequence_length = max_sequence_length
        self.early_stopping = early_stopping
        self.early_stopping_delta = early_stopping_delta
        self.early_stopping_patience = early_stopping_patience
        self.devices = devices
        self.accelerator = accelerator
        self.num_nodes = num_nodes
        self.threads = threads if threads != -1 else os.cpu_count()
        self.precision = precision
        
        # å±‚æ¬¡åŒ–å‚æ•°
        self.use_hierarchical_loss = use_hierarchical_loss and ENHANCED_AVAILABLE
        self.use_multitask_learning = use_multitask_learning and ENHANCED_AVAILABLE
        self.hierarchical_loss_weights = hierarchical_loss_weights or {1: 8.0, 2: 4.0, 3: 2.0, 4: 1.0}
        self.task_weights = task_weights or {1: 0.1, 2: 0.2, 3: 0.3, 4: 0.4}
        
        if self.verbose:
            print(f"âœ… ä¸­æ–‡BERTåˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ")
            print(f"   æ¨¡å‹: {transformer_model}")
            print(f"   å±‚æ¬¡åŒ–æŸå¤±: {'å¯ç”¨' if self.use_hierarchical_loss else 'ç¦ç”¨'}")
            print(f"   å¤šä»»åŠ¡å­¦ä¹ : {'å¯ç”¨' if self.use_multitask_learning else 'ç¦ç”¨'}")

    def _create_text_dataset(self, y, X, labels_groups=None):
        """åˆ›å»ºæ–‡æœ¬æ•°æ®é›†"""
        return TextDataset(
            X, 
            labels=y,
            num_labels=self.last_level_labels_count if hasattr(self, 'last_level_labels_count') else None,
            lazy_encode=True, 
            labels_dense_vec=False, 
            labels_groups=labels_groups
        )

    def _setup_data_module(self, dataset):
        """è®¾ç½®æ•°æ®æ¨¡å—"""
        text_dataset = {}
        
        if 'train' in dataset:
            text_dataset['train'] = self._create_text_dataset(
                *dataset['train'], 
                labels_groups=dataset.get('labels_groups', None)
            )
        
        if 'val' in dataset:
            text_dataset['val'] = self._create_text_dataset(
                *dataset['val'], 
                labels_groups=dataset.get('labels_groups', None)
            )
        
        if 'test' in dataset:
            text_dataset['test'] = self._create_text_dataset(
                *dataset['test'], 
                labels_groups=dataset.get('labels_groups', None)
            )
        
        data_module = TransformerDataModule(
            text_dataset,
            self.transformer_model,
            train_batch_size=self.batch_size,
            eval_batch_size=self.batch_size,
            max_seq_length=self.max_sequence_length,
            num_workers=self.threads if self.devices == 1 else 0
        )
        
        data_module.setup()
        return data_module

    def _fit(self, y, X, y_val=None, X_val=None, output_size=None, 
             labels_groups=None, labels_paths=None, labels_groups_mapping=None):
        """å†…éƒ¨è®­ç»ƒæ–¹æ³•"""
        if output_size is None:
            raise RuntimeError("Output size is not provided")

        dataset = {"train": (y, X)}

        if y_val is not None and X_val is not None:
            dataset["val"] = (y_val, X_val)
        
        if labels_groups is not None:
            dataset['labels_groups'] = labels_groups

        data_module = self._setup_data_module(dataset)
        
        trainer = TrainerWrapper(
            ckpt_dir=os.path.join(self.model_dir, "ckpts"),
            trainer_args={
                "max_epochs": self.max_epochs,
                "devices": self.devices,
                "num_nodes": self.num_nodes,
                "precision": self.precision,
                "accelerator": self.accelerator
            },
            early_stopping=self.early_stopping,
            early_stopping_args={
                "patience": self.early_stopping_patience,
                "min_delta": self.early_stopping_delta
            },
            verbose=self.verbose
        )

        # åˆ›å»ºæ¨¡å‹
        if (self.use_hierarchical_loss or self.use_multitask_learning) and ENHANCED_AVAILABLE:
            # ä½¿ç”¨å¢å¼ºç‰ˆ
            if self.verbose:
                print("ğŸ”§ ä½¿ç”¨å¢å¼ºç‰ˆTransformerClassifier")
            
            self.base_model = EnhancedTransformerClassifier(
                model_name_or_path=self.transformer_model,
                output_size=output_size,
                adam_epsilon=self.adam_epsilon,
                learning_rate=self.learning_rate,
                weight_decay=self.weight_decay,
                train_batch_size=self.batch_size,
                eval_batch_size=self.batch_size,
                verbose=self.verbose,

                isco_hierarchy=self.hierarchy,
                use_hierarchical_loss=self.use_hierarchical_loss,
                use_multitask_learning=self.use_multitask_learning,
                hierarchical_loss_weights=self.hierarchical_loss_weights,
                task_weights=self.task_weights,
            )

        else:
            # ä½¿ç”¨æ ‡å‡†ç‰ˆ
            if self.verbose:
                print("ğŸ“ ä½¿ç”¨æ ‡å‡†ç‰ˆTransformerClassifier")
            
            self.base_model = TransformerClassifier(
                model_name_or_path=self.transformer_model,
                output_size=output_size,
                adam_epsilon=self.adam_epsilon,
                learning_rate=self.learning_rate,
                weight_decay=self.weight_decay,
                train_batch_size=self.batch_size,
                eval_batch_size=self.batch_size,
                verbose=self.verbose,
            )

        trainer.fit(self.base_model, datamodule=data_module, ckpt_path=self.ckpt_path)
        self.base_model.save_transformer(self.model_dir)

    def fit(self, y, X, y_val=None, X_val=None):
        """è®­ç»ƒæ¨¡å‹"""
        self._init_fit()

        # ä¿å­˜æ¶æ„ä¿¡æ¯
        arch_info = {
            'transformer_model': self.transformer_model,
            'transformer_ckpt': self.transformer_ckpt_path,
            'use_hierarchical_loss': self.use_hierarchical_loss,
            'use_multitask_learning': self.use_multitask_learning,
            'hierarchical_loss_weights': self.hierarchical_loss_weights,
            'task_weights': self.task_weights,
            'enhanced_available': ENHANCED_AVAILABLE
        }
        
        save_obj(os.path.join(self.model_dir, "transformer_arch.bin"), arch_info)

        if self.verbose:
            print(f"ğŸ¯ å¼€å§‹è®­ç»ƒï¼Œæ ·æœ¬æ•°: {len(y)}")
            if y_val is not None:
                print(f"   éªŒè¯é›†: {len(y_val)} æ ·æœ¬")

        # å¤„ç†æ ‡ç­¾
        y = self._process_y(y)
        if y_val is not None:
            y_val = self._process_y(y_val)

        # è®­ç»ƒ
        self._fit(y, X, y_val=y_val, X_val=X_val, 
                 output_size=self.last_level_labels_count)

    def load(self, model_dir):
        """åŠ è½½æ¨¡å‹"""
        self._init_load(model_dir)

        # è¯»å–æ¶æ„ä¿¡æ¯
        arch_info = load_obj(os.path.join(self.model_dir, "transformer_arch.bin"))
        self.transformer_model = arch_info['transformer_model']
        self.transformer_ckpt_path = arch_info.get('transformer_ckpt', '')
        
        # è¯»å–å±‚æ¬¡åŒ–é…ç½®
        self.use_hierarchical_loss = arch_info.get('use_hierarchical_loss', False)
        self.use_multitask_learning = arch_info.get('use_multitask_learning', False)
        self.hierarchical_loss_weights = arch_info.get('hierarchical_loss_weights', {1: 8.0, 2: 4.0, 3: 2.0, 4: 1.0})
        self.task_weights = arch_info.get('task_weights', {1: 0.1, 2: 0.2, 3: 0.3, 4: 0.4})

        # åˆ›å»ºæ¨¡å‹
        if (self.use_hierarchical_loss or self.use_multitask_learning) and ENHANCED_AVAILABLE:
            self.base_model = EnhancedTransformerClassifier(
                model_name_or_path=self.model_dir,
                output_size=self.last_level_labels_count,
                isco_hierarchy=self.hierarchy,
                use_hierarchical_loss=self.use_hierarchical_loss,
                use_multitask_learning=self.use_multitask_learning,
                hierarchical_loss_weights=self.hierarchical_loss_weights,
                task_weights=self.task_weights,
                train_batch_size=self.batch_size,
                eval_batch_size=self.batch_size,
                verbose=self.verbose,
            )
        else:
            self.base_model = TransformerClassifier(
                model_name_or_path=self.model_dir,
                output_size=self.last_level_labels_count,
                train_batch_size=self.batch_size,
                eval_batch_size=self.batch_size,
                verbose=self.verbose,
            )

        # æŸ¥æ‰¾checkpoint
        self.ckpt_path = self._find_checkpoint()

    def _find_checkpoint(self):
        """æŸ¥æ‰¾checkpointæ–‡ä»¶"""
        ckpt_path = os.path.join(self.model_dir, "transformer_classifier.ckpt")
        
        if not os.path.exists(ckpt_path):
            ckpts_dir = os.path.join(self.model_dir, "ckpts")
            if os.path.exists(ckpts_dir):
                ckpt_files = [f for f in os.listdir(ckpts_dir) if f.endswith(".ckpt")]
                if ckpt_files:
                    # æŒ‰æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                    ckpt_files.sort()
                    ckpt_path = os.path.join(ckpts_dir, ckpt_files[-1])
        
        if os.path.exists(ckpt_path):
            if self.verbose:
                print(f"âœ“ æ‰¾åˆ°checkpoint: {ckpt_path}")
            return ckpt_path
        else:
            raise RuntimeError(f"æ‰¾ä¸åˆ°checkpointåœ¨ {self.model_dir}")

    def _predict(self, X):
        """å†…éƒ¨é¢„æµ‹æ–¹æ³•"""
        if self.base_model is None:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒæˆ–åŠ è½½")

        dataset = {"test": (None, X)}
        data_module = self._setup_data_module(dataset)

        trainer = TrainerWrapper(
            ckpt_dir=self.ckpt_path,
            trainer_args={"devices": self.devices, "precision": self.precision}
        )
        
        pred = trainer.predict(self.base_model, 
                              dataloaders=data_module.test_dataloader(), 
                              ckpt_path=self.ckpt_path)
        
        return np.array(torch.vstack(pred))

    def predict(self, X, output_level="last", format='array', top_k=None):
        """é¢„æµ‹"""
        if self.verbose:
            print(f"ğŸ”® é¢„æµ‹ {len(X)} ä¸ªæ ·æœ¬...")
        
        pred = self._predict(X)
        return self._get_output(pred, output_level=output_level, 
                               format=format, top_k=top_k)


# å·¥å…·å‡½æ•°
def get_recommended_chinese_models():
    """è·å–æ¨èçš„ä¸­æ–‡æ¨¡å‹"""
    return {
        'roberta': {
            'model_name': 'hfl/chinese-roberta-wwm-ext',
            'description': 'HFL Chinese RoBERTa (æ¨è) - æ€§èƒ½æœ€å¥½',
            'recommended': True
        },
        'bert': {
            'model_name': 'hfl/chinese-bert-wwm-ext',
            'description': 'HFL Chinese BERT - ç»å…¸ç¨³å®š',
            'recommended': True
        },
        'macbert': {
            'model_name': 'hfl/chinese-macbert-base',
            'description': 'HFL Chinese MacBERT - æ”¹è¿›æ¶æ„',
            'recommended': True
        },
        'google': {
            'model_name': 'bert-base-chinese',
            'description': 'Google Chinese BERT - åŸºç¡€æ¨¡å‹',
            'recommended': False
        }
    }


if __name__ == "__main__":
    print("ğŸ¯ ä¸­æ–‡èŒä¸šåˆ†ç±»å™¨æ¨¡å—")
    print("=" * 50)
    
    print("\næ¨èçš„ä¸­æ–‡BERTæ¨¡å‹:")
    for key, info in get_recommended_chinese_models().items():
        status = "â­" if info['recommended'] else "  "
        print(f"  {status} {key}: {info['model_name']} - {info['description']}")
    
    print("\nä½¿ç”¨ç¤ºä¾‹:")
    print("""
    # åˆ›å»ºå±‚çº§ç»“æ„
    from job_offers_utils import create_hierarchy_from_isco_codes
    hierarchy = create_hierarchy_from_isco_codes(['1121', '1122', '2121'])
    
    # æ ‡å‡†æ¨¡å¼ï¼ˆæ— å±‚æ¬¡åŒ–æŸå¤±ï¼‰
    classifier = ChineseTransformerJobOffersClassifier(
        model_dir='./models/standard',
        hierarchy=hierarchy,
        transformer_model='hfl/chinese-roberta-wwm-ext',
        max_epochs=5
    )
    
    # å±‚æ¬¡åŒ–æŸå¤±æ¨¡å¼
    classifier_hierarchical = ChineseTransformerJobOffersClassifier(
        model_dir='./models/hierarchical',
        hierarchy=hierarchy,
        transformer_model='hfl/chinese-roberta-wwm-ext',
        use_hierarchical_loss=True,
        hierarchical_loss_weights={1: 8.0, 2: 4.0, 3: 2.0, 4: 1.0},
        max_epochs=5
    )
    
    # è®­ç»ƒæ¨¡å‹
    texts = ["è´¢åŠ¡ç»ç†èŒè´£...", "è½¯ä»¶å·¥ç¨‹å¸ˆ..."]
    labels = ["1121", "2512"]
    classifier.fit(labels, texts)
    
    # é¢„æµ‹
    predictions = classifier.predict(new_texts, format='dataframe', top_k=5)
    """)